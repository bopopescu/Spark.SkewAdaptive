[INFO ] [2015-07-02 20:03:23] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 20:03:25] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:03:25] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:03:25] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:03:30] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 20:03:31] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:03:31] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:03:31] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:03:32] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  ly started service on port 6066.
  [INFO ] [2015-07-02 20:03:28] [Logging$class:logInfo:59] Started REST server for submitting applications on port 6066
  [INFO ] [2015-07-02 20:03:28] [Logging$class:logInfo:59] Starting Spark master at spark://spark1:7077
  [INFO ] [2015-07-02 20:03:28] [Logging$class:logInfo:59] Running Spark version 1.4.0
  [INFO ] [2015-07-02 20:03:28] [Logging$class:logInfo:59] Successfully started service 'MasterUI' on port 8080.
  [INFO ] [2015-07-02 20:03:28] [Logging$class:logInfo:59] Started MasterWebUI at http://10.0.0.38:8080
  [INFO ] [2015-07-02 20:03:28] [Logging$class:logInfo:59] I have been elected leader! New state: ALIVE
  [INFO ] [2015-07-02 20:03:33] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:03:34] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkWorker@10.0.0.41:41593]
  [INFO ] [2015-07-02 20:03:34] [Logging$class:logInfo:59] Successfully started service 'sparkWorker' on port 41593.
  [INFO ] [2015-07-02 20:03:34] [Logging$class:logInfo:59] Starting Spark worker 10.0.0.41:41593 with 4 cores, 8.8 GB RAM
  [INFO ] [2015-07-02 20:03:34] [Logging$class:logInfo:59] Running Spark version 1.4.0
  [INFO ] [2015-07-02 20:03:34] [Logging$class:logInfo:59] Spark home: /home/spark/spark-1.3.1
  [INFO ] [2015-07-02 20:03:35] [Logging$class:logInfo:59] Successfully started service 'WorkerUI' on port 8081.
  [INFO ] [2015-07-02 20:03:35] [Logging$class:logInfo:59] Started WorkerWebUI at http://10.0.0.41:8081
  [INFO ] [2015-07-02 20:03:35] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-07-02 20:03:36] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  [INFO ] [2015-07-02 20:04:14] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 20:04:15] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:04:15] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:04:15] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:04:16] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:04:16] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:04:17] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkWorker@10.0.0.42:36843]
  [INFO ] [2015-07-02 20:04:17] [Logging$class:logInfo:59] Successfully started service 'sparkWorker' on port 36843.
  [INFO ] [2015-07-02 20:04:17] [Logging$class:logInfo:59] Starting Spark worker 10.0.0.42:36843 with 4 cores, 8.8 GB RAM
  [INFO ] [2015-07-02 20:04:17] [Logging$class:logInfo:59] Running Spark version 1.4.0
  [INFO ] [2015-07-02 20:04:17] [Logging$class:logInfo:59] Spark home: /home/spark/spark-1.3.1
  [INFO ] [2015-07-02 20:04:18] [Logging$class:logInfo:59] Successfully started service 'WorkerUI' on port 8081.
  [INFO ] [2015-07-02 20:04:18] [Logging$class:logInfo:59] Started WorkerWebUI at http://10.0.0.42:8081
  [INFO ] [2015-07-02 20:04:18] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-07-02 20:04:18] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  ker@10.0.0.39:55458]
  [INFO ] [2015-07-02 20:04:19] [Logging$class:logInfo:59] Successfully started service 'sparkWorker' on port 55458.
  [INFO ] [2015-07-02 20:04:19] [Logging$class:logInfo:59] Starting Spark worker 10.0.0.39:55458 with 4 cores, 8.8 GB RAM
  [INFO ] [2015-07-02 20:04:19] [Logging$class:logInfo:59] Running Spark version 1.4.0
  [INFO ] [2015-07-02 20:04:19] [Logging$class:logInfo:59] Spark home: /home/spark/spark-1.3.1
  [INFO ] [2015-07-02 20:04:20] [Logging$class:logInfo:59] Successfully started service 'WorkerUI' on port 8081.
  [INFO ] [2015-07-02 20:04:20] [Logging$class:logInfo:59] Started WorkerWebUI at http://10.0.0.39:8081
  [INFO ] [2015-07-02 20:04:20] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-07-02 20:04:21] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  [INFO ] [2015-07-02 20:05:31] [Logging$class:logInfo:59] Running Spark version 1.4.0
  [INFO ] [2015-07-02 20:05:31] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:05:31] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:05:31] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:05:32] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:05:33] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:05:33] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.0.0.38:55023]
  [INFO ] [2015-07-02 20:05:33] [Logging$class:logInfo:59] Successfully started service 'sparkDriver' on port 55023.
  [INFO ] [2015-07-02 20:05:33] [Logging$class:logInfo:59] Registering MapOutputTracker
  [INFO ] [2015-07-02 20:05:33] [Logging$class:logInfo:59] Registering BlockManagerMaster
  [INFO ] [2015-07-02 20:05:33] [Logging$class:logInfo:59] Created local directory at /tmp/spark-0f0acbb0-3da5-4291-8641-f947eb36dc94/blockmgr-5f05bcda-62cc-4c75-9b16-31255052aad3
  [INFO ] [2015-07-02 20:05:33] [Logging$class:logInfo:59] MemoryStore started with capacity 530.3 MB
  [INFO ] [2015-07-02 20:05:33] [Logging$class:logInfo:59] HTTP File server directory is /tmp/spark-0f0acbb0-3da5-4291-8641-f947eb36dc94/httpd-22725900-33b8-4289-ba70-3da66a3c11fc
  [INFO ] [2015-07-02 20:05:33] [Logging$class:logInfo:59] Starting HTTP Server
  [INFO ] [2015-07-02 20:05:33] [Logging$class:logInfo:59] Successfully started service 'HTTP file server' on port 41399.
  [INFO ] [2015-07-02 20:05:33] [Logging$class:logInfo:59] Registering OutputCommitCoordinator
  [INFO ] [2015-07-02 20:05:34] [Logging$class:logInfo:59] Successfully started service 'SparkUI' on port 4040.
  [INFO ] [2015-07-02 20:05:34] [Logging$class:logInfo:59] Started SparkUI at http://10.0.0.38:4040
  [INFO ] [2015-07-02 20:05:38] [Logging$class:logInfo:59] Added JAR file:/home/spark/spark-1.3.1/./examples/target/scala-2.10/spark-examples-1.4.0-hadoop1.2.1.jar at http://10.0.0.38:41399/jars/spark-examples-1.4.0-hadoop1.2.1.jar with timestamp 1435838738112
  [INFO ] [2015-07-02 20:05:38] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-07-02 20:05:38] [Logging$class:logInfo:59] Registering app PageRank
  [INFO ] [2015-07-02 20:05:38] [Logging$class:logInfo:59] Registered app PageRank with ID app-20150702200538-0000
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Launching executor app-20150702200538-0000/0 on worker worker-20150702200417-10.0.0.42-36843
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Launching executor app-20150702200538-0000/1 on worker worker-20150702200419-10.0.0.39-55458
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Launching executor app-20150702200538-0000/2 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Connected to Spark cluster with app ID app-20150702200538-0000
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Executor added: app-20150702200538-0000/0 on worker-20150702200417-10.0.0.42-36843 (10.0.0.42:36843) with 4 cores
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Granted executor ID app-20150702200538-0000/0 on hostPort 10.0.0.42:36843 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Executor added: app-20150702200538-0000/1 on worker-20150702200419-10.0.0.39-55458 (10.0.0.39:55458) with 4 cores
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Granted executor ID app-20150702200538-0000/1 on hostPort 10.0.0.39:55458 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Executor added: app-20150702200538-0000/2 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Granted executor ID app-20150702200538-0000/2 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:05:43] [Logging$class:logInfo:59] Asked to launch executor app-20150702200538-0000/1 for PageRank
  [INFO ] [2015-07-02 20:05:43] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=55023" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:55023/user/CoarseGrainedScheduler" "--executor-id" "1" "--hostname" "10.0.0.39" "--cores" "4" "--app-id" "app-20150702200538-0000" "--worker-url" "akka.tcp://sparkWorker@10.0.0.39:55458/user/Worker"
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Executor updated: app-20150702200538-0000/2 is now LOADING
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52424.
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Server created on 52424
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Registering block manager 10.0.0.38:52424 with 530.3 MB RAM, BlockManagerId(driver, 10.0.0.38, 52424)
  [INFO ] [2015-07-02 20:05:39] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:05:44] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 20:05:45] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:05:45] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:05:45] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  nd is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
  [INFO ] [2015-07-02 20:05:42] [Logging$class:logInfo:59] ensureFreeSpace(45808) called with curMem=0, maxMem=556038881
  [INFO ] [2015-07-02 20:05:42] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 44.7 KB, free 530.2 MB)
  [INFO ] [2015-07-02 20:05:43] [Logging$class:logInfo:59] ensureFreeSpace(4294) called with curMem=45808, maxMem=556038881
  [INFO ] [2015-07-02 20:05:43] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 530.2 MB)
  [INFO ] [2015-07-02 20:05:43] [Logging$class:logInfo:59] Added broadcast_0_piece0 in memory on 10.0.0.38:52424 (size: 4.2 KB, free: 530.3 MB)
  [INFO ] [2015-07-02 20:05:43] [Logging$class:logInfo:59] Created broadcast 0 from textFile at SparkPageRank.scala:56
  [WARN ] [2015-07-02 20:05:45] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-07-02 20:05:45] [FileInputFormat:listStatus:199] Total input paths to process : 1
  [INFO ] [2015-07-02 20:05:45] [Logging$class:logInfo:59] Starting job: collect at SparkPageRank.scala:71
  [INFO ] [2015-07-02 20:05:45] [Logging$class:logInfo:59] Registering RDD 3 (distinct at SparkPageRank.scala:60)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Registering RDD 5 (distinct at SparkPageRank.scala:60)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Registering RDD 12 (flatMap at SparkPageRank.scala:64)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Got job 0 (collect at SparkPageRank.scala:71) with 1 output partitions (allowLocal=false)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Final stage: ResultStage 3(collect at SparkPageRank.scala:71)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Parents of final stage: List(ShuffleMapStage 2)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Missing parents: List(ShuffleMapStage 2)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at distinct at SparkPageRank.scala:60), which has no missing parents
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] ensureFreeSpace(4024) called with curMem=50102, maxMem=556038881
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 530.2 MB)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] ensureFreeSpace(2289) called with curMem=54126, maxMem=556038881
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 530.2 MB)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Added broadcast_1_piece0 in memory on 10.0.0.38:52424 (size: 2.2 KB, free: 530.3 MB)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Created broadcast 1 from broadcast at DAGScheduler.scala:874
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at distinct at SparkPageRank.scala:60)
  [INFO ] [2015-07-02 20:05:46] [Logging$class:logInfo:59] Adding task set 0.0 with 1 tasks
  [INFO ] [2015-07-02 20:06:02] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  es; check your cluster UI to ensure that workers are registered and have sufficient resources
  [INFO ] [2015-07-02 20:06:07] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  07-02 20:06:11] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:06:11] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:06:11] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:06:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.42:38753]
  [I[INFO ] [2015-07-02 20:06:20] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  port 38753.
   UI to ensure that workers are registered and have sufficient resources
  [INFO ] [2015-07-02 20:06:36] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:06:36] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:06:36] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:06:36] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:06:36] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:06:39] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:06:47] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:06:47] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-0[INFO ] [2015-07-02 20:06:55] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  ent.TimeoutException: Futures timed out after [10000 milliseconds]
  [INFO ] [2015-07-02 20:06:55] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:06:55] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:07:00] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 20:07:00] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 20:07:00] [Logging$class:logInfo:59] Executor app-20150702200538-0000/0 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:07:00] [Logging$class:logInfo:59] Asked to launch executor app-20150702200538-0000/3 for PageRank
  [INFO ] [2015-07-02 20:07:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=55023" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:55023/user/CoarseGrainedScheduler" "--executor-id" "3" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702200538-0000" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  ng$class:logInfo:59] Executor updated: app-20150702200538-0000/3 is now LOADING
  [INFO ] [2015-07-02 20:07:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.39:41098]
  [INFO ] [2015-07-02 20:07:02] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 41098.
  ogging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:07:03] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:07:03] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:07:03] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.42:58472]
  [INFO ] [2015-07-02 20:07:03] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 58472.
  [INFO ] [2015-07-02 20:07:03] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:07:03] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:07:03] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:07:03] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:07:03] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:07:03] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:07:03] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 20:07:03] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:07:03] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.0.42:34025]
  [INFO ] [2015-07-02 20:07:03] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 34025.
  [INFO ] [2015-07-02 20:07:04] [Logging$class:logInfo:59] Created local directory at /tmp/spark-355db919-c195-4621-a3f6-e4ec22ae1b37/executor-277e2fdd-b5cb-4e8c-a82b-99e1777d949b/blockmgr-7a4b8d0d-6048-479f-80b1-ef9f7ab21a31
  [INFO ] [2015-07-02 20:07:04] [Logging$class:logInfo:59] MemoryStore started with capacity 4.5 GB
  [INFO ] [2015-07-02 20:07:06] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@10.0.0.38:55023/user/CoarseGrainedScheduler
  [INFO ] [2015-07-02 20:07:06] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker
  [INFO ] [2015-07-02 20:07:06] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker
  [INFO ] [2015-07-02 20:07:06] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-07-02 20:07:06] [Logging$class:logInfo:59] Starting executor ID 3 on host 10.0.0.42
  [INFO ] [2015-07-02 20:07:10] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56288.
  [INFO ] [2015-07-02 20:07:10] [Logging$class:logInfo:59] Server created on 56288
  [INFO ] [2015-07-02 20:07:10] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:07:10] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:07:16] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 20:07:16] [Logging$class:logInfo:59] Executor app-20150702200538-0000/2 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:07:16] [Logging$class:logInfo:59] Asked to launch executor app-20150702200538-0000/4 for PageRank
  [INFO ] [2015-07-02 20:07:16] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=55023" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:55023/user/CoarseGrainedScheduler" "--executor-id" "4" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702200538-0000" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  ng$class:logInfo:59] Executor updated: app-20150702200538-0000/4 is now LOADING
  [INFO ] [2015-07-02 20:07:20] [Logging$class:logInfo:59] Got assigned task 0
  [INFO ] [2015-07-02 20:07:21] [Logging$class:logInfo:59] Running task 0.0 in stage 0.0 (TID 0)
  [INFO ] [2015-07-02 20:07:21] [Logging$class:logInfo:59] Fetching http://10.0.0.38:41399/jars/spark-examples-1.4.0-hadoop1.2.1.jar with timestamp 1435838738112
  [INFO ] [2015-07-02 20:07:21] [Logging$class:logInfo:59] Fetching http://10.0.0.38:41399/jars/spark-examples-1.4.0-hadoop1.2.1.jar to /tmp/spark-355db919-c195-4621-a3f6-e4ec22ae1b37/executor-277e2fdd-b5cb-4e8c-a82b-99e1777d949b/fetchFileTemp7466020433845894344.tmp
  02 20:07:25] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:07:29] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:07:29] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:07:29] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:07:29] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:07:29] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  te EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:07:35] [Logging$class:logInfo:59] Asked to launch executor app-20150702200538-0000/5 for PageRank
  [INFO ] [2015-07-02 20:07:35] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=55023" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:55023/user/CoarseGrainedScheduler" "--executor-id" "5" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702200538-0000" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  ng$class:logInfo:59] Executor updated: app-20150702200538-0000/5 is now RUNNING
  [INFO ] [2015-07-02 20:07:37] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 20:07:38] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:07:38] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:07:38] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:07:38] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:07:38] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:07:39] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.41:58207]
  [INFO ] [2015-07-02 20:07:43] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 58207.
  [INFO ] [2015-07-02 20:07:[INFO ] [2015-07-02 20:08:05] [Logging$class:logInfo:59] MemoryStore started with capacity 4.5 GB
  $class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:07:46] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:07:46] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:07:46] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:07:48] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:07:48] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [ERROR] [2015-07-02 20:08:14] [UserGroupInformation:doAs:1193] PriviledgedActionException as:spark cause:java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
  [INFO ] [2015-07-02 20:08:16] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 20:08:16] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:08:16] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:08:16] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 20:08:21] [Logging$class:logInfo:59] Executor app-20150702200538-0000/5 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:08:21] [Logging$class:logInfo:59] Asked to launch executor app-20150702200538-0000/6 for PageRank
  [INFO ] [2015-07-02 20:08:21] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=55023" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:55023/user/CoarseGrainedScheduler" "--executor-id" "6" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702200538-0000" "--worker-url" "akka.tcp://sparkWo[INFO ] [2015-07-02 20:08:21] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@10.0.0.38:55023/[INFO ] [2015-07-02 20:08:27] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 20:08:31] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:08:31] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:08:31] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:08:36] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:08:36] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:08:37] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.41:55832]
  [INFO ] [2015-07-02 20:08:37] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 55832.
  [INFO ] [2015-07-02 20:08:38] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:08:38] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:08:39] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:08:39] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:08:39] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:08:39] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:08:39] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 20:08:39] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:08:39] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 48290.
  [INFO ] [2015-07-02 20:08:39] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.0.41:48290]
  [INFO ] [2015-07-02 20:08:41] [Logging$class:logInfo:59] Created local directory at /tmp/spark-e4904e3b-0d4a-410e-838e-4b7d2fe544ab/executor-58bfc494-e5ce-4db5-a339-ad81e1204f27/blockmgr-225ecef0-cbdb-4e07-867a-fb3ce9c733ff
  [INFO ] [2015-07-02 20:08:41] [Logging$class:logInfo:59] MemoryStore started with capacity 4.5 GB
  [INFO ] [2015-07-02 20:08:42] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@10.0.0.38:55023/user/CoarseGrainedScheduler
  [INFO ] [2015-07-02 20:08:42] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker
  [INFO ] [2015-07-02 20:08:42] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker
  [INFO ] [2015-07-02 20:08:42] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-07-02 20:08:42] [Logging$class:logInfo:59] Starting executor ID 6 on host 10.0.0.41
  [INFO ] [2015-07-02 20:08:42] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35799.
  [INFO ] [2015-07-02 20:08:42] [Logging$class:logInfo:59] Server created on 35799
  [INFO ] [2015-07-02 20:08:42] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:08:42] [Logging$class:logInfo:59] Registered BlockManager
  ging$class:logInfo:59] Starting executor ID 1 on host 10.0.0.39
  [WARN ] [2015-07-02 20:08:53] [Slf4JLogger:warn:136] Failed to generate a seed from SecureRandom within 3 seconds. Not enough entrophy?
  [INFO ] [2015-07-02 20:09:14] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40587.
  [INFO ] [2015-07-02 20:09:14] [Logging$class:logInfo:59] Server created on 40587
  [INFO ] [2015-07-02 20:09:14] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:09:14] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:09:47] [Logging$class:logInfo:59] Copying /tmp/spark-355db919-c195-4621-a3f6-e4ec22ae1b37/executor-277e2fdd-b5cb-4e8c-a82b-99e1777d949b/-13421085181435838738112_cache to /home/spark/spark-1.3.1/work/app-20150702200538-0000/3/./spark-examples-1.4.0-hadoop1.2.1.jar
  2015-07-02 20:09:49] [Logging$class:logInfo:59] Job 0 failed: collect at SparkPageRank.scala:71, took 243.876280 s
  [INFO ] [2015-07-02 20:09:49] [Logging$class:logInfo:59] ShuffleMapStage 0 (distinct at SparkPageRank.scala:60) failed in 243.100 s
  [INFO ] [2015-07-02 20:09:49] [Logging$class:logInfo:59] Shutting down all executors
  [INFO ] [2015-07-02 20:09:49] [Logging$class:logInfo:59] Asking each executor to shut down
  [INFO ] [2015-07-02 20:09:53] [Logging$class:logInfo:59] Driver commanded a shutdown
  om application app-20150702200538-0000
  [INFO ] [2015-07-02 20:09:54] [Logging$class:logInfo:59] Driver commanded a shutdown
  8-0000
  [INFO ] [2015-07-02 20:09:49] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:09:49] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:09:49] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:09:54] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-07-02 20:09:56] [Logging$class:logInfo:59] Asked to kill executor app-20150702200538-0000/1
  41-f947eb36dc94/blockmgr-5f05bcda-62cc-4c75-9b16-31255052aad3, already present as root for deletion.
  [INFO ] [2015-07-02 20:09:50] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-07-02 20:09:50] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-07-02 20:09:50] [Logging$class:logInfo:59] BlockManagerMaster stopped
  [INFO ] [2015-07-02 20:09:50] [Logging$class:logInfo:59] OutputCommitCoordinator stopped!
  [INFO ] [2015-07-02 20:09:50] [Logging$class:logInfo:59] Successfully stopped SparkContext
  [INFO ] [2015-07-02 20:09:50] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 20:09:50] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:09:50] [Logging$class:logInfo:59] Deleting directory /tmp/spark-0f0acbb0-3da5-4291-8641-f947eb36dc94
  [INFO ] [2015-07-02 20:09:50] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [WARN ] [2015-07-02 20:09:50] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkDriver@10.0.0.38:55023] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 20:09:52] [Logging$class:logInfo:59] akka.tcp://sparkDriver@10.0.0.38:55023 got disassociated, removing it.
  [INFO ] [2015-07-02 20:09:52] [Logging$class:logInfo:59] akka.tcp://sparkDriver@10.0.0.38:55023 got disassociated, removing it.
  [INFO ] [2015-07-02 20:09:57] [Logging$class:logInfo:59] Runner thread for executor app-20150702200538-0000/1 interrupted
  [INFO ] [2015-07-02 20:09:57] [Logging$class:logInfo:59] Killing process!
  0702200538-0000/6 interrupted
  [INFO ] [2015-07-02 20:09:57] [Logging$class:logInfo:59] Killing process!
  [ERROR] [2015-07-02 20:09:59] [Logging$class:logError:96] Error writing stream to file /home/spark/spark-1.3.1/work/app-20150702200538-0000/6/stderr
  java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)
[ERROR] [2015-07-02 20:09:59] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 1[ERROR] [2015-07-02 20:10:05] [Logging$class:logError:96] Error writing stream to file /home/spark/spark-1.3.1/work/app-20150702200538-0000/3/stderr
  java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)
07-02 20:10:15] [Logging$class:logInfo:59] Registering MapOutputTracker
  [INFO ] [2015-07-02 20:10:15] [Logging$class:logInfo:59] Registering BlockManagerMaster
  [INFO ] [2015-07-02 20:10:15] [Logging$class:logInfo:59] Created local directory at /tmp/spark-f3de98ac-ae9b-4391-b5bc-8ae93dbd0c70/blockmgr-d5eec9e4-c7c2-44fd-aef9-c8760dfde31e
  [INFO ] [2015-07-02 20:10:15] [Logging$class:logInfo:59] MemoryStore started with capacity 530.3 MB
  [INFO ] [2015-07-02 20:10:15] [Logging$class:logInfo:59] HTTP File server directory is /tmp/spark-f3de98ac-ae9b-4391-b5bc-8ae93dbd0c70/httpd-1602fa53-22f1-432c-b4f1-9571d5db10d8
  [INFO ] [2015-07-02 20:10:15] [Logging$class:logInfo:59] Starting HTTP Server
  [INFO ] [2015-07-02 20:10:16] [Logging$class:logInfo:59] Successfully started service 'HTTP file server' on port 37040.
  [INFO ] [2015-07-02 20:10:16] [Logging$class:logInfo:59] Registering OutputCommitCoordinator
  [INFO ] [2015-07-02 20:10:16] [Logging$class:logInfo:59] Successfully started service 'SparkUI' on port 4040.
  [INFO ] [2015-07-02 20:10:16] [Logging$class:logInfo:59] Started SparkUI at http://10.0.0.38:4040
  [WARN ] [2015-07-02 20:10:22] [Logging$class:logWarning:92] GET /stages/ failed: java.lang.NullPointerException
  java.lang.NullPointerException
	at org.apache.spark.SparkContext.getAllPools(SparkContext.scala:1484)
	at org.apache.spark.ui.jobs.AllStagesPage$$anonfun$5.apply(AllStagesPage.scala:59)
	at org.apache.spark.ui.jobs.AllStagesPage$$anonfun$5.apply(AllStagesPage.scala:59)
	at scala.Option.map(Option.scala:145)
	at org.apache.spark.ui.jobs.AllStagesPage.render(AllStagesPage.scala:59)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)
	at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:69)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
[WARN ] [2015-07-02 20:10:22] [ServletHandler:doHandle:561] /stages/
  java.lang.NullPointerException
	at org.apache.spark.SparkContext.getAllPools(SparkContext.scala:1484)
	at org.apache.spark.ui.jobs.AllStagesPage$$anonfun$5.apply(AllStagesPage.scala:59)
	at org.apache.spark.ui.jobs.AllStagesPage$$anonfun$5.apply(AllStagesPage.scala:59)
	at scala.Option.map(Option.scala:145)
	at org.apache.spark.ui.jobs.AllStagesPage.render(AllStagesPage.scala:59)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)
	at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:69)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
[INFO ] [2015-07-02 20:10:25] [Logging$class:logInfo:59] Added JAR file:/home/spark/spark-1.3.1/./examples/target/scala-2.10/spark-examples-1.4.0-hadoop1.2.1.jar at http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar with timestamp 1435839025941
  [INFO ] [2015-07-02 20:10:26] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-07-02 20:10:26] [Logging$class:logInfo:59] Registering app PageRank
  [INFO ] [2015-07-02 20:10:26] [Logging$class:logInfo:59] Registered app PageRank with ID app-20150702201026-0001
  [INFO ] [2015-07-02 20:10:26] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/0 on worker worker-20150702200417-10.0.0.42-36843
  [INFO ] [2015-07-02 20:10:26] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/1 on worker worker-20150702200419-10.0.0.39-55458
  [INFO ] [2015-07-02 20:10:27] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/2 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:10:27] [Logging$class:logInfo:59] Connected to Spark cluster with app ID app-20150702201026-0001
  [INFO ] [2015-07-02 20:10:31] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/1 for PageRank
  [INFO ] [2015-07-02 20:10:33] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "1" "--hostname" "10.0.0.39" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.39:55458/user/Worker"
  th 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:10:31] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/0 for PageRank
  [INFO ] [2015-07-02 20:10:33] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "0" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  [INFO ] [2015-07-02 20:10:28] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/0 is now LOADING
  [INFO ] [2015-07-02 20:10:28] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41917.
  [INFO ] [2015-07-02 20:10:28] [Logging$class:logInfo:59] Server created on 41917
  [INFO ] [2015-07-02 20:10:28] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:10:28] [Logging$class:logInfo:59] Registering block manager 10.0.0.38:41917 with 530.3 MB RAM, BlockManagerId(driver, 10.0.0.38, 41917)
  [INFO ] [2015-07-02 20:10:28] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:10:28] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/1 is now LOADING
  [WARN ] [2015-07-02 20:10:29] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO ] [2015-07-02 20:10:29] [Logging$class:logInfo:59] Logging events to file:///home/spark/history-spark/app-20150702201026-0001
  [INFO ] [2015-07-02 20:10:29] [Logging$class:logInfo:59] SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
  [INFO ] [2015-07-02 20:10:31] [Logging$class:logInfo:59] ensureFreeSpace(45808) called with curMem=0, maxMem=556038881
  [INFO ] [2015-07-02 20:10:31] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 44.7 KB, free 530.2 MB)
  [INFO ] [2015-07-02 20:10:31] [Logging$class:logInfo:59] ensureFreeSpace(4294) called with curMem=45808, maxMem=556038881
  [INFO ] [2015-07-02 20:10:31] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 530.2 MB)
  [INFO ] [2015-07-02 20:10:31] [Logging$class:logInfo:59] Added broadcast_0_piece0 in memory on 10.0.0.38:41917 (size: 4.2 KB, free: 530.3 MB)
  [INFO ] [2015-07-02 20:10:32] [Logging$class:logInfo:59] Created broadcast 0 from textFile at SparkPageRank.scala:56
  [WARN ] [2015-07-02 20:10:33] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-07-02 20:10:33] [FileInputFormat:listStatus:199] Total input paths to process : 1
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Starting job: collect at SparkPageRank.scala:71
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Registering RDD 3 (distinct at SparkPageRank.scala:60)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Registering RDD 5 (distinct at SparkPageRank.scala:60)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Registering RDD 12 (flatMap at SparkPageRank.scala:64)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Got job 0 (collect at SparkPageRank.scala:71) with 20 output partitions (allowLocal=false)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Final stage: ResultStage 3(collect at SparkPageRank.scala:71)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Parents of final stage: List(ShuffleMapStage 2)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Missing parents: List(ShuffleMapStage 2)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at distinct at SparkPageRank.scala:60), which has no missing parents
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] ensureFreeSpace(4024) called with curMem=50102, maxMem=556038881
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 530.2 MB)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] ensureFreeSpace(2291) called with curMem=54126, maxMem=556038881
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 530.2 MB)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Added broadcast_1_piece0 in memory on 10.0.0.38:41917 (size: 2.2 KB, free: 530.3 MB)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Created broadcast 1 from broadcast at DAGScheduler.scala:874
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Submitting 20 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at distinct at SparkPageRank.scala:60)
  [INFO ] [2015-07-02 20:10:34] [Logging$class:logInfo:59] Adding task set 0.0 with 20 tasks
  [ERROR] [2015-07-02 20:10:42] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-07-02 20:10:42] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-07-02 20:10:42] [Logging$class:logInfo:59] BlockManager stopped
  [ERROR] [2015-07-02 20:10:42] [Logging$class:logError:96] Error writing stream to file /home/spark/spark-1.3.1/work/app-20150702200538-0000/1/stderr
  java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)
[ERROR] [2015-07-02 20:10:48] [Logging$class:logError:75] Driver 10.0.0.38:55023 disassociated! Shutting down.
  r cluster UI to ensure that workers are registered and have sufficient resources
  [WARN ] [2015-07-02 20:10:52] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkDriver@10.0.0.38:55023] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  00538-0000
  [WARN ] [2015-07-02 20:11:07] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkExecutor@10.0.0.41:48290] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 20:11:08] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 20:11:09] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:11:09] [Logging$class:logInfo:59] Changing modify acls to[INFO ] [2015-07-02 20:11:15] [Logging$class:logInfo:59] Executor app-20150702200538-0000/1 finished with state KILLED exitStatus 143
  [INFO ] [2015-07-02 20:11:15] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150702200538-0000
  [WARN ] [2015-07-02 20:11:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkExecutor@10.0.0.39:53208] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.41:51078]
  [INFO ] [2015-07-02 20:11:10] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 51078.
  [INFO ] [2015-07-02 20:11:17] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:11:17] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:11:17] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:11:17] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:11:17] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:11:18] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:11:18] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:11:18] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 20:11:18] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.0.41:57374]
  [INFO ] [2015-07-02 20:11:18] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 57374.
  [INFO ] [2015-07-02 20:11:18] [Logging$class:logInfo:59] Created local directory at /tmp/spark-e4904e3b-0d4a-410e-838e-4b7d2fe544ab/executor-b0c44b47-b315-4bee-a7e3-9acaa0703452/blockmgr-3ada2ba0-8a1a-48bb-9109-5b0b8126c94d
  [INFO ] [2015-07-02 20:11:18] [Logging$class:logInfo:59] MemoryStore started with capacity 4.5 GB
  [INFO ] [2015-07-02 20:11:19] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler
  [INFO ] [2015-07-02 20:11:19] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker
  [INFO ] [2015-07-02 20:11:19] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker
  [INFO ] [2015-07-02 20:11:19] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-07-02 20:11:19] [Logging$class:logInfo:59] Starting executor ID 2 on host 10.0.0.41
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36774.
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Server created on 36774
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Got assigned task 0
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Got assigned task 1
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Got assigned task 2
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Got assigned task 3
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Running task 0.0 in stage 0.0 (TID 0)
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Running task 1.0 in stage 0.0 (TID 1)
  [INFO ] [2015-07-02 20:11:20] [Logging$class:logInfo:59] Running task 3.0 in stage 0.0 (TID 3)
  [INFO ] [2015-07-02 20:11:21] [Logging$class:logInfo:59] Running task 2.0 in stage 0.0 (TID 2)
  [INFO ] [2015-07-02 20:11:22] [Logging$class:logInfo:59] Fetching http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar with timestamp 1435839025941
  [INFO ] [2015-07-02 20:11:23] [Logging$class:logInfo:59] Fetching http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar to /tmp/spark-e4904e3b-0d4a-410e-838e-4b7d2fe544ab/executor-b0c44b47-b315-4bee-a7e3-9acaa0703452/fetchFileTemp140548770888870450.tmp
  t(spark)
  [INFO ] [2015-07-02 20:11:24] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:11:24] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:11:24] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 20:11:24] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:11:24] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:11:24] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.0.39:37097]
  [INFO ] [2015-07-02 20:11:24] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 37097.
  [INFO ] [2015-07-02 20:11:25] [Logging$class:logInfo:59] Created local directory at /tmp/spark-19286363-5ffb-4037-aed7-3b6a627f7d32/executor-d555957d-2ef7-4437-a53b-0a0c624669be/blockmgr-fb6e8b82-2c3a-43a9-ae8c-f96fb293179d
  [INFO ] [2015-07-02 20:11:25] [Logging$class:logInfo:59] MemoryStore started with capacity 4.5 GB
  [INFO ] [2015-07-02 20:11:26] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler
  [INFO ] [2015-07-02 20:11:26] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@10.0.0.39:55458/user/Worker
  [INFO ] [2015-07-02 20:11:26] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@10.0.0.39:55458/user/Worker
  [INFO ] [2015-07-02 20:11:26] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-07-02 20:11:26] [Logging$class:logInfo:59] Starting executor ID 1 on host 10.0.0.39
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56818.
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Server created on 56818
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Got assigned task 4
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Got assigned task 5
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Got assigned task 6
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Running task 4.0 in stage 0.0 (TID 4)
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Running task 5.0 in stage 0.0 (TID 5)
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Got assigned task 7
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Running task 6.0 in stage 0.0 (TID 6)
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Running task 7.0 in stage 0.0 (TID 7)
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Fetching http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar with timestamp 1435839025941
  [INFO ] [2015-07-02 20:11:27] [Logging$class:logInfo:59] Fetching http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar to /tmp/spark-19286363-5ffb-4037-aed7-3b6a627f7d32/executor-d555957d-2ef7-4437-a53b-0a0c624669be/fetchFileTemp1865135122980532948.tmp
  [ERROR] [2015-07-02 20:11:39] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-07-02 20:11:58] [Logging$class:logInfo:59] BlockManager stopped
  [WARN ] [2015-07-02 20:11:58] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkDriver@10.0.0.38:55023] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [ERROR] [2015-07-02 20:12:05] [Logging$class:logError:75] Driver 10.0.0.38:55023 disassociated! Shutting down.
  [INFO ] [2015-07-02 20:12:05] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 20:12:09] [Logging$class:logInfo:59] Executor app-20150702200538-0000/3 finished with state KILLED exitStatus 143
  [INFO ] [2015-07-02 20:12:09] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150702200538-0000
  [WARN ] [2015-07-02 20:12:10] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkExecutor@10.0.0.42:34025] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 20:12:16] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 20:12:17] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:12:17] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:12:17] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:12:18] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:12:18] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [ERROR] [2015-07-02 20:12:29] [UserGroupInformation:doAs:1193] PriviledgedActionException as:spark cause:java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
  [WARN ] [2015-07-02 20:12:29] [JdkLogger:warn:76] Failed to get all boss threads ready within 10 second(s). Make sure to specify the executor which has more threads than the requested bossCount. If unsure, use Executors.newCachedThreadPool().
  [INFO ] [2015-07-02 20:12:30] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:12:30] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:12:30] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 20:12:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/0 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:12:30] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/3 for PageRank
  [INFO ] [2015-07-02 20:12:30] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "3" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  ng$class:logInfo:59] Executor updated: app-20150702201026-0001/3 is now RUNNING
  [INFO ] [2015-07-02 20:12:32] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 20:12:32] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:12:32] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:12:32] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:12:33] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:12:33] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:12:34] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.42:45903]
  [INFO ] [2015-07-02 20:12:34] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 45903.
  [INFO ] [2015-07-02 20:12:34] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 20:12:34] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 20:12:34] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 20:12:34] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 20:12:34] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 20:12:34] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 20:12:34] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 20:12:34] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 20:12:34] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.0.42:46768]
  [INFO ] [2015-07-02 20:12:34] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 46768.
  [INFO ] [2015-07-02 20:12:34] [Logging$class:logInfo:59] Created local directory at /tmp/spark-355db919-c195-4621-a3f6-e4ec22ae1b37/executor-d0afeb26-6e8a-4200-ac74-40a10210deaf/blockmgr-b10639b6-d35b-4a17-ab9b-05a29e63340a
  [INFO ] [2015-07-02 20:12:34] [Logging$class:logInfo:59] MemoryStore started with capacity 4.5 GB
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Starting executor ID 3 on host 10.0.0.42
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53639.
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Server created on 53639
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Got assigned task 8
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Got assigned task 9
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Got assigned task 10
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Got assigned task 11
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Running task 9.0 in stage 0.0 (TID 9)
  [INFO ] [2015-07-02 20:12:35] [Logging$class:logInfo:59] Running task 8.0 in stage 0.0 (TID 8)
  [INFO ] [2015-07-02 20:12:36] [Logging$class:logInfo:59] Running task 10.0 in stage 0.0 (TID 10)
  [INFO ] [2015-07-02 20:12:36] [Logging$class:logInfo:59] Running task 11.0 in stage 0.0 (TID 11)
  [INFO ] [2015-07-02 20:12:36] [Logging$class:logInfo:59] Fetching http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar with timestamp 1435839025941
  [INFO ] [2015-07-02 20:12:36] [Logging$class:logInfo:59] Fetching http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar to /tmp/spark-355db919-c195-4621-a3f6-e4ec22ae1b37/executor-d0afeb26-6e8a-4200-ac74-40a10210deaf/fetchFileTemp7536451203558640163.tmp
  [INFO ] [2015-07-02 20:13:40] [Logging$class:logInfo:59] Copying /tmp/spark-e4904e3b-0d4a-410e-838e-4b7d2fe544ab/executor-b0c44b47-b315-4bee-a7e3-9acaa0703452/-4441382601435839025941_cache to /home/spark/spark-1.3.1/work/app-20150702201026-0001/2/./spark-examples-1.4.0-hadoop1.2.1.jar
  [INFO ] [2015-07-02 20:14:18] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150702201026-0001/2/./spark-examples-1.4.0-hadoop1.2.1.jar to class loader
  [INFO ] [2015-07-02 20:14:18] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ] [2015-07-02 20:14:21] [Logging$class:logInfo:59] ensureFreeSpace(2291) called with curMem=0, maxMem=4829950771
  [INFO ] [2015-07-02 20:14:21] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 4.5 GB)
  [INFO ] [2015-07-02 20:14:21] [Logging$class:logInfo:59] Reading broadcast variable 1 took 2133 ms
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] ensureFreeSpace(4024) called with curMem=2291, maxMem=4829950771
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 4.5 GB)
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:134217728+67108864
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:0+67108864
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:67108864+67108864
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:201326592+67108864
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] ensureFreeSpace(4294) called with curMem=6315, maxMem=4829950771
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 4.5 GB)
  [INFO ] [2015-07-02 20:14:22] [Logging$class:logInfo:59] Reading broadcast variable 0 took 32 ms
  [INFO ] [2015-07-02 20:14:23] [Logging$class:logInfo:59] ensureFreeSpace(71248) called with curMem=10609, maxMem=4829950771
  [INFO ] [2015-07-02 20:14:23] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 69.6 KB, free 4.5 GB)
  [WARN ] [2015-07-02 20:14:24] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-07-02 20:14:24] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-07-02 20:15:43] [Logging$class:logInfo:59] Copying /tmp/spark-19286363-5ffb-4037-aed7-3b6a627f7d32/executor-d555957d-2ef7-4437-a53b-0a0c624669be/-4441382601435839025941_cache to /home/spark/spark-1.3.1/work/app-20150702201026-0001/1/./spark-examples-1.4.0-hadoop1.2.1.jar
  [INFO ] [2015-07-02 20:16:32] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150702201026-0001/3/./spark-examples-1.4.0-hadoop1.2.1.jar to class loader
  [INFO ] [2015-07-02 20:16:34] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ][INFO ] [2015-07-02 20:16:45] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 354.7 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:16:47] [Logging$class:logInfo:59] Thread 52 spilling in-memory map of 348.2 MB to disk (1 time so far)
  B)
  [INFO ] [2015-07-02 20:16:35] [Logging$class:logInfo:59] Reading broadcast variable 1 took 915 ms
  [INFO ] [2015-07-02 20:16:35] [Logging$class:logInfo:59] ensureFreeSpace(4024) called with curMem=2291, maxMem=4829950771
  [INFO ] [2015-07-02 20:16:35] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 4.5 GB)
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:536870912+67108864
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:738197504+67108864
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:603979776+67108864
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:671088640+67108864
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] ensureFreeSpace(4294) called with curMem=6315, maxMem=4829950771
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 4.5 GB)
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] Reading broadcast variable 0 took 30 ms
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] ensureFreeSpace(71248) called with curMem=10609, maxMem=4829950771
  [INFO ] [2015-07-02 20:16:36] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 69.6 KB, free 4.5 GB)
  [WARN ] [2015-07-02 20:16:40] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-07-02 20:16:40] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-07-02 20:17:09] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:17:45] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 20:18:26] [Logging$class:logInfo:59] Thread 52 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 20:19:56] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 20:20:19] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 358.4 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:22:18] [Logging$class:logInfo:59] Finished task 0.0 in stage 0.0 (TID 0). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 20:22:18] [Logging$class:logInfo:59] Got assigned task 12
  [INFO ] [2015-07-02 20:22:18] [Logging$class:logInfo:59] Running task 12.0 in stage 0.0 (TID 12)
  [INFO ] [2015-07-02 20:22:18] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:805306368+67108864
  [INFO ] [2015-07-02 20:22:18] [Logging$class:logInfo:59] Finished task 1.0 in stage 0.0 (TID 1). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 20:22:18] [Logging$class:logInfo:59] Got assigned task 13
  [INFO ] [2015-07-02 20:22:18] [Logging$class:logInfo:59] Running task 13.0 in stage 0.0 (TID 13)
  [INFO ] [2015-07-02 20:22:18] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:872415232+67108864
  [INFO ] [2015-07-02 20:22:29] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 346.9 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:22:32] [Logging$class:logInfo:59] Finished task 2.0 in stage 0.0 (TID 2). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 20:22:32] [Logging$class:logInfo:59] Got assigned task 14
  [INFO ] [2015-07-02 20:22:32] [Logging$class:logInfo:59] Running task 14.0 in stage 0.0 (TID 14)
  [INFO ] [2015-07-02 20:22:32] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:939524096+67108864
  [INFO ] [2015-07-02 20:24:36] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150702201026-0001/1/./spark-examples-1.4.0-hadoop1.2.1.jar to class loader
  :59] Thread 52 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:24:56] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 354[INFO ] [2015-07-02 20:25:11] [Logging$class:logInfo:59] ensureFreeSpace(2291) called with curMem=0, maxMem=4829950771
  [INFO ] [2015-07-02 2[INFO ] [2015-07-02 20:25:35] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (1 time so far)
  NFO ] [2015-07-02 20:25:15] [Logging$class:logInfo:59] Reading broadcast variable 1 took 29319 ms
  [INFO ] [2015-07-02 20:26:48] [Logging$class:logInfo:59] Finished task 3.0 in stage 0.0 (TID 3). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 20:26:49] [Logging$class:logInfo:59] Got assigned task 15
  [INFO ] [2015-07-02 20:26:49] [Logging$class:logInfo:59] Running task 15.0 in stage 0.0 (TID 15)
  [INFO ] [2015-07-02 20:26:49] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:1006632960+67108864
  [INFO ] [2015-07-02 20:26:58] [Logging$class:logInfo:59] Finished task 13.0 in stage 0.0 (TID 13). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 20:26:58] [Logging$class:logInfo:59] Got assigned task 16
  [INFO ] [2015-07-02 20:26:58] [Logging$class:logInfo:59] Running task 16.0 in stage 0.0 (TID 16)
  [INFO ] [2015-07-02 20:26:58] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:1073741824+67108864
  [INFO ] [2015-07-02 20:27:21] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:28:22] [Logging$class:logInfo:59] ensureFreeSpace(4024) called with curMem=2291, maxMem=4829950771
  [INFO ] [2015-07-02 20:28:22] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 4.5 GB)
  [INFO ] [2015-07-02 20:28:45] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:402653184+67108864
  [INFO ] [2015-07-02 20:28:45] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:268435456+67108864
  [INFO ] [2015-07-02 20:28:45] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:335544320+67108864
  [INFO ] [2015-07-02 20:28:45] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:469762048+67108864
  [INFO ] [2015-07-02 20:28:45] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-07-02 20:30:26] [Logging$class:logInfo:59] ensureFreeSpace(4294) called with curMem=6315, maxMem=4829950771
  [INFO ] [2015-07-0[INFO ] [2015-07-02 20:30:31] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 355.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 20:30:51] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (2 times so far)
  39] [Logging$class:logInfo:59] ensureFreeSpace(71248) called with curMem=10609, maxMem=4829950771
  [INFO ] [2015-07-02 20:30:39] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 69.6 KB, free 4.5 GB)
  [WARN ] [2015-07-02 20:31:08] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-07-02 20:31:08] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-07-02 20:32:42] [Logging$class:logInfo:59] Finished task 12.0 in stage 0.0 (TID 12). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 20:32:42] [Logging$class:logInfo:59] Got assigned task 17
  [INFO ] [2015-07-02 20:32:42] [Logging$class:logInfo:59] Running task 17.0 in stage 0.0 (TID 17)
  [INFO ] [2015-07-02 20:32:42] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:1140850688+67108864
  [INFO ] [2015-07-02 20:32:59] [Logging$class:logInfo:59] Thread 52 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:33:25] [Logging$class:logInfo:59] Thread 52 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 20:34:15] [Logging$class:logInfo:59] Finished task 14.0 in stage 0.0 (TID 14). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 20:34:15] [Logging$class:logInfo:59] Got assigned task 18
  [INFO ] [2015-07-02 20:34:15] [Logging$class:logInfo:59] Running task 18.0 in stage 0.0 (TID 18)
  [INFO ] [2015-07-02 20:34:15] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:1207959552+67108864
  [INFO ] [2015-07-02 20:34:15] [Logging$class:logInfo:59] Finished task 16.0 in stage 0.0 (TID 16). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 20:34:16] [Logging$class:logInfo:59] Got assigned task 19
  [INFO ] [2015-07-02 20:34:16] [Logging$class:logInfo:59] Running task 19.0 in stage 0.0 (TID 19)
  [INFO ] [2015-07-02 20:34:16] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:1275068416+44213409
  [INFO ] [2015-07-02 20:34:35] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:34:36] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 346.2 MB to disk (1 time so far)
  utorLostFailure (executor 3 lost)
  [WARN ] [2015-07-02 20:34:33] [Logging$class:logWarning:71] Lost task 10.0 in stage 0.0 (TID 10, 10.0.0.42): ExecutorLostFailure (executor 3 lost)
  [WARN ] [2015-07-02 20:34:33] [Logging$class:logWarning:71] Lost task 9.0 in stage 0.0 (TID 9, 10.0.0.42): ExecutorLostFailure (executor 3 lost)
  [INFO ] [2015-07-02 20:34:35] [Logging$class:logInfo:59] Executor lost: 3 (epoch 0)
  [INFO ] [2015-07-02 20:34:36] [Logging$class:logInfo:59] Trying to remove executor 3 from BlockManagerMaster.
  [INFO ] [2015-07-02 20:34:37] [Logging$class:logInfo:59] Removing block manager BlockManagerId(3, 10.0.0.42, 53639)
  [INFO ] [2015-07-02 20:34:37] [Logging$class:logInfo:59] Removed 3 successfully in removeExecutor
  [WARN ] [2015-07-02 20:35:02] [Logging$class:logWarning:71] Told to re-register on heartbeat
  [INFO ] [2015-07-02 20:35:03] [Logging$class:logInfo:59] BlockManager re-registering with master
  [INFO ] [2015-07-02 20:35:03] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:35:03] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:35:04] [Logging$class:logInfo:59] Reporting 4 blocks to the master.
  35:07] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 20:35:07] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [WARN ] [2015-07-02 20:36:39] [Logging$class:logWarning:71] Removing worker-20150702200334-10.0.0.41-41593 because we got no heartbeat in 60 seconds
  [INFO ] [2015-07-02 20:36:45] [Logging$class:logInfo:59] Removing worker worker-20150702200334-10.0.0.41-41593 on 10.0.0.41:41593
  [INFO ] [2015-07-02 20:36:50] [Logging$class:logInfo:59] Telling app of lost executor: 2
  [INFO ] [2015-07-02 20:36:53] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/2 is now LOST (worker lost)
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Executor app-20150702201026-0001/2 removed: worker lost
  [ERROR] [2015-07-02 20:36:54] [Logging$class:logError:75] Lost executor 2 on 10.0.0.41: worker lost
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Re-queueing tasks for 2 from TaskSet 0.0
  [WARN ] [2015-07-02 20:36:54] [Logging$class:logWarning:71] Lost task 17.0 in stage 0.0 (TID 17, 10.0.0.41): ExecutorLostFailure (executor 2 lost)
  [WARN ] [2015-07-02 20:36:54] [Logging$class:logWarning:71] Lost task 19.0 in stage 0.0 (TID 19, 10.0.0.41): ExecutorLostFailure (executor 2 lost)
  [WARN ] [2015-07-02 20:36:54] [Logging$class:logWarning:71] Lost task 18.0 in stage 0.0 (TID 18, 10.0.0.41): ExecutorLostFailure (executor 2 lost)
  [WARN ] [2015-07-02 20:36:54] [Logging$class:logWarning:71] Lost task 15.0 in stage 0.0 (TID 15, 10.0.0.41): ExecutorLostFailure (executor 2 lost)
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 2), so marking it as still running
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 14), so marking it as still running
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 13), so marking it as still running
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 16), so marking it as still running
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 1), so marking it as still running
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 3), so marking it as still running
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 12), so marking it as still running
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 0), so marking it as still running
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Executor lost: 2 (epoch 3)
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Trying to remove executor 2 from BlockManagerMaster.
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Removing block manager BlockManagerId(2, 10.0.0.41, 36774)
  [INFO ] [2015-07-02 20:36:54] [Logging$class:logInfo:59] Removed 2 successfully in removeExecutor
  [INFO ] [2015-07-02 20:36:55] [Logging$class:logInfo:59] ShuffleMapStage 0 is now unavailable on executor 2 (0/20, false)
  [WARN ] [2015-07-02 20:36:57] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 20:36:59] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 20:36:59] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 20:36:59] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 20:36:59] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 20:36:59] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 20:36:59] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [INFO ] [2015-07-02 20:37:10] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:37:13] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 346.2 MB to disk (1 time so far)
  d worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 20:37:32] [Logging$class:logWarning:71] Told to re-register on heartbeat
  ker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [INFO ] [2015-07-02 20:37:38] [Logging$class:logInfo:59] BlockManager re-registering with master
  [INFO ] [2015-07-02 20:37:39] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:37:40] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:37:40] [Logging$class:logInfo:59] Reporting 4 blocks to the master.
  st_1_piece0 in memory on 10.0.0.41:36774 (size: 2.2 KB, free: 4.5 GB)
  [INFO ] [2015-07-02 20:37:47] [Logging$class:logInfo:59] Finished task 17.0 in stage 0.0 (TID 17). 1945 bytes result sent to driver
  WARN ] [2015-07-02 20:37:44] [Logging$class:logWarning:71] Ignored task status update (17 state FINISHED) from unknown executor $sender with ID $executorId
  [INFO ] [2015-07-02 20:37:44] [Logging$class:logInfo:59] Ignoring possibly bogus ShuffleMapTask completion from 2
  [INFO ] [2015-07-02 20:37:55] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:37:56] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-07-02 20:37:58] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 20:38:01] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  [INFO ] [2015-07-02 20:38:09] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/4 for PageRank
  [INFO ] [2015-07-02 20:38:15] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "4" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:38:45] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/4 because it is EXITED
  [INFO ] [2015-07-02 20:38:45] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/5 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:38:45] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/4 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:38:45] [Logging$class:logInfo:59] Executor app-20150702201026-0001/4 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:38:46] [Logging$class:logError:75] Asked to remove non-existent executor 4
  [INFO ] [2015-07-02 20:38:46] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/5 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:38:46] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/5 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:38:46] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/5 is now RUNNING
  [INFO ] [2015-07-02 20:38:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/4 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:38:50] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/5 for PageRank
  [INFO ] [2015-07-02 20:38:52] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "5" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:38:57] [Logging$class:logInfo:59] Executor app-20150702201026-0001/5 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:38:59] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/6 for PageRank
  [INFO ] [2015-07-02 20:39:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "6" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  ng$class:logInfo:59] Executor updated: app-20150702201026-0001/6 is now LOADING
  [INFO ] [2015-07-02 20:39:06] [Logging$class:logInfo:59] Executor app-20150702201026-0001/6 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:39:08] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/7 for PageRank
  [INFO ] [2015-07-02 20:39:11] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "7" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  ng$class:logInfo:59] Executor updated: app-20150702201026-0001/7 is now RUNNING
  [INFO ] [2015-07-02 20:39:13] [Logging$class:logInfo:59] Executor app-20150702201026-0001/7 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/8 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:39:10] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/7 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:39:10] [Logging$class:logInfo:59] Executor app-20150702201026-0001/7 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:39:10] [Logging$class:logError:75] Asked to remove non-existent executor 7
  [INFO ] [2015-07-02 20:39:10] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/8 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:39:10] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/8 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:39:10] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/8 is now RUNNING
  [INFO ] [2015-07-02 20:39:17] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/8 for PageRank
  [INFO ] [2015-07-02 20:39:19] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "8" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:39:21] [Logging$class:logInfo:59] Executor app-20150702201026-0001/8 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/9 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:39:17] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/8 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:39:17] [Logging$class:logInfo:59] Executor app-20150702201026-0001/8 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:39:17] [Logging$class:logError:75] Asked to remove non-existent executor 8
  [INFO ] [2015-07-02 20:39:17] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/9 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:39:17] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/9 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:39:17] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/9 is now RUNNING
  [INFO ] [2015-07-02 20:39:22] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/9 for PageRa[INFO ] [2015-07-02 20:39:25] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 353.9 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:39:26] [Logging$class:logInfo:59] Thread 57 spilling in-memory map of 346.2 MB to disk (1 time so far)
  anaged/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "9" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:39:25] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/9 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:39:25] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/9 because it is EXITED
  [INFO ] [2015-07-02 20:39:25] [Logging$class:logInfo:59] Executor app-20150702201026-0001/9 removed: Command exited with code 1
  [INFO ] [2015-07-02 20:39:25] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/10 on worker worker-20150702200334-10.0.0.41-41593
  [ERROR] [2015-07-02 20:39:25] [Logging$class:logError:75] Asked to remove non-existent executor 9
  [INFO ] [2015-07-02 20:39:25] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/10 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:39:25] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/10 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:39:25] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/10 is now RUNNING
  [INFO ] [2015-07-02 20:39:29] [Logging$class:logInfo:59] Executor app-20150702201026-0001/9 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:39:31] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/10 for PageRank
  [INFO ] [2015-07-02 20:39:34] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "10" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:39:36] [Logging$class:logInfo:59] Executor app-20150702201026-0001/10 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:39:37] [Logging$class:logInfo:59] Finished task 18.0 in stage 0.0 (TID 18). 1945 bytes result sent to driver
  th code 1)
  [INFO ] [2015-07-02 20:39:33] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/10 because it is EXITED
  [INFO ] [2015-07-02 20:39:33] [Logging$class:logInfo:59] Executor app-20150702201026-0001/10 removed: Command exited with code 1
  [INFO ] [2015-07-02 20:39:33] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/11 on worker worker-20150702200334-10.0.0.41-41593
  [ERROR] [2015-07-02 20:39:33] [Logging$class:logError:75] Asked to remove non-existent executor 10
  [INFO ] [2015-07-02 20:39:37] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/11 for PageRank
  -10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:39:33] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/11 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:39:33] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/11 is now RUNNING
  [WARN ] [2015-07-02 20:39:34] [Logging$class:logWarning:71] Ignored task status update (18 state FINISHED) from unknown executor $sender with ID $executorId
  [INFO ] [2015-07-02 20:39:34] [Logging$class:logInfo:59] Ignoring possibly bogus ShuffleMapTask completion from 2
  [INFO ] [2015-07-02 20:39:34] [Logging$class:logInfo:59] Finished task 18.0 in stage 0.0 (TID 18) in 323181 ms on 10.0.0.41 (2/20)
  [INFO ] [2015-07-02 20:39:36] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/11 is now LOADING
  [INFO ] [2015-07-02 20:39:42] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "11" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:39:43] [Logging$class:logInfo:59] Finished task 19.0 in stage 0.0 (TID 19). 1945 bytes result sent to driver
  er with ID $executorId
  [INFO ] [2015-07-02 20:39:39] [Logging$class:logInfo:59] Ignoring possibly bogus ShuffleMapTask completion from 2
  [INFO ] [2015-07-02 20:39:39] [Logging$class:logInfo:59] Finished task 19.0 in stage 0.0 (TID 19) in 327809 ms on 10.0.0.41 (3/20)
  [INFO ] [2015-07-02 20:39:45] [Logging$class:logInfo:59] Executor app-20150702201026-0001/11 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/12 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:39:42] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/11 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:39:42] [Logging$class:logInfo:59] Executor app-20150702201026-0001/11 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:39:42] [Logging$class:logError:75] Asked to remove non-existent executor 11
  [INFO ] [2015-07-02 20:39:42] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/12 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:39:42] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/12 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:39:42] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/12 is now RUNNING
  [INFO ] [2015-07-02 20:39:49] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/12 for PageRank
  [INFO ] [2015-07-02 20:39:50] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "12" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:39:46] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/12 is now LOADING
  [INFO ] [2015-07-02 20:39:53] [Logging$class:logInfo:59] Executor app-20150702201026-0001/12 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:39:54] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/13 for PageRank
  -02 20:39:49] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/12 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:39:49] [Logging$class:logInfo:59] Executor app-20150702201026-0001/12 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:39:49] [Logging$class:logError:75] Asked to remove non-existent executor 12
  [INFO ] [2015-07-02 20:39:49] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/13 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:39:49] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/13 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:39:50] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/13 is now RUNNING
  [INFO ] [2015-07-02 20:39:50] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/13 is now LOADING
  [INFO ] [2015-07-02 20:39:56] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "13" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:40:02] [Logging$class:logInfo:59] Executor app-20150702201026-0001/13 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/14 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:39:59] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/13 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:39:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/13 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:39:59] [Logging$class:logError:75] Asked to remove non-existent executor 13
  [INFO ] [2015-07-02 20:39:59] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/14 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:39:59] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/14 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:40:00] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/14 is now RUNNING
  [INFO ] [2015-07-02 20:40:07] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/14 for PageRank
  [INFO ] [2015-07-02 20:40:09] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "14" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:40:13] [Logging$class:logInfo:59] Executor app-20150702201026-0001/14 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:40:14] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/15 for PageRank
  -02 20:40:10] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/14 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:40:10] [Logging$class:logInfo:59] Executor app-20150702201026-0001/14 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:40:10] [Logging$class:logError:75] Asked to remove non-existent executor 14
  [INFO ] [2015-07-02 20:40:10] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/15 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:40:10] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/15 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:40:10] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/15 is now RUNNING
  [INFO ] [2015-07-02 20:40:15] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "15" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:40:17] [Logging$class:logInfo:59] Executor app-20150702201026-0001/15 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:40:18] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/16 for PageRank
  [INFO ] [2015-07-02 20:40:19] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "16" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/16 is now LOADING
  [INFO ] [2015-07-02 20:40:21] [Logging$class:logInfo:59] Executor app-20150702201026-0001/16 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:40:21] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/17 for PageRank
  -02 20:40:17] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/16 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:40:17] [Logging$class:logInfo:59] Executor app-20150702201026-0001/16 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:40:17] [Logging$class:logError:75] Asked to remove non-existent executor 16
  [INFO ] [2015-07-02 20:40:17] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/17 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:40:17] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/17 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:40:17] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/17 is now RUNNING
  [INFO ] [2015-07-02 20:40:22] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "17" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:40:25] [Logging$class:logInfo:59] Executor app-20150702201026-0001/17 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/18 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:40:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/17 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:40:22] [Logging$class:logInfo:59] Executor app-20150702201026-0001/17 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:40:22] [Logging$class:logError:75] Asked to remove non-existent executor 17
  [INFO ] [2015-07-02 20:40:22] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/18 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:40:22] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/18 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:40:26] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/18 for PageRank
  [INFO ] [2015-07-02 20:40:27] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "18" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:40:29] [Logging$class:logInfo:59] Executor app-20150702201026-0001/18 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:40:30] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/19 for PageRank
  [INFO ] [2015-07-02 20:40:30] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "19" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/19 is now LOADING
  [INFO ] [2015-07-02 20:40:33] [Logging$class:logInfo:59] Executor app-20150702201026-0001/19 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:40:33] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/20 for PageRank
  [INFO ] [2015-07-02 20:40:33] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "20" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/20 is now LOADING
  [INFO ] [2015-07-02 20:40:34] [Logging$class:logInfo:59] Executor app-20150702201026-0001/20 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:40:34] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/21 for PageRank
  [INFO ] [2015-07-02 20:40:36] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "21" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/21 is now LOADING
  [INFO ] [2015-07-02 20:40:37] [Logging$class:logInfo:59] Executor app-20150702201026-0001/21 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:40:39] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/22 for PageRank
  -02 20:40:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/21 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:40:35] [Logging$class:logInfo:59] Executor app-20150702201026-0001/21 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:40:35] [Logging$class:logError:75] Asked to remove non-existent executor 21
  [INFO ] [2015-07-02 20:40:35] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/22 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:40:35] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/22 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:40:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/22 is now LOADING
  [INFO ] [2015-07-02 20:40:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/22 is now RUNNING
  [INFO ] [2015-07-02 20:40:43] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "22" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:40:46] [Logging$class:logInfo:59] Executor app-20150702201026-0001/22 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:40:46] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/23 for PageRank
  -02 20:40:41] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/22 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:40:41] [Logging$class:logInfo:59] Executor app-20150702201026-0001/22 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:40:41] [Logging$class:logError:75] Asked to remove non-existent executor 22
  [INFO ] [2015-07-02 20:40:41] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/23 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:40:41] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/23 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:40:42] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/23 is now RUNNING
  [INFO ] [2015-07-02 20:40:44] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/23 is now LOADING
  [INFO ] [2015-07-02 20:40:49] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "23" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:40:49] [Logging$class:logInfo:59] Executor app-20150702201026-0001/23 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/24 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:40:46] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/23 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:40:46] [Logging$class:logInfo:59] Executor app-20150702201026-0001/23 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:40:46] [Logging$class:logError:75] Asked to remove non-existent executor 23
  [INFO ] [2015-07-02 20:40:46] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/24 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:40:46] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/24 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:40:46] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/24 is now RUNNING
  [INFO ] [2015-07-02 20:40:53] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/24 for PageRank
  [INFO ] [2015-07-02 20:40:53] [Logging$class:logInfo:59] Finished task 15.0 in stage 0.0 (TID 15). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 20:40:55] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "24" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:40:53] [Logging$class:logInfo:59] Ignoring possibly bogus ShuffleMapTask completion from 2
  [INFO ] [2015-07-02 20:40:53] [Logging$class:logInfo:59] Finished task 15.0 in stage 0.0 (TID 15) in 848528 ms on 10.0.0.41 (4/20)
  [INFO ] [2015-07-02 20:40:53] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/24 is now LOADING
  [INFO ] [2015-07-02 20:40:58] [Logging$class:logInfo:59] Executor app-20150702201026-0001/24 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:40:59] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/25 for PageRank
  [INFO ] [2015-07-02 20:41:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "25" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:41:02] [Logging$class:logInfo:59] Executor app-20150702201026-0001/25 finished with state EXITED message Command exited with code 1 exitStatus 1
  o:59] Removing executor app-20150702201026-0001/25 because it is EXITED
  [INFO ] [2015-07-02 20:41:01] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/26 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:41:01] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/25 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:41:01] [Logging$class:logInfo:59] Executor app-20150702201026-0001/25 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:41:01] [Logging$class:logError:75] Asked to remove non-existent executor 25
  [INFO ] [2015-07-02 20:41:01] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/26 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:41:01] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/26 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:41:02] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/26 is now RUNNING
  [INFO ] [2015-07-02 20:41:06] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/26 for PageRank
  [INFO ] [2015-07-02 20:41:08] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "26" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:41:09] [Logging$class:logInfo:59] Executor app-20150702201026-0001/26 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:41:11] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/27 for PageRank
  -02 20:41:06] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/26 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:41:06] [Logging$class:logInfo:59] Executor app-20150702201026-0001/26 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:41:07] [Logging$class:logError:75] Asked to remove non-existent executor 26
  [INFO ] [2015-07-02 20:41:07] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/27 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:41:07] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/27 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:41:07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/27 is now RUNNING
  [INFO ] [2015-07-02 20:41:07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/27 is now LOADING
  [INFO ] [2015-07-02 20:41:12] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "27" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:41:39] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/27 because it is EXITED
  [INFO ] [2015-07-02 20:41:39] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/28 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:41:39] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/27 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:41:39] [Logging$class:logInfo:59] Executor app-20150702201026-0001/27 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:41:39] [Logging$class:logError:75] Asked to remove non-existent executor 27
  [INFO ] [2015-07-02 20:41:39] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/28 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:41:39] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/28 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:41:39] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/28 is now RUNNING
  [INFO ] [2015-07-02 20:41:43] [Logging$class:logInfo:59] Executor app-20150702201026-0001/27 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:41:44] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/28 for PageRank
  [INFO ] [2015-07-02 20:41:45] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "28" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:41:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/28 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:41:53] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/29 for PageRank
  [INFO ] [2015-07-02 20:41:56] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "29" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/29 is now LOADING
  [INFO ] [2015-07-02 20:41:58] [Logging$class:logInfo:59] Executor app-20150702201026-0001/29 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:41:59] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/30 for PageRank
  [INFO ] [2015-07-02 20:41:59] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "30" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/30 is now LOADING
  [INFO ] [2015-07-02 20:42:01] [Logging$class:logInfo:59] Executor app-20150702201026-0001/30 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/31 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:41:57] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/30 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:41:57] [Logging$class:logInfo:59] Executor app-20150702201026-0001/30 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:41:57] [Logging$class:logError:75] Asked to remove non-existent executor 30
  [INFO ] [2015-07-02 20:41:57] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/31 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:41:57] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/31 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:41:57] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/31 is now RUNNING
  [INFO ] [2015-07-02 20:42:02] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/31 for PageRank
  [INFO ] [2015-07-02 20:42:05] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "31" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:42:07] [Logging$class:logInfo:59] Executor app-20150702201026-0001/31 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:08] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/32 for PageRank
  -02 20:42:03] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/31 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:42:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/31 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:42:03] [Logging$class:logError:75] Asked to remove non-existent executor 31
  [INFO ] [2015-07-02 20:42:03] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/32 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:42:03] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/32 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:42:03] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/32 is now RUNNING
  [INFO ] [2015-07-02 20:42:04] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/32 is now LOADING
  [INFO ] [2015-07-02 20:42:10] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "32" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:42:12] [Logging$class:logInfo:59] Executor app-20150702201026-0001/32 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:13] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/33 for PageRank
  [INFO ] [2015-07-02 20:42:14] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "33" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/33 is now LOADING
  [INFO ] [2015-07-02 20:42:16] [Logging$class:logInfo:59] Executor app-20150702201026-0001/33 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:12] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/33 because it is EXITED
  [INFO ] [2015-07-02 20:42:17] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/34 for PageRank
  [INFO ] [2015-07-02 20:42:18] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "34" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  now RUNNING
  [INFO ] [2015-07-02 20:42:14] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/34 is now LOADING
  [INFO ] [2015-07-02 20:42:20] [Logging$class:logInfo:59] Executor app-20150702201026-0001/34 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:20] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/35 for PageRank
  [INFO ] [2015-07-02 20:42:20] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "35" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/35 is now LOADING
  [INFO ] [2015-07-02 20:42:22] [Logging$class:logInfo:59] Executor app-20150702201026-0001/35 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:23] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/36 for PageRank
  [INFO ] [2015-07-02 20:42:23] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "36" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/36 is now LOADING
  [INFO ] [2015-07-02 20:42:24] [Logging$class:logInfo:59] Executor app-20150702201026-0001/36 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:25] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/37 for PageRank
  -02 20:42:20] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/36 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:42:20] [Logging$class:logInfo:59] Executor app-20150702201026-0001/36 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:42:20] [Logging$class:logError:75] Asked to remove non-existent executor 36
  [INFO ] [2015-07-02 20:42:20] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/37 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:42:20] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/37 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:42:20] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/37 is now RUNNING
  [INFO ] [2015-07-02 20:42:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/37 is now LOADING
  [INFO ] [2015-07-02 20:42:26] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "37" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:42:28] [Logging$class:logInfo:59] Executor app-20150702201026-0001/37 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:28] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/38 for PageRank
  [INFO ] [2015-07-02 20:42:29] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "38" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/38 is now LOADING
  [INFO ] [2015-07-02 20:42:31] [Logging$class:logInfo:59] Executor app-20150702201026-0001/38 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/39 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:42:28] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/38 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:42:28] [Logging$class:logInfo:59] Executor app-20150702201026-0001/38 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:42:28] [Logging$class:logError:75] Asked to remove non-existent executor 38
  [INFO ] [2015-07-02 20:42:28] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/39 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:42:28] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/39 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:42:33] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/39 for PageRank
  [INFO ] [2015-07-02 20:42:34] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "39" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:42:35] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 366.7 MB to disk (2 times so far)
  ommand exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:36] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/40 for PageRank
  [INFO ] [2015-07-02 20:42:37] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "40" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/40 is now LOADING
  [INFO ] [2015-07-02 20:42:38] [Logging$class:logInfo:59] Executor app-20150702201026-0001/40 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:39] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/41 for PageRank
  [INFO ] [2015-07-02 20:42:39] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "41" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/41 is now LOADING
  [INFO ] [2015-07-02 20:42:41] [Logging$class:logInfo:59] Executor app-20150702201026-0001/41 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:41] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/42 for PageRank
  -02 20:42:37] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/41 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:42:37] [Logging$class:logInfo:59] Executor app-20150702201026-0001/41 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:42:37] [Logging$class:logError:75] Asked to remove non-existent executor 41
  [INFO ] [2015-07-02 20:42:37] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/42 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:42:37] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/42 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:42:37] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/42 is now RUNNING
  [INFO ] [2015-07-02 20:42:38] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/42 is now LOADING
  [INFO ] [2015-07-02 20:42:43] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "42" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:42:47] [Logging$class:logInfo:59] Executor app-20150702201026-0001/42 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:49] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/43 for PageRank
  [INFO ] [2015-07-02 20:42:50] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "43" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:42:47] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/43 is now LOADING
  [INFO ] [2015-07-02 20:42:54] [Logging$class:logInfo:59] Executor app-20150702201026-0001/43 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:42:56] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/44 for PageRank
  [INFO ] [2015-07-02 20:42:57] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "44" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [ERROR] [2015-07-02 20:42:52] [Logging$class:logError:75] Asked to remove non-existent executor 43
  [INFO ] [2015-07-02 20:42:53] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/44 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:42:53] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/44 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:42:53] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/44 is now LOADING
  [INFO ] [2015-07-02 20:42:53] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/44 is now RUNNING
  [INFO ] [2015-07-02 20:42:58] [Logging$class:logInfo:59] Executor app-20150702201026-0001/44 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:43:00] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/45 for PageRank
  [INFO ] [2015-07-02 20:43:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "45" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:43:01] [Logging$class:logInfo:59] Executor app-20150702201026-0001/45 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:43:01] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/46 for PageRank
  [INFO ] [2015-07-02 20:43:01] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "46" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  or ID app-20150702201026-0001/46 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:42:57] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/46 is now RUNNING
  [INFO ] [2015-07-02 20:42:57] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/46 is now LOADING
  [INFO ] [2015-07-02 20:43:05] [Logging$class:logInfo:59] Executor app-20150702201026-0001/46 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:43:06] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/47 for PageRank
  [INFO ] [2015-07-02 20:43:08] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "47" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/47 is now LOADING
  [INFO ] [2015-07-02 20:43:19] [Logging$class:logInfo:59] Executor app-20150702201026-0001/47 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:43:22] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/48 for PageRank
  [INFO ] [2015-07-02 20:43:25] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "48" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/48 is now RUNNING
  [INFO ] [2015-07-02 20:43:42] [Logging$class:logInfo:59] Executor app-20150702201026-0001/48 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:43:43] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/49 for PageRank
  [INFO ] [2015-07-02 20:43:43] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "49" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/49 is now LOADING
  [INFO ] [2015-07-02 20:43:44] [Logging$class:logInfo:59] Executor app-20150702201026-0001/49 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:43:45] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/50 for PageRank
  [INFO ] [2015-07-02 20:43:45] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "50" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/50 is now LOADING
  [INFO ] [2015-07-02 20:43:46] [Logging$class:logInfo:59] Executor app-20150702201026-0001/50 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:43:47] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/51 for PageRank
  [INFO ] [2015-07-02 20:43:48] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "51" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/51 is now RUNNING
  [INFO ] [2015-07-02 20:43:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/51 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/52 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:43:47] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/51 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:43:47] [Logging$class:logInfo:59] Executor app-20150702201026-0001/51 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:43:47] [Logging$class:logError:75] Asked to remove non-existent executor 51
  [INFO ] [2015-07-02 20:43:47] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/52 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:43:47] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/52 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:43:47] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/52 is now RUNNING
  [INFO ] [2015-07-02 20:43:52] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/52 for PageRank
  [INFO ] [2015-07-02 20:43:53] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "52" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:43:55] [Logging$class:logInfo:59] Executor app-20150702201026-0001/52 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:43:55] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/53 for PageRank
  [INFO ] [2015-07-02 20:43:57] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "53" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:43:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/53 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:43:59] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/54 for PageRank
  [INFO ] [2015-07-02 20:43:59] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "54" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:43:56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/54 is now RUNNING
  [INFO ] [2015-07-02 20:43:56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/54 is now LOADING
  [INFO ] [2015-07-02 20:44:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/54 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:03] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/55 for PageRank
  [INFO ] [2015-07-02 20:44:04] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "55" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/55 is now LOADING
  [INFO ] [2015-07-02 20:44:07] [Logging$class:logInfo:59] Executor app-20150702201026-0001/55 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:08] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/56 for PageRank
  [INFO ] [2015-07-02 20:44:08] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "56" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/56 is now LOADING
  [INFO ] [2015-07-02 20:44:10] [Logging$class:logInfo:59] Executor app-20150702201026-0001/56 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:10] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/57 for PageRank
  [INFO ] [2015-07-02 20:44:10] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "57" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/57 is now LOADING
  [INFO ] [2015-07-02 20:44:11] [Logging$class:logInfo:59] Executor app-20150702201026-0001/57 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:12] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/58 for PageRank
  [INFO ] [2015-07-02 20:44:13] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "58" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:44:15] [Logging$class:logInfo:59] Executor app-20150702201026-0001/58 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:15] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/59 for PageRank
  [INFO ] [2015-07-02 20:44:15] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "59" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/59 is now RUNNING
  [INFO ] [2015-07-02 20:44:12] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/59 is now LOADING
  [INFO ] [2015-07-02 20:44:17] [Logging$class:logInfo:59] Executor app-20150702201026-0001/59 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:18] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/60 for PageRank
  [INFO ] [2015-07-02 20:44:18] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "60" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/60 is now LOADING
  [INFO ] [2015-07-02 20:44:19] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 346.2 MB to disk (2 times so far)
  ommand exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:19] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/61 for PageRank
  [INFO ] [2015-07-02 20:44:20] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "61" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/61 is now LOADING
  [INFO ] [2015-07-02 20:44:20] [Logging$class:logInfo:59] Executor app-20150702201026-0001/61 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:21] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/62 for PageRank
  [INFO ] [2015-07-02 20:44:21] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "62" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/62 is now LOADING
  [INFO ] [2015-07-02 20:44:22] [Logging$class:logInfo:59] Executor app-20150702201026-0001/62 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:22] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/63 for PageRank
  [INFO ] [2015-07-02 20:44:24] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "63" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/63 is now RUNNING
  [INFO ] [2015-07-02 20:44:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/63 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:27] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/64 for PageRank
  [INFO ] [2015-07-02 20:44:27] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "64" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/64 is now LOADING
  [INFO ] [2015-07-02 20:44:28] [Logging$class:logInfo:59] Executor app-20150702201026-0001/64 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:28] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/65 for PageRank
  -02 20:44:24] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/64 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:44:24] [Logging$class:logInfo:59] Executor app-20150702201026-0001/64 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:44:24] [Logging$class:logError:75] Asked to remove non-existent executor 64
  [INFO ] [2015-07-02 20:44:24] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/65 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:44:24] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/65 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:44:24] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/65 is now RUNNING
  [INFO ] [2015-07-02 20:44:24] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/65 is now LOADING
  [INFO ] [2015-07-02 20:44:29] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "65" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:44:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/65 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:30] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/66 for PageRank
  [INFO ] [2015-07-02 20:44:31] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "66" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/66 is now LOADING
  [INFO ] [2015-07-02 20:44:31] [Logging$class:logInfo:59] Executor app-20150702201026-0001/66 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:32] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/67 for PageRank
  [INFO ] [2015-07-02 20:44:33] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "67" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:44:28] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/67 is now LOADING
  [INFO ] [2015-07-02 20:44:34] [Logging$class:logInfo:59] Executor app-20150702201026-0001/67 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:35] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/68 for PageRank
  [INFO ] [2015-07-02 20:44:35] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "68" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/68 is now LOADING
  [INFO ] [2015-07-02 20:44:36] [Logging$class:logInfo:59] Executor app-20150702201026-0001/68 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:44:38] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/69 for PageRank
  [INFO ] [2015-07-02 20:44:38] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "69" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:44:34] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/69 is now LOADING
  [INFO ] [2015-07-02 20:45:05] [Logging$class:logInfo:59] Executor app-20150702201026-0001/69 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/70 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:45:05] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/70 for PageRank
   exited with code 1)
  [INFO ] [2015-07-02 20:45:01] [Logging$class:logInfo:59] Executor app-20150702201026-0001/69 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:45:01] [Logging$class:logError:75] Asked to remove non-existent executor 69
  [INFO ] [2015-07-02 20:45:01] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/70 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:45:01] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/70 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:45:01] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/70 is now RUNNING
  [INFO ] [2015-07-02 20:45:01] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/70 is now LOADING
  [INFO ] [2015-07-02 20:45:05] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "70" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:45:12] [Logging$class:logInfo:59] Executor app-20150702201026-0001/70 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:16] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/71 for PageRank
  [INFO ] [2015-07-02 20:45:16] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "71" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/71 is now LOADING
  [INFO ] [2015-07-02 20:45:18] [Logging$class:logInfo:59] Executor app-20150702201026-0001/71 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:19] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/72 for PageRank
  [INFO ] [2015-07-02 20:45:19] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "72" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/72 is now LOADING
  [INFO ] [2015-07-02 20:45:20] [Logging$class:logInfo:59] Executor app-20150702201026-0001/72 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:22] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/73 for PageRank
  [INFO ] [2015-07-02 20:45:22] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "73" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/73 is now LOADING
  [INFO ] [2015-07-02 20:45:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/73 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:26] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/74 for PageRank
  [INFO ] [2015-07-02 20:45:26] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "74" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:45:22] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/74 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:45:22] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/74 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:45:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/74 is now LOADING
  [INFO ] [2015-07-02 20:45:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/74 is now RUNNING
  [INFO ] [2015-07-02 20:45:32] [Logging$class:logInfo:59] Executor app-20150702201026-0001/74 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:32] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/75 for PageRank
  [INFO ] [2015-07-02 20:45:32] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "75" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:45:35] [Logging$class:logInfo:59] Executor app-20150702201026-0001/75 finished with state EXITED message Command exited with code 1 exitStatus 1
  o:59] Removing executor app-20150702201026-0001/75 because it is EXITED
  [INFO ] [2015-07-02 20:45:31] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/76 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:45:31] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/75 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:45:31] [Logging$class:logInfo:59] Executor app-20150702201026-0001/75 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:45:31] [Logging$class:logError:75] Asked to remove non-existent executor 75
  [INFO ] [2015-07-02 20:45:31] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/76 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:45:31] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/76 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:45:35] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/76 for PageRank
  [INFO ] [2015-07-02 20:45:35] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "76" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:45:36] [Logging$class:logInfo:59] Executor app-20150702201026-0001/76 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:37] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/77 for PageRank
  [INFO ] [2015-07-02 20:45:37] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "77" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/77 is now RUNNING
  [INFO ] [2015-07-02 20:45:38] [Logging$class:logInfo:59] Executor app-20150702201026-0001/77 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:39] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/78 for PageRank
  [INFO ] [2015-07-02 20:45:39] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "78" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:45:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/78 is now LOADING
  [INFO ] [2015-07-02 20:45:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/78 is now RUNNING
  [INFO ] [2015-07-02 20:45:41] [Logging$class:logInfo:59] Executor app-20150702201026-0001/78 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:42] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/79 for PageRank
  -02 20:45:37] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/78 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:45:37] [Logging$class:logInfo:59] Executor app-20150702201026-0001/78 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:45:37] [Logging$class:logError:75] Asked to remove non-existent executor 78
  [INFO ] [2015-07-02 20:45:42] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "79" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:45:45] [Logging$class:logInfo:59] Executor app-20150702201026-0001/79 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:46] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/80 for PageRank
  [INFO ] [2015-07-02 20:45:46] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "80" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:45:42] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/80 is now RUNNING
  [INFO ] [2015-07-02 20:45:48] [Logging$class:logInfo:59] Executor app-20150702201026-0001/80 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:48] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/81 for PageRank
  [INFO ] [2015-07-02 20:45:48] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "81" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [ERROR] [2015-07-02 20:45:44] [Logging$class:logError:75] Asked to remove non-existent executor 80
  [INFO ] [2015-07-02 20:45:44] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/81 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:45:44] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/81 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:45:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/81 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:51] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/82 for PageRank
  [INFO ] [2015-07-02 20:45:51] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "82" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  or ID app-20150702201026-0001/82 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:45:55] [Logging$class:logInfo:59] Executor app-20150702201026-0001/82 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:45:55] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/83 for PageRank
  [INFO ] [2015-07-02 20:45:55] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "83" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  or ID app-20150702201026-0001/83 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:45:58] [Logging$class:logInfo:59] Executor app-20150702201026-0001/83 finished with state EXITED message Command exited with code 1 exitStatus 1
  o:59] Executor updated: app-20150702201026-0001/83 is now RUNNING
  [INFO ] [2015-07-02 20:45:54] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/83 because it is EXITED
  [INFO ] [2015-07-02 20:45:54] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/84 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:45:55] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/83 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:45:55] [Logging$class:logInfo:59] Executor app-20150702201026-0001/83 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:45:55] [Logging$class:logError:75] Asked to remove non-existent executor 83
  [INFO ] [2015-07-02 20:45:59] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/84 for PageRank
  -10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:45:55] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/84 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:46:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "84" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:46:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/84 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:46:06] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/85 for PageRank
  -02 20:45:59] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/84 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:45:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/84 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:45:59] [Logging$class:logError:75] Asked to remove non-existent executor 84
  [INFO ] [2015-07-02 20:45:59] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/85 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:45:59] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/85 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:46:00] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/85 is now RUNNING
  [INFO ] [2015-07-02 20:46:06] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "85" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:46:10] [Logging$class:logInfo:59] Executor app-20150702201026-0001/85 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:46:11] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/86 for PageRank
  -02 20:46:06] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/85 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:46:06] [Logging$class:logInfo:59] Executor app-20150702201026-0001/85 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:46:07] [Logging$class:logError:75] Asked to remove non-existent executor 85
  [INFO ] [2015-07-02 20:46:07] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/86 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:46:07] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/86 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:46:07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/86 is now LOADING
  [INFO ] [2015-07-02 20:46:07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/86 is now RUNNING
  [INFO ] [2015-07-02 20:46:12] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "86" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:46:13] [Logging$class:logInfo:59] Executor app-20150702201026-0001/86 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:46:16] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/87 for PageRank
  -02 20:46:10] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/86 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:46:10] [Logging$class:logInfo:59] Executor app-20150702201026-0001/86 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:46:10] [Logging$class:logError:75] Asked to remove non-existent executor 86
  [INFO ] [2015-07-02 20:46:10] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/87 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:46:10] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/87 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:46:10] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/87 is now RUNNING
  [INFO ] [2015-07-02 20:46:12] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/87 is now LOADING
  [INFO ] [2015-07-02 20:46:17] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "87" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:46:21] [Logging$class:logInfo:59] Executor app-20150702201026-0001/87 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:46:22] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/88 for PageRank
  [INFO ] [2015-07-02 20:46:23] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "88" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:46:20] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/88 is now LOADING
  [INFO ] [2015-07-02 20:46:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/88 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:46:28] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/89 for PageRank
  [INFO ] [2015-07-02 20:46:30] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "89" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  Logging$class:logWarning:71] Removing executor 3 with no recent heartbeats: 122123 ms exceeds timeout 120000 ms
  [ERROR] [2015-07-02 20:46:26] [Logging$class:logError:75] Lost executor 3 on 10.0.0.42: Executor heartbeat timed out after 122123 ms
  [INFO ] [2015-07-02 20:46:26] [Logging$class:logInfo:59] Re-queueing tasks for 3 from TaskSet 0.0
  [INFO ] [2015-07-02 20:46:26] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/89 is now LOADING
  [INFO ] [2015-07-02 20:46:26] [Logging$class:logInfo:59] Executor lost: 3 (epoch 6)
  [INFO ] [2015-07-02 20:46:26] [Logging$class:logInfo:59] Trying to remove executor 3 from BlockManagerMaster.
  [INFO ] [2015-07-02 20:46:26] [Logging$class:logInfo:59] Removing block manager BlockManagerId(3, 10.0.0.42, 53639)
  [INFO ] [2015-07-02 20:46:26] [Logging$class:logInfo:59] Removed 3 successfully in removeExecutor
  [INFO ] [2015-07-02 20:46:27] [Logging$class:logInfo:59] Host added was in lost list earlier: 10.0.0.42
  [INFO ] [2015-07-02 20:46:34] [Logging$class:logInfo:59] Executor app-20150702201026-0001/89 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/90 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:46:30] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/89 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:46:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/89 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:46:30] [Logging$class:logError:75] Asked to remove non-existent executor 89
  [INFO ] [2015-07-02 20:46:30] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/90 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:46:30] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/90 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:46:30] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/90 is now RUNNING
  [INFO ] [2015-07-02 20:46:36] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/90 for PageRank
  [INFO ] [2015-07-02 20:46:37] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "90" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:46:33] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/90 is now LOADING
  [INFO ] [2015-07-02 20:46:39] [Logging$class:logInfo:59] Executor app-20150702201026-0001/90 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/91 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:46:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/90 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:46:35] [Logging$class:logInfo:59] Executor app-20150702201026-0001/90 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:46:35] [Logging$class:logError:75] Asked to remove non-existent executor 90
  [INFO ] [2015-07-02 20:46:35] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/91 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:46:35] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/91 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:46:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/91 is now RUNNING
  [INFO ] [2015-07-02 20:46:39] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/91 for PageRank
  [INFO ] [2015-07-02 20:46:43] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "91" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:46:46] [Logging$class:logInfo:59] Executor app-20150702201026-0001/91 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/92 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:46:43] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/91 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:46:43] [Logging$class:logInfo:59] Executor app-20150702201026-0001/91 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:46:43] [Logging$class:logError:75] Asked to remove non-existent executor 91
  [INFO ] [2015-07-02 20:46:43] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/92 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:46:43] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/92 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:46:43] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/92 is now RUNNING
  [INFO ] [2015-07-02 20:46:50] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/92 for PageRank
  [INFO ] [2015-07-02 20:46:51] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "92" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:46:50] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/92 because it is EXITED
  [INFO ] [2015-07-02 20:46:50] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/93 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:46:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/92 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:46:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/92 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:46:51] [Logging$class:logError:75] Asked to remove non-existent executor 92
  [INFO ] [2015-07-02 20:46:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/93 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:46:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/93 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:46:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/93 is now RUNNING
  [INFO ] [2015-07-02 20:46:54] [Logging$class:logInfo:59] Executor app-20150702201026-0001/92 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:46:56] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/93 for PageRank
  [INFO ] [2015-07-02 20:46:57] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "93" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:46:53] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/93 is now LOADING
  [INFO ] [2015-07-02 20:47:00] [Logging$class:logInfo:59] Executor app-20150702201026-0001/93 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/94 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:46:56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/93 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:46:56] [Logging$class:logInfo:59] Executor app-20150702201026-0001/93 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:46:56] [Logging$class:logError:75] Asked to remove non-existent executor 93
  [INFO ] [2015-07-02 20:46:56] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/94 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:46:56] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/94 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:46:56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/94 is now RUNNING
  [INFO ] [2015-07-02 20:47:03] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/94 for PageRank
  [INFO ] [2015-07-02 20:47:06] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "94" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:47:08] [Logging$class:logInfo:59] Executor app-20150702201026-0001/94 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/95 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:47:11] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/95 for PageRank
   exited with code 1)
  [INFO ] [2015-07-02 20:47:07] [Logging$class:logInfo:59] Executor app-20150702201026-0001/94 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:47:07] [Logging$class:logError:75] Asked to remove non-existent executor 94
  [INFO ] [2015-07-02 20:47:07] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/95 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:47:07] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/95 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:47:07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/95 is now RUNNING
  [INFO ] [2015-07-02 20:47:13] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "95" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:47:15] [Logging$class:logInfo:59] Executor app-20150702201026-0001/95 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:47:16] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/96 for PageRank
  fo:59] Launching executor app-20150702201026-0001/96 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:47:11] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/95 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:47:11] [Logging$class:logInfo:59] Executor app-20150702201026-0001/95 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:47:11] [Logging$class:logError:75] Asked to remove non-existent executor 95
  [INFO ] [2015-07-02 20:47:11] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/96 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:47:11] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/96 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:47:11] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/96 is now RUNNING
  [INFO ] [2015-07-02 20:47:12] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/96 is now LOADING
  [INFO ] [2015-07-02 20:47:18] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "96" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:47:21] [Logging$class:logInfo:59] Executor app-20150702201026-0001/96 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/97 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:47:23] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/97 for PageRank
  [INFO ] [2015-07-02 20:47:25] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "97" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:47:28] [Logging$class:logInfo:59] Executor app-20150702201026-0001/97 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/98 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:47:24] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/97 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:47:24] [Logging$class:logInfo:59] Executor app-20150702201026-0001/97 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:47:24] [Logging$class:logError:75] Asked to remove non-existent executor 97
  [INFO ] [2015-07-02 20:47:24] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/98 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:47:24] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/98 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:47:24] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/98 is now RUNNING
  [INFO ] [2015-07-02 20:47:29] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/98 for PageRank
  [INFO ] [2015-07-02 20:47:31] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "98" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:47:33] [Logging$class:logInfo:59] Executor app-20150702201026-0001/98 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:47:33] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/99 for PageRank
  -02 20:47:29] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/98 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:47:29] [Logging$class:logInfo:59] Executor app-20150702201026-0001/98 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:47:29] [Logging$class:logError:75] Asked to remove non-existent executor 98
  [INFO ] [2015-07-02 20:47:29] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/99 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:47:29] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/99 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:47:29] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/99 is now RUNNING
  [INFO ] [2015-07-02 20:47:29] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/99 is now LOADING
  [INFO ] [2015-07-02 20:47:34] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "99" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:47:36] [Logging$class:logInfo:59] Executor app-20150702201026-0001/99 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:47:37] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/100 for PageRank
  [INFO ] [2015-07-02 20:47:37] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "100" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:47:38] [Logging$class:logInfo:59] Executor app-20150702201026-0001/100 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:47:40] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/101 for PageRank
  [INFO ] [2015-07-02 20:47:40] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "101" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/101 is now RUNNING
  [INFO ] [2015-07-02 20:47:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/101 is now LOADING
  [INFO ] [2015-07-02 20:47:42] [Logging$class:logInfo:59] Executor app-20150702201026-0001/101 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/102 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:47:38] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/101 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:47:38] [Logging$class:logInfo:59] Executor app-20150702201026-0001/101 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:47:38] [Logging$class:logError:75] Asked to remove non-existent executor 101
  [INFO ] [2015-07-02 20:47:38] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/102 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:47:38] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/102 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:47:38] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/102 is now RUNNING
  [WARN ] [2015-07-02 20:47:43] [Logging$class:logWarning:71] Told to re-register on heartbeat
  [INFO ] [2015-07-02 20:47:43] [Logging$class:logInfo:59] BlockManager re-registering with master
  [INFO ] [2015-07-02 20:47:43] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:47:43] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:47:43] [Logging$class:logInfo:59] Reporting 4 blocks to the master.
  47:44] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/102 for PageRank
  [INFO ] [2015-07-02 20:47:48] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "102" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:47:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/102 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:47:51] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/103 for PageRank
  [INFO ] [2015-07-02 20:47:51] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "103" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  47] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/103 is now LOADING
  [INFO ] [2015-07-02 20:47:53] [Logging$class:logInfo:59] Executor app-20150702201026-0001/103 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/104 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:47:50] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/103 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:47:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/103 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:47:50] [Logging$class:logError:75] Asked to remove non-existent executor 103
  [INFO ] [2015-07-02 20:47:50] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/104 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:47:50] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/104 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:47:50] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/104 is now RUNNING
  [INFO ] [2015-07-02 20:47:55] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/104 for PageRank
  [INFO ] [2015-07-02 20:47:56] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "104" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:47:52] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/104 is now LOADING
  [INFO ] [2015-07-02 20:47:57] [Logging$class:logInfo:59] Executor app-20150702201026-0001/104 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:47:57] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/105 for PageRank
  [INFO ] [2015-07-02 20:47:57] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "105" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:00] [Logging$class:logInfo:59] Executor app-20150702201026-0001/105 finished with state EXITED message Command exited with code 1 exitStatus 1
  o:59] Removing executor app-20150702201026-0001/105 because it is EXITED
  [INFO ] [2015-07-02 20:47:56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/105 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:47:56] [Logging$class:logInfo:59] Executor app-20150702201026-0001/105 removed: Command exited with code 1
  [INFO ] [2015-07-02 20:47:56] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/106 on worker worker-20150702200334-10.0.0.41-41593
  [ERROR] [2015-07-02 20:47:56] [Logging$class:logError:75] Asked to remove non-existent executor 105
  [INFO ] [2015-07-02 20:47:56] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/106 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:47:56] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/106 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:47:56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/106 is now RUNNING
  [INFO ] [2015-07-02 20:48:03] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/106 for PageRank
  [INFO ] [2015-07-02 20:48:03] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "106" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:07] [Logging$class:logInfo:59] Executor app-20150702201026-0001/106 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:48:07] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/107 for PageRank
  [INFO ] [2015-07-02 20:48:08] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "107" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:10] [Logging$class:logInfo:59] Executor app-20150702201026-0001/107 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:48:12] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/108 for PageRank
  [INFO ] [2015-07-02 20:48:12] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "108" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/108 is now LOADING
  [INFO ] [2015-07-02 20:48:07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/108 is now RUNNING
  [INFO ] [2015-07-02 20:48:13] [Logging$class:logInfo:59] Executor app-20150702201026-0001/108 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:48:14] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/109 for PageRank
  -02 20:48:09] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/108 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:48:09] [Logging$class:logInfo:59] Executor app-20150702201026-0001/108 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:48:09] [Logging$class:logError:75] Asked to remove non-existent executor 108
  [INFO ] [2015-07-02 20:48:09] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/109 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:48:09] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/109 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:48:09] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/109 is now RUNNING
  [INFO ] [2015-07-02 20:48:15] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "109" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:18] [Logging$class:logInfo:59] Executor app-20150702201026-0001/109 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:48:18] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/110 for PageRank
  [INFO ] [2015-07-02 20:48:18] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "110" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:14] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/110 is now RUNNING
  [INFO ] [2015-07-02 20:48:20] [Logging$class:logInfo:59] Executor app-20150702201026-0001/110 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:48:20] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/111 for PageRank
  [INFO ] [2015-07-02 20:48:20] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "111" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:16] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/111 is now LOADING
  [INFO ] [2015-07-02 20:48:16] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/111 is now RUNNING
  [INFO ] [2015-07-02 20:48:22] [Logging$class:logInfo:59] Executor app-20150702201026-0001/111 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:48:23] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/112 for PageRank
  [INFO ] [2015-07-02 20:48:23] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "112" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/112 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:48:27] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/113 for PageRank
  -02 20:48:20] [Logging$class:logError:75] Asked to remove non-existent executor 111
  [INFO ] [2015-07-02 20:48:20] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/112 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:48:20] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/112 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:48:20] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/112 is now LOADING
  [INFO ] [2015-07-02 20:48:20] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/112 is now RUNNING
  [INFO ] [2015-07-02 20:48:22] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/112 because it is EXITED
  [INFO ] [2015-07-02 20:48:22] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/113 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:48:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/112 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:48:22] [Logging$class:logInfo:59] Executor app-20150702201026-0001/112 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:48:22] [Logging$class:logError:75] Asked to remove non-existent executor 112
  [INFO ] [2015-07-02 20:48:22] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/113 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:48:22] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/113 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:48:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/113 is now LOADING
  [INFO ] [2015-07-02 20:48:23] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/113 is now RUNNING
  [INFO ] [2015-07-02 20:48:28] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "113" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/113 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/114 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:48:26] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/113 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:48:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/113 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:48:27] [Logging$class:logError:75] Asked to remove non-existent executor 113
  [INFO ] [2015-07-02 20:48:27] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/114 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:48:27] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/114 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:48:27] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/114 is now RUNNING
  [INFO ] [2015-07-02 20:48:32] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/114 for PageRank
  [INFO ] [2015-07-02 20:48:34] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "114" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:37] [Logging$class:logInfo:59] Executor app-20150702201026-0001/114 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/115 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:48:33] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/114 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:48:33] [Logging$class:logInfo:59] Executor app-20150702201026-0001/114 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:48:33] [Logging$class:logError:75] Asked to remove non-existent executor 114
  [INFO ] [2015-07-02 20:48:33] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/115 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:48:33] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/115 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:48:33] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/115 is now RUNNING
  [INFO ] [2015-07-02 20:48:38] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/115 for PageRank
  [INFO ] [2015-07-02 20:48:39] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "115" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:40] [Logging$class:logInfo:59] Executor app-20150702201026-0001/115 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/116 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:48:36] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/115 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:48:36] [Logging$class:logInfo:59] Executor app-20150702201026-0001/115 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:48:36] [Logging$class:logError:75] Asked to remove non-existent executor 115
  [INFO ] [2015-07-02 20:48:36] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/116 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:48:36] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/116 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:48:36] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/116 is now RUNNING
  [INFO ] [2015-07-02 20:48:42] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/116 for PageRank
  [INFO ] [2015-07-02 20:48:45] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "116" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:49] [Logging$class:logInfo:59] Executor app-20150702201026-0001/116 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:48:49] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/117 for PageRank
  -02 20:48:45] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/116 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:48:45] [Logging$class:logInfo:59] Executor app-20150702201026-0001/116 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:48:45] [Logging$class:logError:75] Asked to remove non-existent executor 116
  [INFO ] [2015-07-02 20:48:45] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/117 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:48:45] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/117 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:48:45] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/117 is now RUNNING
  [INFO ] [2015-07-02 20:48:45] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/117 is now LOADING
  [INFO ] [2015-07-02 20:48:50] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "117" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:48:56] [Logging$class:logInfo:59] Executor app-20150702201026-0001/117 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:49:00] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/118 for PageRank
  -02 20:48:54] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/117 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:48:54] [Logging$class:logInfo:59] Executor app-20150702201026-0001/117 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:48:54] [Logging$class:logError:75] Asked to remove non-existent executor 117
  [INFO ] [2015-07-02 20:48:54] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/118 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:48:54] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/118 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:48:54] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/118 is now RUNNING
  [INFO ] [2015-07-02 20:48:56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/118 is now LOADING
  [INFO ] [2015-07-02 20:49:01] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "118" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:49:05] [Logging$class:logInfo:59] Executor app-20150702201026-0001/118 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Executor updated: app-20150702201026-0001/118 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:49:02] [Logging$class:logInfo:59] Executor app-20150702201026-0001/118 removed: Command exited with code 1
  [INFO ] [2015-07-02 20:49:02] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/119 on worker worker-20150702200334-10.0.0.41-41593
  [ERROR] [2015-07-02 20:49:02] [Logging$class:logError:75] Asked to remove non-existent executor 118
  [INFO ] [2015-07-02 20:49:02] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/119 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:49:02] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/119 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:49:02] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/119 is now RUNNING
  [INFO ] [2015-07-02 20:49:07] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/119 for PageRank
  [INFO ] [2015-07-02 20:49:08] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "119" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:49:11] [Logging$class:logInfo:59] Executor app-20150702201026-0001/119 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:49:11] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/120 for PageRank
  [INFO ] [2015-07-02 20:49:11] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "120" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:49:07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/120 is now LOADING
  [INFO ] [2015-07-02 20:49:07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/120 is now RUNNING
  [INFO ] [2015-07-02 20:49:12] [Logging$class:logInfo:59] Executor app-20150702201026-0001/120 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/121 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:49:09] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/120 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:49:09] [Logging$class:logInfo:59] Executor app-20150702201026-0001/120 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:49:09] [Logging$class:logError:75] Asked to remove non-existent executor 120
  [INFO ] [2015-07-02 20:49:09] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/121 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:49:09] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/121 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:49:09] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/121 is now RUNNING
  [INFO ] [2015-07-02 20:49:14] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/121 for PageRank
  [INFO ] [2015-07-02 20:49:15] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "121" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:49:19] [Logging$class:logInfo:59] Executor app-20150702201026-0001/121 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:49:23] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/122 for PageRank
  [INFO ] [2015-07-02 20:49:23] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "122" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  19] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/122 is now LOADING
  [INFO ] [2015-07-02 20:49:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/122 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:49:27] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/123 for PageRank
  [INFO ] [2015-07-02 20:49:28] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "123" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:49:24] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/123 is now LOADING
  [INFO ] [2015-07-02 20:49:32] [Logging$class:logInfo:59] Executor app-20150702201026-0001/123 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/124 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:49:29] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/123 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:49:29] [Logging$class:logInfo:59] Executor app-20150702201026-0001/123 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:49:29] [Logging$class:logError:75] Asked to remove non-existent executor 123
  [INFO ] [2015-07-02 20:49:29] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/124 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:49:29] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/124 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:49:30] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/124 is now RUNNING
  [INFO ] [2015-07-02 20:49:36] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/124 for PageRank
  [INFO ] [2015-07-02 20:49:37] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "124" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:49:38] [Logging$class:logInfo:59] Executor app-20150702201026-0001/124 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:49:39] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/125 for PageRank
  [INFO ] [2015-07-02 20:49:40] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "125" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/125 is now LOADING
  [INFO ] [2015-07-02 20:49:42] [Logging$class:logInfo:59] Executor app-20150702201026-0001/125 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/126 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:49:39] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/125 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:49:39] [Logging$class:logInfo:59] Executor app-20150702201026-0001/125 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:49:39] [Logging$class:logError:75] Asked to remove non-existent executor 125
  [INFO ] [2015-07-02 20:49:39] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/126 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:49:39] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/126 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:49:39] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/126 is now RUNNING
  [INFO ] [2015-07-02 20:49:43] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/126 for PageRank
  [INFO ] [2015-07-02 20:49:45] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "126" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:49:47] [Logging$class:logInfo:59] Executor app-20150702201026-0001/126 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/127 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:49:43] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/126 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:49:43] [Logging$class:logInfo:59] Executor app-20150702201026-0001/126 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:49:43] [Logging$class:logError:75] Asked to remove non-existent executor 126
  [INFO ] [2015-07-02 20:49:43] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/127 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:49:43] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/127 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:49:43] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/127 is now RUNNING
  [INFO ] [2015-07-02 20:49:49] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/127 for PageRank
  [INFO ] [2015-07-02 20:49:49] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "127" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:49:53] [Logging$class:logInfo:59] Executor app-20150702201026-0001/127 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:49:56] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/128 for PageRank
  [INFO ] [2015-07-02 20:49:56] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "128" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  53] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/128 is now LOADING
  [INFO ] [2015-07-02 20:49:58] [Logging$class:logInfo:59] Executor app-20150702201026-0001/128 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:49:58] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/129 for PageRank
  [INFO ] [2015-07-02 20:49:59] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "129" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  54] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/129 is now RUNNING
  [INFO ] [2015-07-02 20:50:01] [Logging$class:logInfo:59] Executor app-20150702201026-0001/129 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:02] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/130 for PageRank
  -02 20:49:57] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/129 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:49:57] [Logging$class:logInfo:59] Executor app-20150702201026-0001/129 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:49:57] [Logging$class:logError:75] Asked to remove non-existent executor 129
  [INFO ] [2015-07-02 20:49:57] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/130 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:49:57] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/130 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:49:57] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/130 is now RUNNING
  [INFO ] [2015-07-02 20:49:57] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/130 is now LOADING
  [INFO ] [2015-07-02 20:50:02] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "130" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:06] [Logging$class:logInfo:59] Executor app-20150702201026-0001/130 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:08] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/131 for PageRank
  -02 20:50:03] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/130 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:50:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/130 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:50:03] [Logging$class:logError:75] Asked to remove non-existent executor 130
  [INFO ] [2015-07-02 20:50:03] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/131 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:03] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/131 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:03] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/131 is now LOADING
  [INFO ] [2015-07-02 20:50:04] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/131 is now RUNNING
  [INFO ] [2015-07-02 20:50:09] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "131" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:10] [Logging$class:logInfo:59] Executor app-20150702201026-0001/131 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:11] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/132 for PageRank
  -02 20:50:06] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/131 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:50:06] [Logging$class:logInfo:59] Executor app-20150702201026-0001/131 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:50:06] [Logging$class:logError:75] Asked to remove non-existent executor 131
  [INFO ] [2015-07-02 20:50:06] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/132 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:06] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/132 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:06] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/132 is now LOADING
  [INFO ] [2015-07-02 20:50:06] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/132 is now RUNNING
  [INFO ] [2015-07-02 20:50:13] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "132" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:14] [Logging$class:logInfo:59] Executor app-20150702201026-0001/132 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:15] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/133 for PageRank
  [INFO ] [2015-07-02 20:50:15] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "133" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  11] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/133 is now LOADING
  [INFO ] [2015-07-02 20:50:14] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/133 because it is EXITED
  [INFO ] [2015-07-02 20:50:14] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/134 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:50:14] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/133 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:50:14] [Logging$class:logInfo:59] Executor app-20150702201026-0001/133 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:50:14] [Logging$class:logError:75] Asked to remove non-existent executor 133
  [INFO ] [2015-07-02 20:50:14] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/134 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:14] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/134 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:14] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/134 is now RUNNING
  [INFO ] [2015-07-02 20:50:18] [Logging$class:logInfo:59] Executor app-20150702201026-0001/133 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:19] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/134 for PageRank
  [INFO ] [2015-07-02 20:50:19] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "134" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:22] [Logging$class:logInfo:59] Executor app-20150702201026-0001/134 finished with state EXITED message Command exited with code 1 exitStatus 1
  o:59] Removing executor app-20150702201026-0001/134 because it is EXITED
  [INFO ] [2015-07-02 20:50:18] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/135 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:50:19] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/134 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:50:19] [Logging$class:logInfo:59] Executor app-20150702201026-0001/134 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:50:19] [Logging$class:logError:75] Asked to remove non-existent executor 134
  [INFO ] [2015-07-02 20:50:19] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/135 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:19] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/135 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:19] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/135 is now RUNNING
  [INFO ] [2015-07-02 20:50:24] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/135 for PageRank
  [INFO ] [2015-07-02 20:50:24] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "135" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/135 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:26] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/136 for PageRank
  -02 20:50:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/135 is now LOADING
  [INFO ] [2015-07-02 20:50:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/135 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:50:22] [Logging$class:logInfo:59] Executor app-20150702201026-0001/135 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:50:22] [Logging$class:logError:75] Asked to remove non-existent executor 135
  [INFO ] [2015-07-02 20:50:22] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/136 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:22] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/136 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/136 is now LOADING
  [INFO ] [2015-07-02 20:50:23] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/136 is now RUNNING
  [INFO ] [2015-07-02 20:50:28] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "136" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:29] [Logging$class:logInfo:59] Executor app-20150702201026-0001/136 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:30] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/137 for PageRank
  -02 20:50:26] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/136 is now EXITED (Command exited with code 1)
  [WARN ] [2015-07-02 20:50:26] [Logging$class:logWarning:71] Removing executor 3 with no recent heartbeats: 165035 ms exceeds timeout 120000 ms
  [INFO ] [2015-07-02 20:50:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/136 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:50:26] [Logging$class:logError:75] Lost executor 3 on 10.0.0.42: Executor heartbeat timed out after 165035 ms
  [ERROR] [2015-07-02 20:50:26] [Logging$class:logError:75] Asked to remove non-existent executor 136
  [INFO ] [2015-07-02 20:50:26] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/137 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:26] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/137 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:26] [Logging$class:logInfo:59] Re-queueing tasks for 3 from TaskSet 0.0
  [INFO ] [2015-07-02 20:50:26] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/137 is now RUNNING
  [INFO ] [2015-07-02 20:50:31] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "137" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:33] [Logging$class:logInfo:59] Executor app-20150702201026-0001/137 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:33] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/138 for PageRank
  [INFO ] [2015-07-02 20:50:33] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "138" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  lass:logInfo:59] Executor updated: app-20150702201026-0001/138 is now LOADING
  [INFO ] [2015-07-02 20:50:34] [Logging$class:logInfo:59] Executor app-20150702201026-0001/138 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:35] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/139 for PageRank
  [INFO ] [2015-07-02 20:50:36] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "139" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  31] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/139 is now RUNNING
  [INFO ] [2015-07-02 20:50:31] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/139 is now LOADING
  [INFO ] [2015-07-02 20:50:39] [Logging$class:logInfo:59] Executor app-20150702201026-0001/139 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:39] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/140 for PageRank
  -02 20:50:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/139 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:50:35] [Logging$class:logInfo:59] Executor app-20150702201026-0001/139 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:50:35] [Logging$class:logError:75] Asked to remove non-existent executor 139
  [INFO ] [2015-07-02 20:50:35] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/140 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:35] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/140 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/140 is now LOADING
  [INFO ] [2015-07-02 20:50:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/140 is now RUNNING
  [INFO ] [2015-07-02 20:50:39] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "140" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:41] [Logging$class:logInfo:59] Executor app-20150702201026-0001/140 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:43] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/141 for PageRank
  [INFO ] [2015-07-02 20:50:43] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "141" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  39] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/141 is now LOADING
  [INFO ] [2015-07-02 20:50:46] [Logging$class:logInfo:59] Executor app-20150702201026-0001/141 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/142 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:50:48] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/142 for PageRank
  [INFO ] [2015-07-02 20:50:48] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "142" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:44] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/141 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:50:44] [Logging$class:logInfo:59] Executor app-20150702201026-0001/141 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:50:44] [Logging$class:logError:75] Asked to remove non-existent executor 141
  [INFO ] [2015-07-02 20:50:44] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/142 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:44] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/142 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:44] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/142 is now LOADING
  [INFO ] [2015-07-02 20:50:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/142 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:51] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/143 for PageRank
  [INFO ] [2015-07-02 20:50:51] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "143" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:46] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/143 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:47] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/143 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:47] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/143 is now LOADING
  [INFO ] [2015-07-02 20:50:52] [Logging$class:logInfo:59] Executor app-20150702201026-0001/143 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:50:53] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/144 for PageRank
  fo:59] Executor updated: app-20150702201026-0001/143 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:50:49] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/144 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:50:49] [Logging$class:logInfo:59] Executor app-20150702201026-0001/143 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:50:49] [Logging$class:logError:75] Asked to remove non-existent executor 143
  [INFO ] [2015-07-02 20:50:49] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/144 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:50:49] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/144 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:50:49] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/144 is now RUNNING
  [INFO ] [2015-07-02 20:50:54] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "144" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:50:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/144 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:00] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/145 for PageRank
  [INFO ] [2015-07-02 20:51:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "145" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/145 is now LOADING
  [INFO ] [2015-07-02 20:51:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/145 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/146 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:51:00] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/145 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:51:00] [Logging$class:logInfo:59] Executor app-20150702201026-0001/145 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:51:00] [Logging$class:logError:75] Asked to remove non-existent executor 145
  [INFO ] [2015-07-02 20:51:00] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/146 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:00] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/146 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:51:00] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/146 is now RUNNING
  [INFO ] [2015-07-02 20:51:05] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/146 for PageRank
  [INFO ] [2015-07-02 20:51:06] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "146" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:51:07] [Logging$class:logInfo:59] Executor app-20150702201026-0001/146 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:07] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/147 for PageRank
  [INFO ] [2015-07-02 20:51:07] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "147" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [ERROR] [2015-07-02 20:51:03] [Logging$class:logError:75] Asked to remove non-existent executor 146
  [INFO ] [2015-07-02 20:51:03] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/147 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:03] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/147 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:51:10] [Logging$class:logInfo:59] Executor app-20150702201026-0001/147 finished with state EXITED message Command exited with code 1 exitStatus 1
  o:59] Executor updated: app-20150702201026-0001/147 is now RUNNING
  [INFO ] [2015-07-02 20:51:06] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/147 because it is EXITED
  [INFO ] [2015-07-02 20:51:06] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/148 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:51:06] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/147 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:51:06] [Logging$class:logInfo:59] Executor app-20150702201026-0001/147 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:51:06] [Logging$class:logError:75] Asked to remove non-existent executor 147
  [INFO ] [2015-07-02 20:51:06] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/148 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:06] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/148 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:51:06] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/148 is now RUNNING
  [INFO ] [2015-07-02 20:51:11] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/148 for PageRank
  [INFO ] [2015-07-02 20:51:12] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "148" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:51:13] [Logging$class:logInfo:59] Executor app-20150702201026-0001/148 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:13] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/149 for PageRank
  [INFO ] [2015-07-02 20:51:13] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "149" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:51:15] [Logging$class:logInfo:59] Executor app-20150702201026-0001/149 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:17] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/150 for PageRank
  [INFO ] [2015-07-02 20:51:17] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "150" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  13] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/150 is now LOADING
  [INFO ] [2015-07-02 20:51:13] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/150 is now RUNNING
  [INFO ] [2015-07-02 20:51:21] [Logging$class:logInfo:59] Executor app-20150702201026-0001/150 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:21] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/151 for PageRank
  [INFO ] [2015-07-02 20:51:21] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "151" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  17] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/151 is now RUNNING
  [INFO ] [2015-07-02 20:51:23] [Logging$class:logInfo:59] Executor app-20150702201026-0001/151 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:23] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/152 for PageRank
  -02 20:51:19] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/151 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:51:19] [Logging$class:logInfo:59] Executor app-20150702201026-0001/151 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:51:19] [Logging$class:logError:75] Asked to remove non-existent executor 151
  [INFO ] [2015-07-02 20:51:19] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/152 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:19] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/152 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:51:19] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/152 is now RUNNING
  [INFO ] [2015-07-02 20:51:25] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "152" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:51:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/152 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:32] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/153 for PageRank
  [INFO ] [2015-07-02 20:51:32] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "153" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:51:28] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/153 is now LOADING
  [INFO ] [2015-07-02 20:51:34] [Logging$class:logInfo:59] Executor app-20150702201026-0001/153 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/154 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:51:30] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/153 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:51:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/153 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:51:30] [Logging$class:logError:75] Asked to remove non-existent executor 153
  [INFO ] [2015-07-02 20:51:30] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/154 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:30] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/154 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:51:30] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/154 is now RUNNING
  [INFO ] [2015-07-02 20:51:36] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/154 for PageRank
  [INFO ] [2015-07-02 20:51:37] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "154" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:51:39] [Logging$class:logInfo:59] Executor app-20150702201026-0001/154 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:39] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/155 for PageRank
  -02 20:51:34] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/154 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:51:34] [Logging$class:logInfo:59] Executor app-20150702201026-0001/154 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:51:34] [Logging$class:logError:75] Asked to remove non-existent executor 154
  [INFO ] [2015-07-02 20:51:34] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/155 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:34] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/155 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:51:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/155 is now RUNNING
  [INFO ] [2015-07-02 20:51:35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/155 is now LOADING
  [INFO ] [2015-07-02 20:51:40] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "155" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:51:46] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/155 because it is EXITED
  [INFO ] [2015-07-02 20:51:46] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/155 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:51:46] [Logging$class:logInfo:59] Executor app-20150702201026-0001/155 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:51:46] [Logging$class:logError:75] Asked to remove non-existent executor 155
  [INFO ] [2015-07-02 20:51:46] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/156 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:51:46] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/156 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:46] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/156 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:51:46] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/156 is now RUNNING
  [INFO ] [2015-07-02 20:51:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/155 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:52] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/156 for PageRank
  [INFO ] [2015-07-02 20:51:52] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "156" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:51:53] [Logging$class:logInfo:59] Executor app-20150702201026-0001/156 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:53] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/157 for PageRank
  [INFO ] [2015-07-02 20:51:53] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "157" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:51:54] [Logging$class:logInfo:59] Executor app-20150702201026-0001/157 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:51:56] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/158 for PageRank
  [INFO ] [2015-07-02 20:51:56] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "158" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  executor app-20150702201026-0001/157 because it is EXITED
  [INFO ] [2015-07-02 20:51:51] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/158 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:51:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/157 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:51:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/157 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:51:51] [Logging$class:logError:75] Asked to remove non-existent executor 157
  [INFO ] [2015-07-02 20:51:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/158 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/158 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:51:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/158 is now RUNNING
  [INFO ] [2015-07-02 20:51:52] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/158 is now LOADING
  [INFO ] [2015-07-02 20:51:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/158 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/159 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:51:59] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/159 for PageRank
   exited with code 1)
  [INFO ] [2015-07-02 20:51:55] [Logging$class:logInfo:59] Executor app-20150702201026-0001/158 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:51:55] [Logging$class:logError:75] Asked to remove non-existent executor 158
  [INFO ] [2015-07-02 20:51:55] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/159 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:59] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "159" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:52:00] [Logging$class:logInfo:59] Executor app-20150702201026-0001/159 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:52:01] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/160 for PageRank
  [INFO ] [2015-07-02 20:52:02] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "160" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  57] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/160 is now LOADING
  [INFO ] [2015-07-02 20:52:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/160 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:52:03] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/161 for PageRank
  -02 20:51:59] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/160 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:51:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/160 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:51:59] [Logging$class:logError:75] Asked to remove non-existent executor 160
  [INFO ] [2015-07-02 20:51:59] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/161 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:51:59] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/161 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:51:59] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/161 is now LOADING
  [INFO ] [2015-07-02 20:51:59] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/161 is now RUNNING
  [INFO ] [2015-07-02 20:52:07] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "161" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:52:13] [Logging$class:logInfo:59] Executor app-20150702201026-0001/161 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:52:14] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/162 for PageRank
  [INFO ] [2015-07-02 20:52:14] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "162" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  10] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/162 is now RUNNING
  [INFO ] [2015-07-02 20:52:16] [Logging$class:logInfo:59] Executor app-20150702201026-0001/162 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/163 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:52:13] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/162 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:52:13] [Logging$class:logInfo:59] Executor app-20150702201026-0001/162 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:52:13] [Logging$class:logError:75] Asked to remove non-existent executor 162
  [INFO ] [2015-07-02 20:52:13] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/163 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:52:13] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/163 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:52:13] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/163 is now RUNNING
  [INFO ] [2015-07-02 20:52:18] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/163 for PageRank
  [INFO ] [2015-07-02 20:52:20] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "163" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:52:21] [Logging$class:logInfo:59] Executor app-20150702201026-0001/163 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:52:26] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/164 for PageRank
   [INFO ] [2015-07-02 20:52:21] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/164 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:52:21] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/163 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:52:21] [Logging$class:logInfo:59] Executor app-20150702201026-0001/163 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:52:21] [Logging$class:logError:75] Asked to remove non-existent executor 163
  [INFO ] [2015-07-02 20:52:21] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/164 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:52:21] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/164 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:52:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/164 is now RUNNING
  [INFO ] [2015-07-02 20:52:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/164 is now LOADING
  [INFO ] [2015-07-02 20:52:27] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "164" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:52:29] [Logging$class:logInfo:59] Executor app-20150702201026-0001/164 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:52:29] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/165 for PageRank
  [INFO ] [2015-07-02 20:52:29] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "165" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:52:33] [Logging$class:logInfo:59] Executor app-20150702201026-0001/165 finished with state EXITED message Command exited with code 1 exitStatus 1
  o:59] Removing executor app-20150702201026-0001/165 because it is EXITED
  [INFO ] [2015-07-02 20:52:29] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/166 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:52:30] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/165 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:52:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/165 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:52:30] [Logging$class:logError:75] Asked to remove non-existent executor 165
  [INFO ] [2015-07-02 20:52:30] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/166 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:52:30] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/166 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:52:36] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/166 for PageRank
  [INFO ] [2015-07-02 20:52:38] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "166" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:52:38] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/166 is now RUNNING
  [INFO ] [2015-07-02 20:52:43] [Logging$class:logInfo:59] Executor app-20150702201026-0001/166 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:52:43] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/167 for PageRank
  [INFO ] [2015-07-02 20:52:44] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "167" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:52:39] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/166 is now LOADING
  [INFO ] [2015-07-02 20:52:40] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/166 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:52:40] [Logging$class:logInfo:59] Executor app-20150702201026-0001/166 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:52:40] [Logging$class:logError:75] Asked to remove non-existent executor 166
  [INFO ] [2015-07-02 20:52:40] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/167 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:52:40] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/167 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:52:40] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/167 is now LOADING
  [INFO ] [2015-07-02 20:52:41] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/167 is now RUNNING
  [INFO ] [2015-07-02 20:52:46] [Logging$class:logInfo:59] Executor app-20150702201026-0001/167 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/168 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:52:44] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/167 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:52:44] [Logging$class:logInfo:59] Executor app-20150702201026-0001/167 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:52:44] [Logging$class:logError:75] Asked to remove non-existent executor 167
  [INFO ] [2015-07-02 20:52:44] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/168 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:52:44] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/168 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:52:49] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/168 for PageRank
  [INFO ] [2015-07-02 20:52:49] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "168" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:52:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/168 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:52:51] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/169 for PageRank
  [INFO ] [2015-07-02 20:52:52] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "169" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  48] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/169 is now LOADING
  [INFO ] [2015-07-02 20:52:54] [Logging$class:logInfo:59] Executor app-20150702201026-0001/169 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:52:56] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/170 for PageRank
  [INFO ] [2015-07-02 20:52:56] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "170" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:52:58] [Logging$class:logInfo:59] Executor app-20150702201026-0001/170 finished with state EXITED message Command exited with code 1 exitStatus 1
  o:59] Executor updated: app-20150702201026-0001/170 is now RUNNING
  [INFO ] [2015-07-02 20:52:54] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/170 because it is EXITED
  [INFO ] [2015-07-02 20:52:54] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/171 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:52:54] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/170 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:52:54] [Logging$class:logInfo:59] Executor app-20150702201026-0001/170 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:52:54] [Logging$class:logError:75] Asked to remove non-existent executor 170
  [INFO ] [2015-07-02 20:52:54] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/171 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:52:54] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/171 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:53:00] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/171 for PageRank
  [INFO ] [2015-07-02 20:53:01] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "171" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:53:02] [Logging$class:logInfo:59] Executor app-20150702201026-0001/171 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/172 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:52:58] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/171 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:52:58] [Logging$class:logInfo:59] Executor app-20150702201026-0001/171 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:52:58] [Logging$class:logError:75] Asked to remove non-existent executor 171
  [INFO ] [2015-07-02 20:52:58] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/172 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:52:58] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/172 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:52:59] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/172 is now RUNNING
  [INFO ] [2015-07-02 20:53:05] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/172 for PageRank
  [INFO ] [2015-07-02 20:53:05] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "172" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:53:07] [Logging$class:logInfo:59] Executor app-20150702201026-0001/172 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:09] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/173 for PageRank
  -02 20:53:05] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/172 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:53:05] [Logging$class:logInfo:59] Executor app-20150702201026-0001/172 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:53:05] [Logging$class:logError:75] Asked to remove non-existent executor 172
  [INFO ] [2015-07-02 20:53:05] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/173 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:53:05] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/173 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:53:05] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/173 is now LOADING
  [INFO ] [2015-07-02 20:53:05] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/173 is now RUNNING
  [INFO ] [2015-07-02 20:53:11] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "173" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:53:13] [Logging$class:logInfo:59] Executor app-20150702201026-0001/173 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:13] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/174 for PageRank
  [INFO ] [2015-07-02 20:53:14] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "174" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  09] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/174 is now LOADING
  [INFO ] [2015-07-02 20:53:21] [Logging$class:logInfo:59] Executor app-20150702201026-0001/174 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:21] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/175 for PageRank
  [INFO ] [2015-07-02 20:53:26] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "175" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  17] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/175 is now RUNNING
  [INFO ] [2015-07-02 20:53:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/175 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:30] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/176 for PageRank
  [INFO ] [2015-07-02 20:53:30] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "176" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [ERROR] [2015-07-02 20:53:26] [Logging$class:logError:75] Asked to remove non-existent executor 175
  [INFO ] [2015-07-02 20:53:26] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/176 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:53:26] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/176 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:53:26] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/176 is now LOADING
  [INFO ] [2015-07-02 20:53:26] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/176 is now RUNNING
  [INFO ] [2015-07-02 20:53:32] [Logging$class:logInfo:59] Executor app-20150702201026-0001/176 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:33] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/177 for PageRank
  -02 20:53:28] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/176 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:53:28] [Logging$class:logInfo:59] Executor app-20150702201026-0001/176 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:53:28] [Logging$class:logError:75] Asked to remove non-existent executor 176
  [INFO ] [2015-07-02 20:53:28] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/177 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:53:29] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/177 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:53:29] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/177 is now RUNNING
  [INFO ] [2015-07-02 20:53:29] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/177 is now LOADING
  [INFO ] [2015-07-02 20:53:35] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "177" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:53:37] [Logging$class:logInfo:59] Executor app-20150702201026-0001/177 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:37] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/178 for PageRank
  -02 20:53:32] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/177 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:53:32] [Logging$class:logInfo:59] Executor app-20150702201026-0001/177 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:53:32] [Logging$class:logError:75] Asked to remove non-existent executor 177
  [INFO ] [2015-07-02 20:53:32] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/178 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:53:32] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/178 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:53:32] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/178 is now RUNNING
  [INFO ] [2015-07-02 20:53:34] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/178 is now LOADING
  [INFO ] [2015-07-02 20:53:39] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "178" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:53:40] [Logging$class:logInfo:59] Executor app-20150702201026-0001/178 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:42] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/179 for PageRank
  -02 20:53:37] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/178 because it is EXITED
  [INFO ] [2015-07-02 20:53:37] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/179 on worker worker-20150702200334-10.0.0.41-41593
  [ERROR] [2015-07-02 20:53:37] [Logging$class:logError:75] Asked to remove non-existent executor 178
  [INFO ] [2015-07-02 20:53:37] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/179 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:53:37] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/179 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:53:37] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/179 is now RUNNING
  [INFO ] [2015-07-02 20:53:39] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/179 is now LOADING
  [INFO ] [2015-07-02 20:53:43] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "179" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:53:44] [Logging$class:logInfo:59] Executor app-20150702201026-0001/179 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:44] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/180 for PageRank
  [INFO ] [2015-07-02 20:53:45] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "180" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  40] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/180 is now LOADING
  [INFO ] [2015-07-02 20:53:46] [Logging$class:logInfo:59] Executor app-20150702201026-0001/180 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:46] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/181 for PageRank
  -02 20:53:42] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/180 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:53:42] [Logging$class:logInfo:59] Executor app-20150702201026-0001/180 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:53:42] [Logging$class:logError:75] Asked to remove non-existent executor 180
  [INFO ] [2015-07-02 20:53:42] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/181 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:53:42] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/181 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:53:42] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/181 is now LOADING
  [INFO ] [2015-07-02 20:53:42] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/181 is now RUNNING
  [INFO ] [2015-07-02 20:53:54] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "181" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:53:56] [Logging$class:logInfo:59] Executor app-20150702201026-0001/181 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:53:57] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/182 for PageRank
  -02 20:53:52] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/181 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:53:52] [Logging$class:logInfo:59] Executor app-20150702201026-0001/181 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:53:52] [Logging$class:logError:75] Asked to remove non-existent executor 181
  [INFO ] [2015-07-02 20:53:52] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/182 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:53:52] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/182 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:53:52] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/182 is now RUNNING
  [INFO ] [2015-07-02 20:53:53] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/182 is now LOADING
  [INFO ] [2015-07-02 20:53:58] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "182" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:53:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/182 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:01] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/183 for PageRank
  [INFO ] [2015-07-02 20:54:01] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "183" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/183 is now LOADING
  [INFO ] [2015-07-02 20:54:04] [Logging$class:logInfo:59] Executor app-20150702201026-0001/183 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/184 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:54:00] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/183 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:54:00] [Logging$class:logInfo:59] Executor app-20150702201026-0001/183 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:54:00] [Logging$class:logError:75] Asked to remove non-existent executor 183
  [INFO ] [2015-07-02 20:54:00] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/184 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:54:00] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/184 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:54:00] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/184 is now RUNNING
  [INFO ] [2015-07-02 20:54:07] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/184 for PageRank
  [INFO ] [2015-07-02 20:54:12] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "184" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:54:13] [Logging$class:logInfo:59] Executor app-20150702201026-0001/184 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:14] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/185 for PageRank
  -02 20:54:09] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/184 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:54:09] [Logging$class:logInfo:59] Executor app-20150702201026-0001/184 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:54:09] [Logging$class:logError:75] Asked to remove non-existent executor 184
  [INFO ] [2015-07-02 20:54:09] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/185 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:54:09] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/185 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:54:09] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/185 is now RUNNING
  [INFO ] [2015-07-02 20:54:10] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/185 is now LOADING
  [INFO ] [2015-07-02 20:54:16] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "185" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:54:19] [Logging$class:logInfo:59] Executor app-20150702201026-0001/185 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:20] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/186 for PageRank
  -02 20:54:15] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/185 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:54:15] [Logging$class:logInfo:59] Executor app-20150702201026-0001/185 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:54:15] [Logging$class:logError:75] Asked to remove non-existent executor 185
  [INFO ] [2015-07-02 20:54:15] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/186 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:54:15] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/186 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:54:15] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/186 is now RUNNING
  [INFO ] [2015-07-02 20:54:16] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/186 is now LOADING
  [INFO ] [2015-07-02 20:54:20] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "186" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:54:23] [Logging$class:logInfo:59] Executor app-20150702201026-0001/186 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:23] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/187 for PageRank
  [INFO ] [2015-07-02 20:54:23] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "187" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  19] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/187 is now RUNNING
  [INFO ] [2015-07-02 20:54:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/187 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:26] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/188 for PageRank
  -02 20:54:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/187 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:54:22] [Logging$class:logInfo:59] Executor app-20150702201026-0001/187 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:54:22] [Logging$class:logError:75] Asked to remove non-existent executor 187
  [INFO ] [2015-07-02 20:54:22] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/188 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:54:22] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/188 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:54:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/188 is now LOADING
  [INFO ] [2015-07-02 20:54:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/188 is now RUNNING
  [INFO ] [2015-07-02 20:54:27] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "188" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:54:29] [Logging$class:logInfo:59] Executor app-20150702201026-0001/188 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/189 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:54:25] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/188 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:54:25] [Logging$class:logInfo:59] Executor app-20150702201026-0001/188 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:54:25] [Logging$class:logError:75] Asked to remove non-existent executor 188
  [INFO ] [2015-07-02 20:54:25] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/189 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:54:26] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/189 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:54:26] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/189 is now RUNNING
  [INFO ] [2015-07-02 20:54:35] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/189 for PageRank
  [INFO ] [2015-07-02 20:54:39] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "189" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:54:41] [Logging$class:logInfo:59] Executor app-20150702201026-0001/189 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:42] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/190 for PageRank
  [INFO ] [2015-07-02 20:54:43] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "190" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  38] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/190 is now LOADING
  [INFO ] [2015-07-02 20:54:44] [Logging$class:logInfo:59] Executor app-20150702201026-0001/190 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/191 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:54:39] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/190 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:54:39] [Logging$class:logInfo:59] Executor app-20150702201026-0001/190 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:54:40] [Logging$class:logError:75] Asked to remove non-existent executor 190
  [INFO ] [2015-07-02 20:54:40] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/191 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:54:40] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/191 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:54:40] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/191 is now RUNNING
  [INFO ] [2015-07-02 20:54:45] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/191 for PageRank
  [INFO ] [2015-07-02 20:54:45] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "191" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:54:47] [Logging$class:logInfo:59] Executor app-20150702201026-0001/191 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:47] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/192 for PageRank
  fo:59] Launching executor app-20150702201026-0001/192 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:54:43] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/191 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:54:43] [Logging$class:logInfo:59] Executor app-20150702201026-0001/191 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:54:43] [Logging$class:logError:75] Asked to remove non-existent executor 191
  [INFO ] [2015-07-02 20:54:43] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/192 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:54:43] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/192 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:54:43] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/192 is now RUNNING
  [INFO ] [2015-07-02 20:54:43] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/192 is now LOADING
  [INFO ] [2015-07-02 20:54:48] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "192" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:54:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/192 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:51] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/193 for PageRank
  -02 20:54:46] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/192 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:54:46] [Logging$class:logInfo:59] Executor app-20150702201026-0001/192 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:54:46] [Logging$class:logError:75] Asked to remove non-existent executor 192
  [INFO ] [2015-07-02 20:54:46] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/193 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:54:46] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/193 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:54:46] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/193 is now RUNNING
  [INFO ] [2015-07-02 20:54:47] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/193 is now LOADING
  [INFO ] [2015-07-02 20:54:52] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "193" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:54:53] [Logging$class:logInfo:59] Executor app-20150702201026-0001/193 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:55] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/194 for PageRank
  [INFO ] [2015-07-02 20:54:55] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "194" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:54:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/194 is now LOADING
  [INFO ] [2015-07-02 20:54:56] [Logging$class:logInfo:59] Executor app-20150702201026-0001/194 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:54:58] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/195 for PageRank
  -02 20:54:52] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/194 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:54:52] [Logging$class:logInfo:59] Executor app-20150702201026-0001/194 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:54:52] [Logging$class:logError:75] Asked to remove non-existent executor 194
  [INFO ] [2015-07-02 20:54:52] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/195 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:54:52] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/195 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:54:52] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/195 is now RUNNING
  [INFO ] [2015-07-02 20:54:55] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/195 is now LOADING
  [INFO ] [2015-07-02 20:55:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "195" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:55:05] [Logging$class:logInfo:59] Executor app-20150702201026-0001/195 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:55:08] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/196 for PageRank
  [INFO ] [2015-07-02 20:55:10] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "196" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  04] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/196 is now LOADING
  [INFO ] [2015-07-02 20:55:16] [Logging$class:logInfo:59] Executor app-20150702201026-0001/196 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:55:18] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/197 for PageRank
  -02 20:55:12] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/196 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:55:12] [Logging$class:logInfo:59] Executor app-20150702201026-0001/196 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:55:12] [Logging$class:logError:75] Asked to remove non-existent executor 196
  [INFO ] [2015-07-02 20:55:12] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/197 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:55:12] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/197 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:55:13] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/197 is now RUNNING
  [INFO ] [2015-07-02 20:55:14] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/197 is now LOADING
  [INFO ] [2015-07-02 20:55:21] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "197" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:55:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/197 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:55:32] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/198 for PageRank
  [INFO ] [2015-07-02 20:55:32] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "198" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  28] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/198 is now LOADING
  [INFO ] [2015-07-02 20:55:35] [Logging$class:logInfo:59] Executor app-20150702201026-0001/198 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/199 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:55:32] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/198 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:55:32] [Logging$class:logInfo:59] Executor app-20150702201026-0001/198 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:55:32] [Logging$class:logError:75] Asked to remove non-existent executor 198
  [INFO ] [2015-07-02 20:55:32] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/199 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:55:32] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/199 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:55:36] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/199 for PageRank
  [INFO ] [2015-07-02 20:55:36] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "199" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:55:32] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/199 is now LOADING
  [INFO ] [2015-07-02 20:55:37] [Logging$class:logInfo:59] Executor app-20150702201026-0001/199 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:55:39] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/200 for PageRank
  [INFO ] [2015-07-02 20:55:39] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "200" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  35] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/200 is now LOADING
  [INFO ] [2015-07-02 20:55:41] [Logging$class:logInfo:59] Executor app-20150702201026-0001/200 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/201 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:55:37] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/200 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:55:37] [Logging$class:logInfo:59] Executor app-20150702201026-0001/200 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:55:37] [Logging$class:logError:75] Asked to remove non-existent executor 200
  [INFO ] [2015-07-02 20:55:37] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/201 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:55:37] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/201 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:55:38] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/201 is now RUNNING
  [INFO ] [2015-07-02 20:55:43] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/201 for PageRank
  [INFO ] [2015-07-02 20:55:44] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "201" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:55:49] [Logging$class:logInfo:59] Executor app-20150702201026-0001/201 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:55:52] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/202 for PageRank
  [INFO ] [2015-07-02 20:55:52] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "202" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:55:54] [Logging$class:logInfo:59] Executor app-20150702201026-0001/202 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:55:56] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/203 for PageRank
  fo:59] Launching executor app-20150702201026-0001/203 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:55:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/202 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:55:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/202 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:55:51] [Logging$class:logError:75] Asked to remove non-existent executor 202
  [INFO ] [2015-07-02 20:55:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/203 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:55:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/203 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:55:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/203 is now RUNNING
  [INFO ] [2015-07-02 20:55:52] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/203 is now LOADING
  [INFO ] [2015-07-02 20:55:56] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "203" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:55:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/203 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:55:59] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/204 for PageRank
  [INFO ] [2015-07-02 20:56:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "204" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  55] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/204 is now RUNNING
  [INFO ] [2015-07-02 20:56:06] [Logging$class:logInfo:59] Executor app-20150702201026-0001/204 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:07] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/205 for PageRank
  [INFO ] [2015-07-02 20:56:09] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "205" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  03] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/205 is now LOADING
  [INFO ] [2015-07-02 20:56:10] [Logging$class:logInfo:59] Executor app-20150702201026-0001/205 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:11] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/206 for PageRank
  [INFO ] [2015-07-02 20:56:12] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "206" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  07] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/206 is now LOADING
  [INFO ] [2015-07-02 20:56:12] [Logging$class:logInfo:59] Executor app-20150702201026-0001/206 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:13] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/207 for PageRank
  [INFO ] [2015-07-02 20:56:13] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "207" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:56:09] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/207 is now RUNNING
  [INFO ] [2015-07-02 20:56:14] [Logging$class:logInfo:59] Executor app-20150702201026-0001/207 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:15] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/208 for PageRank
  [INFO ] [2015-07-02 20:56:15] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "208" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:56:10] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/208 is now LOADING
  [INFO ] [2015-07-02 20:56:17] [Logging$class:logInfo:59] Executor app-20150702201026-0001/208 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/209 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:56:14] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/208 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:56:14] [Logging$class:logInfo:59] Executor app-20150702201026-0001/208 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:56:14] [Logging$class:logError:75] Asked to remove non-existent executor 208
  [INFO ] [2015-07-02 20:56:14] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/209 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:56:14] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/209 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:56:14] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/209 is now RUNNING
  [INFO ] [2015-07-02 20:56:19] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/209 for PageRank
  [INFO ] [2015-07-02 20:56:28] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "209" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:56:30] [Logging$class:logInfo:59] Executor app-20150702201026-0001/209 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:31] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/210 for PageRank
  :56:27] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/210 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:56:27] [Logging$class:logInfo:59] Executor app-20150702201026-0001/209 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:56:27] [Logging$class:logError:75] Asked to remove non-existent executor 209
  [INFO ] [2015-07-02 20:56:27] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/210 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:56:27] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/210 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:56:27] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/210 is now LOADING
  [INFO ] [2015-07-02 20:56:27] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/210 is now RUNNING
  [INFO ] [2015-07-02 20:56:32] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "210" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:56:34] [Logging$class:logInfo:59] Executor app-20150702201026-0001/210 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:35] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/211 for PageRank
  -02 20:56:30] [Logging$class:logError:75] Asked to remove non-existent executor 210
  [INFO ] [2015-07-02 20:56:30] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/210 because it is EXITED
  [INFO ] [2015-07-02 20:56:30] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/211 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:56:30] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/211 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:56:30] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/211 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:56:30] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/211 is now RUNNING
  [INFO ] [2015-07-02 20:56:31] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/211 is now LOADING
  [INFO ] [2015-07-02 20:56:36] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "211" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:56:37] [Logging$class:logInfo:59] Executor app-20150702201026-0001/211 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:38] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/212 for PageRank
  [INFO ] [2015-07-02 20:56:38] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "212" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  34] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/212 is now LOADING
  [INFO ] [2015-07-02 20:56:40] [Logging$class:logInfo:59] Executor app-20150702201026-0001/212 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:41] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/213 for PageRank
  [INFO ] [2015-07-02 20:56:41] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "213" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  37] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/213 is now LOADING
  [INFO ] [2015-07-02 20:56:42] [Logging$class:logInfo:59] Executor app-20150702201026-0001/213 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:42] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/214 for PageRank
  [INFO ] [2015-07-02 20:56:44] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "214" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  38] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/214 is now LOADING
  [INFO ] [2015-07-02 20:56:45] [Logging$class:logInfo:59] Executor app-20150702201026-0001/214 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/215 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:56:43] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/214 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:56:43] [Logging$class:logInfo:59] Executor app-20150702201026-0001/214 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:56:43] [Logging$class:logError:75] Asked to remove non-existent executor 214
  [INFO ] [2015-07-02 20:56:43] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/215 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:56:43] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/215 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:56:43] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/215 is now RUNNING
  [INFO ] [2015-07-02 20:56:48] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/215 for PageRank
  [INFO ] [2015-07-02 20:56:49] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "215" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:56:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/215 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/216 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:56:48] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/215 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:56:48] [Logging$class:logInfo:59] Executor app-20150702201026-0001/215 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:56:48] [Logging$class:logError:75] Asked to remove non-existent executor 215
  [INFO ] [2015-07-02 20:56:48] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/216 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:56:48] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/216 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:56:48] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/216 is now RUNNING
  [INFO ] [2015-07-02 20:56:54] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/216 for PageRank
  [INFO ] [2015-07-02 20:56:55] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "216" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:56:58] [Logging$class:logInfo:59] Executor app-20150702201026-0001/216 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:56:58] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/217 for PageRank
  [INFO ] [2015-07-02 20:57:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "217" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:57:02] [Logging$class:logInfo:59] Executor app-20150702201026-0001/217 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:57:02] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/218 for PageRank
  [INFO ] [2015-07-02 20:57:02] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "218" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:57:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/218 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/219 on worker worker-20150702200334-10.0.0.41-41593
  [INFO ] [2015-07-02 20:57:06] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/219 for PageRank
  [INFO ] [2015-07-02 20:57:06] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "219" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:57:11] [Logging$class:logInfo:59] Executor app-20150702201026-0001/219 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:57:11] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/220 for PageRank
  [INFO ] [2015-07-02 20:57:12] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "220" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:57:20] [Logging$class:logInfo:59] Executor app-20150702201026-0001/220 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:57:21] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/221 for PageRank
  [INFO ] [2015-07-02 20:57:21] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "221" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:57:31] [Logging$class:logInfo:59] Executor app-20150702201026-0001/221 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:57:31] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/222 for PageRank
  [INFO ] [2015-07-02 20:57:31] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "222" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:57:36] [Logging$class:logInfo:59] Executor app-20150702201026-0001/222 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:57:37] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/223 for PageRank
  [INFO ] [2015-07-02 20:57:37] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "223" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:57:38] [Logging$class:logInfo:59] Executor app-20150702201026-0001/223 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:57:38] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/224 for PageRank
  [INFO ] [2015-07-02 20:57:38] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "224" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:57:41] [Logging$class:logInfo:59] Executor app-20150702201026-0001/224 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 20:57:41] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/225 for PageRank
  [INFO ] [2015-07-02 20:57:43] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "225" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 20:57:44] [Logging$class:logInfo:59] Executor app-20150702201026-0001/225 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Executor updated: app-20150702201026-0001/216 is now LOADING
  [WARN ] [2015-07-02 20:57:48] [Logging$class:logWarning:71] Removing executor 1 with no recent heartbeats: 120215 ms exceeds timeout 120000 ms
  [INFO ] [2015-07-02 20:57:48] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/216 is now EXITED (Command exited with code 1)
  [ERROR] [2015-07-02 20:57:48] [Logging$class:logError:75] Lost executor 1 on 10.0.0.39: Executor heartbeat timed out after 120215 ms
  [INFO ] [2015-07-02 20:57:48] [Logging$class:logInfo:59] Executor app-20150702201026-0001/216 removed: Command exited with code 1
  [INFO ] [2015-07-02 20:57:48] [Logging$class:logInfo:59] Re-queueing tasks for 1 from TaskSet 0.0
  [WARN ] [2015-07-02 20:57:48] [Logging$class:logWarning:71] Lost task 5.0 in stage 0.0 (TID 5, 10.0.0.39): ExecutorLostFailure (executor 1 lost)
  [WARN ] [2015-07-02 20:57:48] [Logging$class:logWarning:71] Lost task 4.0 in stage 0.0 (TID 4, 10.0.0.39): ExecutorLostFailure (executor 1 lost)
  [WARN ] [2015-07-02 20:57:48] [Logging$class:logWarning:71] Lost task 7.0 in stage 0.0 (TID 7, 10.0.0.39): ExecutorLostFailure (executor 1 lost)
  [WARN ] [2015-07-02 20:57:48] [Logging$class:logWarning:71] Lost task 6.0 in stage 0.0 (TID 6, 10.0.0.39): ExecutorLostFailure (executor 1 lost)
  [INFO ] [2015-07-02 20:57:50] [Logging$class:logInfo:59] Executor lost: 1 (epoch 12)
  [INFO ] [2015-07-02 20:57:50] [Logging$class:logInfo:59] Trying to remove executor 1 from BlockManagerMaster.
  [ERROR] [2015-07-02 20:57:50] [Logging$class:logError:75] Asked to remove non-existent executor 216
  [INFO ] [2015-07-02 20:57:50] [Logging$class:logInfo:59] Removing block manager BlockManagerId(1, 10.0.0.39, 56818)
  [INFO ] [2015-07-02 20:57:50] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/217 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:57:50] [Logging$class:logInfo:59] Removed 1 successfully in removeExecutor
  [WARN ] [2015-07-02 20:57:50] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/217
  [INFO ] [2015-07-02 20:57:50] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/217 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:57:50] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/217 is now LOADING
  [INFO ] [2015-07-02 20:57:50] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/217 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:57:50] [Logging$class:logInfo:59] Executor app-20150702201026-0001/217 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:57:51] [Logging$class:logError:75] Asked to remove non-existent executor 217
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/218 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/218 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/218 is now LOADING
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/218 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/218 removed: Command exited with code 1
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Host added was in lost list earlier: 10.0.0.39
  [ERROR] [2015-07-02 20:57:51] [Logging$class:logError:75] Asked to remove non-existent executor 218
  [WARN ] [2015-07-02 20:57:51] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/218
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/219 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/219 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/219 is now LOADING
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/219 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/219 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:57:51] [Logging$class:logError:75] Asked to remove non-existent executor 219
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/220 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/220 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/220 is now LOADING
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/220 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/220 removed: Command exited with code 1
  [WARN ] [2015-07-02 20:57:51] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/219
  [WARN ] [2015-07-02 20:57:51] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/220
  [ERROR] [2015-07-02 20:57:51] [Logging$class:logError:75] Asked to remove non-existent executor 220
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/221 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [WARN ] [2015-07-02 20:57:51] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/221
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/221 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/221 is now LOADING
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/221 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/221 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:57:51] [Logging$class:logError:75] Asked to remove non-existent executor 221
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/222 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/222 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/222 is now LOADING
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/222 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/222 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:57:51] [Logging$class:logError:75] Asked to remove non-existent executor 222
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/223 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/223 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/223 is now LOADING
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/223 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/223 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:57:51] [Logging$class:logError:75] Asked to remove non-existent executor 223
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/224 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/224 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/224 is now LOADING
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/224 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/224 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:57:51] [Logging$class:logError:75] Asked to remove non-existent executor 224
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/225 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [WARN ] [2015-07-02 20:57:51] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/222
  [WARN ] [2015-07-02 20:57:51] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/223
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/225 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/225 is now LOADING
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/225 is now EXITED (Command exited with code 1)
  [WARN ] [2015-07-02 20:57:51] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/224
  [INFO ] [2015-07-02 20:57:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/225 removed: Command exited with code 1
  [ERROR] [2015-07-02 20:57:51] [Logging$class:logError:75] Asked to remove non-existent executor 225
  [WARN ] [2015-07-02 20:57:51] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/225
  [WARN ] [2015-07-02 20:57:55] [Logging$class:logWarning:71] Told to re-register on heartbeat
  [INFO ] [2015-07-02 20:57:55] [Logging$class:logInfo:59] BlockManager re-registering with master
  [INFO ] [2015-07-02 20:57:55] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:58:04] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:58:04] [Logging$class:logInfo:59] Reporting 4 blocks to the master.
  [WARN ] [2015-07-02 20:58:33] [Logging$class:logWarning:92] Error sending message [message = Heartbeat(3,[Lscala.Tuple2;@566908f9,BlockManagerId(3, 10.0.0.42, 53639))] in 1 attempts
  java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scal[INFO ] [2015-07-02 20:58:53] [Logging$class:logInfo:59] Executor app-20150702201026-0001/2 finished with state EXITED message Command exited with code 137 exitStatus 137
  [WARN ] [2015-07-02 20:58:59] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkExecutor@10.0.0.41:57374] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:464)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[WARN ] [2015-07-02 20:58:37] [Logging$class:logWarning:71] Told to re-register on heartbeat
  [INFO ] [2015-07-02 20:58:37] [Logging$class:logInfo:59] BlockManager re-registering with master
  [INFO ] [2015-07-02 20:58:37] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 20:58:38] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 20:58:38] [Logging$class:logInfo:59] Reporting 4 blocks to the master.
  [INFO ] [2015-07-02 20:59:21] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 355.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 20:59:30] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:00:42] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:00:48] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [WARN ] [2015-07-02 21:01:25] [Logging$class:logWarning:71] Removing executor 2 with no recent heartbeats: 175289 ms exceeds timeout 120000 ms
  [ERROR] [2015-07-02 21:01:27] [Logging$class:logError:75] Lost an executor 2 (already removed): Executor heartbeat timed out after 175289 ms
  [INFO ] [2015-07-02 21:02:58] [Logging$class:logInfo:59] Thread 57 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:03:31] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:03:32] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [WARN ] [2015-07-02 21:06:26] [Logging$class:logWarning:71] Removing executor 1 with no recent heartbeats: 147527 ms exceeds timeout 120000 ms
  [ERROR] [2015-07-02 21:06:28] [Logging$class:logError:75] Lost executor 1 on 10.0.0.39: Executor heartbeat timed out after 147527 ms
  [INFO ] [2015-07-02 21:06:28] [Logging$class:logInfo:59] Re-queueing tasks for 1 from TaskSet 0.0
  [WARN ] [2015-07-02 21:06:32] [Logging$class:logWarning:71] Removing executor 3 with no recent heartbeats: 152062 ms exceeds timeout 120000 ms
  [ERROR] [2015-07-02 21:06:32] [Logging$class:logError:75] Lost executor 3 on 10.0.0.42: Executor heartbeat timed out after 152062 ms
  [INFO ] [2015-07-02 21:06:32] [Logging$class:logInfo:59] Re-queueing tasks for 3 from TaskSet 0.0
  [INFO ] [2015-07-02 21:06:34] [Logging$class:logInfo:59] Executor lost: 1 (epoch 15)
  [INFO ] [2015-07-02 21:06:34] [Logging$class:logInfo:59] Trying to remove executor 1 from BlockManagerMaster.
  [INFO ] [2015-07-02 21:06:34] [Logging$class:logInfo:59] Removing block manager BlockManagerId(1, 10.0.0.39, 56818)
  [INFO ] [2015-07-02 21:06:34] [Logging$class:logInfo:59] Removed 1 successfully in removeExecutor
  [INFO ] [2015-07-02 21:06:35] [Logging$class:logInfo:59] Executor lost: 3 (epoch 18)
  [INFO ] [2015-07-02 21:06:35] [Logging$class:logInfo:59] Trying to remove executor 3 from BlockManagerMaster.
  [INFO ] [2015-07-02 21:06:35] [Logging$class:logInfo:59] Removing block manager BlockManagerId(3, 10.0.0.42, 53639)
  [INFO ] [2015-07-02 21:06:35] [Logging$class:logInfo:59] Removed 3 successfully in removeExecutor
  [INFO ] [2015-07-02 21:06:35] [Logging$class:logInfo:59] Host added was in lost list earlier: 10.0.0.42
  [WARN ] [2015-07-02 21:06:43] [Logging$class:logWarning:71] Told to re-register on heartbeat
  [INFO ] [2015-07-02 21:06:43] [Logging$class:logInfo:59] BlockManager re-registering with master
  [INFO ] [2015-07-02 21:06:43] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 21:06:44] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 21:06:44] [Logging$class:logInfo:59] Reporting 4 b[WARN ] [2015-07-02 21:07:10] [Logging$class:logWarning:71] Told to re-register on heartbeat
  [INFO ] [2015-07-02 21:07:10] [Logging$class:logInfo:59] BlockManager re-registering with master
  [INFO ] [2015-07-02 21:07:10] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 21:07:11] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 21:07:11] [Logging$class:logInfo:59] Reporting 4 b[INFO ] [2015-07-02 21:[INFO ] [2015-07-02 21:07:58] [Logging$class:logInfo:59] Finished task 10.0 in stage 0.0 (TID 10). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 21:08:02] [Logging$class:logInfo:59] Finished task 9.0 in stage 0.0 (TID 9). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 21:08:02] [Logging$class:logInfo:59] Got assigned task 20
  [INFO ] [2015-07-02 21:08:02] [Logging$class:logInfo:59] Running task 6.1 in stage 0.0 (TID 20)
  [INFO ] [2015-07-02 21:08:02] [Logging$class:logInfo:59] Got assigned task 21
  [INFO ] [2015-07-02 21:08:02] [Logging$class:logInfo:59] Running task 7.1 in stage 0.0 (TID 21)
  [INFO ] [2015-07-02 21:08:02] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:402653184+67108864
  [INFO ] [2015-07-02 21:08:02] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:469762048+67108864
  [INFO ] [2015-07-02 21:08:03] [Logging$class:logInfo:59] Finished task 8.0 in stage 0.0 (TID 8). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 21:08:04] [Logging$class:logInfo:59] Got assigned task 22
  [INFO ] [2015-07-02 21:08:04] [Logging$class:logInfo:59] Running task 4.1 in stage 0.0 (TID 22)
  [INFO ] [2015-07-02 21:08:04] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:268435456+67108864
  [INFO ] [2015-07-02 21:09:21] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 355.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:09:23] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:09:23] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:10:28] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:10:34] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:11:02] [Logging$class:logInfo:59] Finished task 4.0 in stage 0.0 (TID 4). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 21:11:02] [Logging$class:logInfo:59] Got assigned task 23
  [INFO ] [2015-07-02 21:11:02] [Logging$class:logInfo:59] Running task 5.1 in stage 0.0 (TID 23)
  [INFO ] [2015-07-02 21:11:02] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:335544320+67108864
  [INFO ] [2015-07-02 21:11:03] [Logging$class:logInfo:59] Finished task 7.0 in stage 0.0 (TID 7). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 21:11:04] [Logging$class:logInfo:59] Got assigned task 24
  [INFO ] [2015-07-02 21:11:04] [Logging$class:logInfo:59] Running task 0.1 in stage 0.0 (TID 24)
  [INFO ] [2015-07-02 21:11:04] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:0+67108864
  [INFO ] [2015-07-02 21:11:08] [Logging$class:logInfo:59] Finished task 5.0 in stage 0.0 (TID 5). 1945 bytes result sent to driver
  15-07-02 21:11:04] [Logging$class:logInfo:59] Ignoring possibly bogus ShuffleMapTask completion from 1
  [INFO ] [2015-07-02 21:11:09] [Logging$class:logInfo:59] Got assigned task 25
  [INFO ] [2015-07-02 21:11:09] [Logging$class:logInfo:59] Running task 12.1 in stage 0.0 (TID 25)
  [INFO ] [2015-07-02 21:11:09] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:805306368+67108864
  [WARN ] [2015-07-02 21:11:28] [Logging$class:logWarning:71] Removing worker-20150702200334-10.0.0.41-41593 because we got no heartbeat in 60 seconds
  [INFO ] [2015-07-02 21:11:29] [Logging$class:logInfo:59] Removing worker worker-20150702200334-10.0.0.41-41593 on 10.0.0.41:41593
  [WARN ] [2015-07-02 21:11:33] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 21:11:33] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 21:11:33] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 21:11:33] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 21:11:34] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 21:11:34] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [WARN ] [2015-07-02 21:11:35] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200334-10.0.0.41-41593. Asking it to re-register.
  [INFO ] [2015-07-02 21:11:40] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:11:40] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-07-02 21:11:41] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:11:41] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:11:41] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:11:41] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:11:41] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:11:41] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:11:42] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:11:42] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:11:42] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:11:42] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:11:48] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:11:52] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:11:52] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:11:52] [Logging$class:logInfo:59] Not spawning an[INFO ] [2015-07-02 21:12:07] [Logging$class:logInfo:59] Finished task 6.0 in stage 0.0 (TID 6). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 21:12:07] [Logging$class:logInfo:59] Got assigned task 26
  [INFO ] [2015-07-02 21:12:07] [Logging$class:logInfo:59] Running task 3.1 in stage 0.0 (TID 26)
  [INFO ] [2015-07-02 21:12:08] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:201326592+67108864
  [INFO ] [2015-07-02 21:12:15] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:12:42] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 348.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:12:54] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 358.4 MB to disk (1 time so far)
  [WARN ] [2015-07-02 21:13:28] [Logging$class:logWarning:71] Removing worker-20150702200334-10.0.0.41-41593 because we got no heartbeat in 60 seconds
  [INFO ] [2015-07-02 21:13:29] [Logging$class:logInfo:59] Removing worker worker-20150702200334-10.0.0.41-41593 on 10.0.0.41:41593
  [INFO ] [2015-07-02 21:13:29] [Logging$class:logInfo:59] Telling app of lost executor: 226
  [INFO ] [2015-07-02 21:13:29] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/226 is now LOST (worker lost)
  [INFO ] [2015-07-02 21:13:29] [Logging$class:logInfo:59] Executor app-20150702201026-0001/226 removed: worker lost
  [ERROR] [2015-07-02 21:13:29] [Logging$class:logError:75] Asked to remove non-existent executor 226
  [WARN ] [2015-07-02 21:14:30] [Logging$class:logWarning:71] Removing worker-20150702200419-10.0.0.39-55458 because we got no heartbeat in 60 seconds
  [INFO ] [2015-07-02 21:14:30] [Logging$class:logInfo:59] Removing worker worker-20150702200419-10.0.0.39-55458 on 10.0.0.39:55458
  [INFO ] [2015-07-02 21:14:30] [Logging$class:logInfo:59] Telling app of lost executor: 1
  [INFO ] [2015-07-02 21:14:31] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/1 is now LOST (worker lost)
  [INFO ] [2015-07-02 21:14:31] [Logging$class:logInfo:59] Executor app-20150702201026-0001/1 removed: worker lost
  [ERROR] [2015-07-02 21:14:31] [Logging$class:logError:75] Lost executor 1 on 10.0.0.39: worker lost
  [INFO ] [2015-07-02 21:14:31] [Logging$class:logInfo:59] Re-queueing tasks for 1 from TaskSet 0.0
  [WARN ] [2015-07-02 21:14:32] [Logging$class:logWarning:71] Lost task 5.1 in stage 0.0 (TID 23, 10.0.0.39): ExecutorLostFailure (executor 1 lost)
  [INFO ] [2015-07-02 21:14:32] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 5), so marking it as still running
  [WARN ] [2015-07-02 21:14:33] [Logging$class:logWarning:71] Lost task 3.1 in stage 0.0 (TID 26, 10.0.0.39): ExecutorLostFailure (executor 1 lost)
  [WARN ] [2015-07-02 21:14:33] [Logging$class:logWarning:71] Lost task 12.1 in stage 0.0 (TID 25, 10.0.0.39): ExecutorLostFailure (executor 1 lost)
  [INFO ] [2015-07-02 21:14:33] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 4), so marking it as still running
  [WARN ] [2015-07-02 21:14:33] [Logging$class:logWarning:71] Lost task 0.1 in stage 0.0 (TID 24, 10.0.0.39): ExecutorLostFailure (executor 1 lost)
  [INFO ] [2015-07-02 21:14:33] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 7), so marking it as still running
  [INFO ] [2015-07-02 21:14:34] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 6), so marking it as still running
  [INFO ] [2015-07-02 21:14:34] [Logging$class:logInfo:59] Executor lost: 1 (epoch 21)
  [INFO ] [2015-07-02 21:14:34] [Logging$class:logInfo:59] Trying to remove executor 1 from BlockManagerMaster.
  [INFO ] [2015-07-02 21:14:34] [Logging$class:logInfo:59] Removing block manager BlockManagerId(1, 10.0.0.39, 56818)
  [INFO ] [2015-07-02 21:14:34] [Logging$class:logInfo:59] Removed 1 successfully in removeExecutor
  [WARN ] [2015-07-02 21:14:43] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200419-10.0.0.39-55458. Asking it to re-register.
  [WARN ] [2015-07-02 21:14:43] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200419-10.0.0.39-55458. Asking it to re-register.
  [WARN ] [2015-07-02 21:14:47] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200419-10.0.0.39-55458. Asking it to re-register.
  [WARN ] [2015-07-02 21:14:48] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200419-10.0.0.39-55458. Asking it to re-register.
  [WARN ] [2015-07-02 21:14:51] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200419-10.0.0.39-55458. Asking it to re-register.
  [WARN ] [2015-07-02 21:14:51] [Logging$class:logWarning:71] Got heartbeat from unregistered worker worker-20150702200419-10.0.0.39-55458. Asking it to re-register.
  [INFO ] [2015-07-02 21:15:07] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:08] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-07-02 21:15:08] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:10] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:10] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:10] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:10] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:10] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:10] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:10] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:10] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:10] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:23] [Logging$class:logInfo:59] Retrying connection to master (attempt # 1)
  [INFO ] [2015-07-02 21:15:23] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  [INFO ] [2015-07-02 21:15:24] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/227 for PageRank
  tor 3 lost)
  [INFO ] [2015-07-02 21:15:26] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 10), so marking it as still running
  [INFO ] [2015-07-02 21:15:26] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 9), so marking it as still running
  [INFO ] [2015-07-02 21:15:26] [Logging$class:logInfo:59] Executor lost: 3 (epoch 24)
  [INFO ] [2015-07-02 21:15:26] [Logging$class:logInfo:59] Trying to remove executor 3 from BlockManagerMaster.
  [INFO ] [2015-07-02 21:15:26] [Logging$class:logInfo:59] Removing block manager BlockManagerId(3, 10.0.0.42, 53639)
  [INFO ] [2015-07-02 21:15:26] [Logging$class:logInfo:59] Removed 3 successfully in removeExecutor
  [INFO ] [2015-07-02 21:15:26] [Logging$class:logInfo:59] ShuffleMapStage 0 is now unavailable on executor 3 (0/20, false)
  [INFO ] [2015-07-02 21:15:26] [Logging$class:logInfo:59] Host added was in lost list earlier: 10.0.0.42
  [INFO ] [2015-07-02 21:15:28] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/227 is now LOADING
  [INFO ] [2015-07-02 21:15:34] [Logging$class:logInfo:59] Retrying connection to master (attempt # 1)
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Master with url spark://spark1:7077 requested this worker to reconnect.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Not spawning another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:35] [Logging$class:logInfo:59] Retrying connection to master (attempt # 2)
  [INFO ] [2015-07-02 21:15:35] [L[WARN ] [2015-07-02 21:15:50] [Logging$class:logWarning:71] Told to re-register on heartbeat
  [INFO ] [2015-07-02 21:15:51] [Logging$class:logInfo:59] BlockManager re-registering with master
  [INFO ] [2015-07-02 21:15:51] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 21:15:51] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 21:15:51] [Logging$class:logInfo:59] Reporting 4 blocks to the master.
   another attempt to register with the master, since there is an attempt scheduled already.
  [INFO ] [2015-07-02 21:15:36] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  [INFO ] [2015-07-02 21:15:39] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/226 for PageRank
  [INFO ] [2015-07-02 21:15:46] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  [INFO ] [2015-07-02 21:15:46] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/228 for PageRank
  [INFO ] [2015-07-02 21:16:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "226" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 21:16:01] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "228" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [WARN ] [2015-07-02 21:16:08] [Logging$class:logWarning:71] Told to re-register on heartbeat
  [INFO ] [2015-07-02 21:16:08] [Logging$class:logInfo:59] BlockManager re-registering with master
  [INFO ] [2015-07-02 21:16:08] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 21:16:08] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 21:16:08] [Logging$class:logInfo:59] Reporting 4 blocks to the master.
  [INFO ] [2015-07-02 21:16:35] [Logging$class:logInfo:59] Finished task 11.0 in stage 0.0 (TID 11). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 21:16:36] [Logging$class:logInfo:59] Got assigned task 27
  [INFO ] [2015-07-02 21:16:36] [Logging$class:logInfo:59] Running task 0.2 in stage 0.0 (TID 27)
  [INFO ] [2015-07-02 21:16:36] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:0+67108864
  [INFO ] [2015-07-02 21:16:37] [Logging$class:logInfo:59] Finished task 11.0 in stage 0.0 (TID 11) in 3846477 ms on 10.0.0.42 (5/20)
  [INFO ] [2015-07-02 21:17:06] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 21:17:59] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 21:18:00] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:18:00] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 21:18:08] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:19:05] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:19:22] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [ERROR] [2015-07-02 21:19:28] [UserGroupInformation:doAs:1193] PriviledgedActionException as:spark cause:java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
  [INFO ] [2015-07-02 21:19:29] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 21:19:29] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 21:19:39] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 459.9 MB to disk (2 times so far)
  nfo:59] Executor app-20150702201026-0001/227 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 21:19:41] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/229 for PageRank
  -02 21:19:36] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/227 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 21:19:36] [Logging$class:logInfo:59] Executor app-20150702201026-0001/227 removed: Command exited with code 1
  [ERROR] [2015-07-02 21:19:36] [Logging$class:logError:75] Asked to remove non-existent executor 227
  [INFO ] [2015-07-02 21:19:36] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/229 on worker-20150702200419-10.0.0.39-55458 (10.0.0.39:55458) with 4 cores
  [INFO ] [2015-07-02 21:19:36] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/229 on hostPort 10.0.0.39:55458 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 21:19:36] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/229 is now RUNNING
  [INFO ] [2015-07-02 21:19:38] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/229 is now LOADING
  [INFO ] [2015-07-02 21:19:42] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "229" "--hostname" "10.0.0.39" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.39:55458/user/Worker"
  [INFO ] [2015-07-02 21:21:28] [Logging$class:logInfo:59] Thread 57 spilling in-memory map of 348.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:21:48] [Logging$class:logInfo:59] Finished task 7.1 in stage 0.0 (TID 21). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 21:21:48] [Logging$class:logInfo:59] Got assigned task 28
  [INFO ] [2015-07-02 21:21:48] [Logging$class:logInfo:59] Running task 12.2 in stage 0.0 (TID 28)
  [INFO ] [2015-07-02 21:21:48] [Logging$class:logInfo:59] Input split: hdfs://sp[INFO ] [2015-07-02 21:21:53] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  Info:59] Finished task 4.1 in stage 0.0 (TID 22). 1945 bytes result sent to driver
  [INFO ] [2015-07-02 21:21:49] [Logging$class:logInfo:59] Got assigned task 29
  [INFO ] [2015-07-02 21:21:49] [Logging$class:logInfo:59] Running task 3.2 in stage 0.0 (TID 29)
  [INFO ] [2015-07-02 21:21:49] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:201326592+67108864
  [INFO ] [2015-07-02 21:22:24] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:22:27] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 [INFO ] [2015-07-02 21:22:25] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  $class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 21:22:33] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:23:14] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:23:20] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  ogInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:23:25] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:23:25] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 21:23:25] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  15-07-02 21:23:33] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 21:23:33] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 21:23:35] [Logging$class:logInfo:59] Executor app-20150702201026-0001/229 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 21:23:36] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/230 for PageRank
  [INFO ] [2015-07-02 21:23:37] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "230" "--hostname" "10.0.0.39" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.39:55458/user/Worker"
  33] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/230 is now LOADING
  [INFO ] [2015-07-02 21:24:00] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 354.6 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:24:05] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 21:24:15] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 21:24:15] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:24:15] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
   still running
  [WARN ] [2015-07-02 21:24:26] [Logging$class:logWarning:71] Lost task 3.2 in stage 0.0 (TID 29, 10.0.0.42): ExecutorLostFailure (executor 3 lost)
  [WARN ] [2015-07-02 21:24:26] [Logging$class:logWarning:71] Lost task 12.2 in stage 0.0 (TID 28, 10.0.0.42): ExecutorLostFailure (executor 3 lost)
  [WARN ] [2015-07-02 21:24:26] [Logging$class:logWarning:71] Lost task 0.2 in stage 0.0 (TID 27, 10.0.0.42): ExecutorLostFailure (executor 3 lost)
  [INFO ] [2015-07-02 21:24:26] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 4), so marking it as still running
  [INFO ] [2015-07-02 21:24:26] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 7), so marking it as still running
  [INFO ] [2015-07-02 21:24:26] [Logging$class:logInfo:59] Executor lost: 3 (epoch 27)
  [INFO ] [2015-07-02 21:24:26] [Logging$class:logInfo:59] Trying to remove executor 3 from BlockManagerMaster.
  [INFO ] [2015-07-02 21:24:26] [Logging$class:logInfo:59] Removing block manager BlockManagerId(3, 10.0.0.42, 53639)
  [INFO ] [2015-07-02 21:24:26] [Logging$class:logInfo:59] Removed 3 successfully in removeExecutor
  [INFO ] [2015-07-02 21:24:26] [Logging$class:logInfo:59] ShuffleMapStage 0 is now unavailable on executor 3 (0/20, false)
  [INFO ] [2015-07-02 21:24:26] [Logging$class:logInfo:59] Host added was in lost list earlier: 10.0.0.42
  [INFO ] [2015-07-02 21:24:36] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:24:39] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  :24:47] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:24:57] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [ERROR] [2015-07-02 21:25:07] [UserGroupInformation:doAs:1193] PriviledgedActionException as:spark cause:java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
  [INFO ] [2015-07-02 21:25:07] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 21:25:08] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 21:25:08] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 21:25:19] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [ERROR] [2015-07-02 21:25:19] [UserGroupInformation:doAs:1193] PriviledgedActionException as:spark cause:java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
  [INFO ] [2015-07-02 21:25:21] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 21:25:19] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 21:25:25] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 21:25:25] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/228 because it is EXITED
  [ERROR] [2015-07-02 21:25:30] [UserGroupInformation:doAs:1193] PriviledgedActionException as:spark cause:java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
  [INFO ] [2015-07-02 21:25:30] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 21:25:30] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
   to remove non-existent executor 228
  [INFO ] [2015-07-02 21:25:26] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/231 on worker-20150702200334-10.0.0.41-41593 (10.0.0.41:41593) with 4 cores
  [INFO ] [2015-07-02 21:25:26] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/231 on hostPort 10.0.0.41:41593 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 21:25:27] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/231 is now RUNNING
  [INFO ] [2015-07-02 21:25:27] [Logging$class:logInfo:59] Executor app-20150702201026-0001/228 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 21:25:33] [Logging$class:logInfo:59] Executor app-20150702201026-0001/226 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 21:25:33] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/231 for PageRank
  gInfo:59] Asked to launch executor app-20150702201026-0001/232 for PageRank
  [INFO ] [2015-07-02 21:25:35] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "232" "--hostname" "10.0.0.39" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.39:55458/user/Worker"
  [INFO ] [2015-07-02 21:25:31] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/232 on hostPort 10.0.0.39:55458 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 21:25:31] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/232 is now LOADING
  [INFO ] [2015-07-02 21:25:31] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/232 is now RUNNING
  [INFO ] [2015-07-02 21:25:34] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "231" "--hostname" "10.0.0.41" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker"
  [INFO ] [2015-07-02 21:25:40] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 21:25:41] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 21:25:41] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:25:41] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 21:25:43] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:25:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  ing$class:logInfo:59] Removed 3 successfully in removeExecutor
  [INFO ] [2015-07-02 21:25:42] [Logging$class:logInfo:59] Executor app-20150702201026-0001/3 finished with state EXITED message Command exited with code 1 exitStatus 1
  [WARN ] [2015-07-02 21:25:45] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkExecutor@10.0.0.42:46768] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 21:25:46] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/233 for PageRank
  e 1
  [ERROR] [2015-07-02 21:25:40] [Logging$class:logError:75] Asked to remove non-existent executor 3
  [INFO ] [2015-07-02 21:25:40] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/233 on worker-20150702200417-10.0.0.42-36843 (10.0.0.42:36843) with 4 cores
  [INFO ] [2015-07-02 21:25:40] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/233 on hostPort 10.0.0.42:36843 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 21:25:40] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/233 is now RUNNING
  [INFO ] [2015-07-02 21:25:42] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/233 is now LOADING
  [INFO ] [2015-07-02 21:25:48] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.41:60960]
  [INFO ] [2015-07-02 21:25:48] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 60960.
  _managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "233" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  [INFO ] [2015-07-02 21:25:55] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 21:25:55] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:25:55] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 21:25:55] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 21:25:55] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 21:25:57] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:25:57] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 21:25:57] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 21:25:57] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.0.41:52942]
  [INFO ] [2015-07-02 21:25:58] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 52942.
  [INFO ] [2015-07-02 21:26:01] [Logging$class:logInfo:59] Created local directory at /tmp/spark-e4904e3b-0d4a-410e-838e-4b7d2fe544ab/executor-b0c44b47-b315-4bee-a7e3-9acaa0703452/blockmgr-8a1c3003-0384-4661-a4b0-948903dfc6a9
  [INFO ] [2015-07-02 21:26:01] [Logging$class:logInfo:59] MemoryStore started with capacity 4.5 GB
  [INFO ] [2015-07-02 21:26:05] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler
  [INFO ] [2015-07-02 21:26:05] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker
  [INFO ] [2015-07-02 21:26:07] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-07-02 21:26:07] [Logging$class:logInfo:59] Starting executor ID 231 on host 10.0.0.41
  [INFO ] [2015-07-02 21:26:11] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 21:26:18] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 21:26:18] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:26:18] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); [INFO ] [2015-07-02 21:26:22] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 21:26:25] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@10.0.0.41:41593/user/Worker
  [INFO ] [2015-07-02 21:26:25] [Logging$class:logInfo:59] Got assigned task 30
  [INFO ] [2015-07-02 21:26:25] [Logging$class:logInfo:59] Got assigned task 31
  [INFO ] [2015-07-02 21:26:25] [Logging$class:logInfo:59] Got assigned task 32
  [INFO ] [2015-07-02 21:26:28] [Logging$class:logInfo:59] Got assigned task 33
  [INFO ] [2015-07-02 21:26:25] [Logging$class:logInfo:59] Running task 0.3 in stage 0.0 (TID 30)
  [INFO ] [2015-07-02 21:26:28] [Logging$class:logInfo:59] Running task 12.3 in stage 0.0 (TID 31)
  [INFO ] [2015-07-02 21:26:28] [Logging$class:logInfo:59] Running task 3.3 in stage 0.0 (TID 32)
  [INFO ] [2015-07-02 21:26:29] [Logging$class:logInfo:59] Running task 1.1 in stage 0.0 (TID 33)
  [INFO ] [2015-07-02 21:26:32] [Logging$class:logInfo:59] Fetching http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar with timestamp 1435839025941
  [INFO ] [2015-07-02 21:26:33] [Logging$class:logInfo:59] Copying /tmp/spark-e4904e3b-0d4a-410e-838e-4b7d2fe544ab/executor-b0c44b47-b315-4bee-a7e3-9acaa0703452/-4441382601435839025941_cache to /home/spark/spark-1.3.1/work/app-20150702201026-0001/231/./spark-examples-1.4.0-hadoop1.2.1.jar
  07-02 21:26:26] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 21:26:26] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 21:26:26] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.0.42:36686]
  [INFO ] [2015-07-02 21:26:26] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 36686.
  [INFO ] [2015-07-02 21:26:28] [Logging$class:logInfo:59] Created local directory at /tmp/spark-355db919-c195-4621-a3f6-e4ec22ae1b37/executor-d0afeb26-6e8a-4200-ac74-40a10210deaf/blockmgr-0ab540be-dbdb-45d4-80d2-602801223ae4
  [INFO ] [2015-07-02 21:26:28] [Logging$class:logInfo:59] MemoryStore started with capacity 4.5 GB
  [INFO ] [2015-07-02 21:26:30] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler
  [INFO ] [2015-07-02 21:26:30] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker
  [INFO ] [2015-07-02 21:26:35] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-07-02 21:26:36] [Logging$class:logInfo:59] Starting executor ID 233 on host 10.0.0.42
  [INFO ] [2015-07-02 21:26:38] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker
  [INFO ] [2015-07-02 21:26:41] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 358[INFO ] [2015-07-02 21:26:42] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 21:26:42] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:26:42] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 21:26:51] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:26:51] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 21:26:41] [Logging$class:logInfo:59] Running task 16.1 in stage 0.0 (TID 34)
  [INFO ] [2015-07-02 21:26:41] [Logging$class:logInfo:59] Running task 14.1 in stage 0.0 (TID 36)
  [INFO ] [2015-07-02 21:26:41] [Logging$class:logInfo:59] Got assigned task 37
  [INFO ] [2015-07-02 21:26:41] [Logging$class:logInfo:59] Running task 2.1 in stage 0.0 (TID 37)
  [INFO ] [2015-07-02 21:26:42] [Logging$class:logInfo:59] Fetching http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar with timestamp 1435839025941
  [INFO ] [2015-07-02 21:26:42] [Logging$class:logInfo:59] Copying /tmp/spark-355db919-c195-4621-a3f6-e4ec22ae1b37/executor-d0afeb26-6e8a-4200-ac74-40a10210deaf/-4441382601435839025941_cache to /home/spark/spark-1.3.1/work/app-20150702201026-0001/233/./spark-examples-1.4.0-hadoop1.2.1.jar
  [ERROR] [2015-07-02 21:27:01] [UserGroupInformation:doAs:1193] PriviledgedActionException as:spark cause:java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
  [INFO ] [2015-07-02 21:27:03] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 21:27:03] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 21:27:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/232 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 21:27:04] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/234 for PageRank
  [INFO ] [2015-07-02 21:27:04] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "234" "--hostname" "10.0.0.39" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.39:55458/user/Worker"
  00] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/234 is now RUNNING
  [INFO ] [2015-07-02 21:27:06] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-07-02 21:27:08] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 21:27:08] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:27:08] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 21:27:10] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:27:11] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [ERROR] [2015-07-02 21:27:26] [UserGroupInformation:doAs:1193] PriviledgedActionException as:spark cause:java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
  [INFO ] [2015-07-02 21:27:26] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 21:27:26] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 21:27:40] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 21:28:49] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 21:29:25] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:29:24] [Logging$class:logInfo:59] Executor app-20150702201026-0001/234 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 21:29:26] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/235 for PageRank
  -02 21:29:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/234 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 21:29:22] [Logging$class:logInfo:59] Executor app-20150702201026-0001/234 removed: Command exited with code 1
  [ERROR] [2015-07-02 21:29:22] [Logging$class:logError:75] Asked to remove non-existent executor 234
  [INFO ] [2015-07-02 21:29:22] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/235 on worker-20150702200419-10.0.0.39-55458 (10.0.0.39:55458) with 4 cores
  [INFO ] [2015-07-02 21:29:22] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/235 on hostPort 10.0.0.39:55458 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 21:29:22] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/235 is now RUNNING
  [INFO ] [2015-07-02 21:29:26] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/235 is now LOADING
  [INFO ] [2015-07-02 21:29:37] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "235" "--hostname" "10.0.0.39" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.39:55458/user/Worker"
  [INFO ] [2015-07-02 21:31:18] [Logging$class:logInfo:59] Finished task 12.1 in stage 0.0 (TID 25). 1945 bytes result sent to driver
  er with ID $executorId
  [INFO ] [2015-07-02 21:31:15] [Logging$class:logInfo:59] Finished task 12.1 in stage 0.0 (TID 25) in 1210472 ms on 10.0.0.39 (5/20)
  [INFO ] [2015-07-02 21:31:15] [Logging$class:logInfo:59] Ignoring possibly bogus ShuffleMapTask completion from 1
  [INFO ] [2015-07-02 21:31:50] [Logging$class:logInfo:59] Finished task 0.1 in stage 0.0 (TID 24). 1945 bytes result sent to driver
  der with ID $executorId
  [INFO ] [2015-07-02 21:31:48] [Logging$class:logInfo:59] Ignoring possibly bogus ShuffleMapTask completion from 1
  [INFO ] [2015-07-02 21:31:48] [Logging$class:logInfo:59] Finished task 0.1 in stage 0.0 (TID 24) in 1247943 ms on 10.0.0.39 (6/20)
  [INFO ] [2015-07-02 21:32:09] [Logging$class:logInfo:59] Finished task 3.1 in stage 0.0 (TID 26). 1945 bytes result sent to driver
  der with ID $executorId
  [INFO ] [2015-07-02 21:32:10] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150702201026-0001/233/./spark-examples-1.4.0-hadoop1.2.1.jar to class loader
  ask 3.1 in stage 0.0 (TID 26) in 1202600 ms on 10.0.0.39 (7/20)
  [INFO ] [2015-07-02 21:32:14] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ] [2015-07-02 21:32:15] [Logging$class:l[INFO ] [2015-07-02 21:32:16] [Logging$class:logInfo:59] Finished task 5.1 in stage 0.0 (TID 23). 1945 bytes result sent to driver
  lock broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 4.5 GB)
  [INFO ] [2015-07-02 21:32:15] [Logging$class:logInfo:59] Reading broadcast variable 1 took 1413 ms
  Info:59] Finished task 5.1 in stage 0.0 (TID 23) in 1274652 ms on 10.0.0.39 (8/20)
  [INFO ] [2015-07-02 21:32:17] [Logging$class:logInfo:59] ensureFreeSpace(4024) called with curMem=2291, maxMem=4829950771
  [INFO ] [2015-07-0[INFO ] [2015-07-02 21:32:38] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150702201026-0001/231/./spark-examples-1.4.0-hadoop1.2.1.jar to class loader
  [INFO ] [2015-07-02 21:32:39] [Logging$class:logInfo:59] Started reading broadcast variable 1
  02 21:32:17] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:134217728+67108864
  [INFO ] [2015-07-02 21:32:17] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:939524096+67108864
  [INFO ] [2015-07-02 21:32:17] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:1073741824+67108864
  [INFO ] [2015-07-02 21:32:17] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-07-02 21:32:18] [Logging$class:logInfo:59] ensureFreeSpace(4294) called with curMem=6315, maxMem=4829950771
  [INFO ] [2015-07-02 21:32:18] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 4.5 GB)
  [INFO ] [2015-07-02 21:32:18] [Logging$class:logInfo:59] Reading broadcast variable 0 took 937 ms
  [INFO ] [2015-07-02 21:32:19] [Logging$class:logInfo:59] ensureFreeSpace(71248) called with curMem=10609, maxMem=4829950771
  [INFO ] [2015-07-02 21:32:19] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 69.6 KB, free 4.5 GB)
  [WARN ] [2015-07-02 21:32:21] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-07-02 21:32:21] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-07-02 21:32:35] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:32:37] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.9 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:32:37] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:32:37] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 346.2 MB to disk (1 time so far)
  [INFO ] [2015-07-02 21:32:59] [Logging$class:logInfo:59] Thread 53 spilling in-memory map of 355.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:33:00] [Logging$class:logInfo:59] Thread 55 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:33:00] [Logging$class:logInfo:59] Thread 56 spilling in-memory map of 346.2 MB to disk (2 times so far)
  [INFO ] [2015-07-02 21:33:00] [Logging$class:logInfo:59] Thread 54 spilling in-memory map of 346.2 MB to disk (2 times so far)
  cala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:89)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:592)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:590)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:590)
	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:584)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1(TorrentBroadcast.scala:127)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply(TorrentBroadcast.scala:137)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1254)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:62)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:32:43] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-07-02 21:32:53] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 1 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:32:53] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (2/3) for 1 outstanding blocks after 5000 ms
  [INFO ] [2015-07-02 21:33:15] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 21:33:15] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:33:15] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 21:33:21] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:33:21] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 21:33:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.39:50734]
  [INFO ] [2015-07-02 21:33:27] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 50734.
  [INFO ] [2015-07-02 21:33:32] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 21:33:32] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 21:33:32] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 21:33:32] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 21:33:32] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 21:33:32] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 21:33:32] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 21:33:32] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.0.39:50764]
  [INFO ] [2015-07-02 21:33:32] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 50764.
  [INFO ] [2015-07-02 21:33:33] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 21:33:34] [Logging$class:logInfo:59] Created local directory at /tmp/spark-19286363-5ffb-4037-aed7-3b6a627f7d32/executor-d555957d-2ef7-4437-a53b-0a0c624669be/blockmg[ERROR] [2015-07-02 21:33:45] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 2 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:33:45] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (3/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-07-02 21:33:56] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 3 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:33:56] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [ERROR] [2015-07-02 21:33:56] [Logging$class:logError:96] Exception in task 3.3 in stage 0.0 (TID 32)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[ERROR] [2015-07-02 21:33:58] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks 
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:89)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:592)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:590)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:590)
	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:584)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1(TorrentBroadcast.scala:127)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply(TorrentBroadcast.scala:137)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1254)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:62)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:33:58] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-07-02 21:34:03] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 1 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:03] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (2/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-07-02 21:34:08] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 2 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:08] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (3/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-07-02 21:34:13] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 3 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[ERROR] [2015-07-02 21:34:13] [Logging$class:logError:96] Exception in task 0.3 in stage 0.0 (TID 30)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:13] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [ERROR] [2015-07-02 21:34:14] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks 
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:89)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:592)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:590)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:590)
	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:584)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1(TorrentBroadcast.scala:127)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply(TorrentBroadcast.scala:137)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1254)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:62)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:14] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
  [INFO ] [2015-07-02 21:34:18] [Logging$class:logInfo:59] Finished task 2.1 in stage 0.0 (TID 37). 1946 bytes result sent to driver
 [ERROR] [2015-07-02 21:34:19] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 1 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:19] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (2/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-07-02 21:34:24] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 2 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:24] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (3/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-07-02 21:34:29] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 3 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[ERROR] [2015-07-02 21:34:29] [Logging$class:logError:96] Exception in task 1.1 in stage 0.0 (TID 33)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:29] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ] [2015-07-02 21:34:33] [Logging$class:logInfo:59] Got assigned task 38
  [ERROR] [2015-07-02 21:34:33] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks 
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:89)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:592)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:590)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:590)
	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:584)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1(TorrentBroadcast.scala:127)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply(TorrentBroadcast.scala:137)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1254)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:62)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:36] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
  [INFO ] [2015-07-02 21:34:36] [Logging$class:logInfo:59] Running task 1.2 in stage 0.0 (TID 38)
  [ERROR] [2015-07-02 21:34:41] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 1 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:41] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (2/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-07-02 21:34:46] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 2 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:46] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (3/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-07-02 21:34:51] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 3 retries)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[ERROR] [2015-07-02 21:34:51] [Logging$class:logError:96] Exception in task 12.3 in stage 0.0 (TID 31)
  java.io.IOException: Failed to connect to /10.0.0.41:36774
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: 拒绝连接: /10.0.0.41:36774
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-07-02 21:34:51] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ] [2015-07-02 21:35:27] [Logging$class:logInfo:59] ensureFreeSpace(2291) called with curMem=0, maxMem=4829950771
  [INFO ] [2015-07-02 21:35:27] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 4.5 GB)
  [INFO ] [2015-07-02 21:35:27] [Logging$class:logInfo:59] Reading broadcast variable 1 took 35741 ms
  [INFO ] [2015-07-02 21:35:37] [Logging$class:logInfo:59] ensureFreeSpace(4024) called with curMem=2291, maxMem=4829950771
  [INFO ] [2015-07-02 21:35:37] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 4.5 GB)
  [INFO ] [2015-07-02 21:35:37] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:67108864+67108864
  [INFO ] [2015-07-02 21:35:37] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-07-02 21:35:38] [Logging$class:logInfo:59] ensureFreeSpace(4294) called with curMem=6315, maxMem=4829950771
  [INFO ] [2015-07-02 21:35:38] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 4.5 GB)
  [INFO ] [2015-07-02 21:35:38] [Logging$class:logInfo:59] Reading broadcast variable 0 took 149 ms
  [INFO ] [2015-07-02 21:35:38] [Logging$class:logInfo:59] ensureFreeSpace(71248) called with curMem=10609, maxMem=4829950771
  [INFO ] [2015-07-02 21:35:38] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 69.6 KB, free 4.5 GB)
  [WARN ] [2015-07-02 21:35:43] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-07-02 21:35:43] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [ERROR] [2015-07-02 22:06:02] [Logging$class:logError:75] Lost executor 233 on 10.0.0.42: remote Rpc client disassociated
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Re-queueing tasks for 233 from TaskSet 0.0
  [WARN ] [2015-07-02 22:06:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkExecutor@10.0.0.42:36686] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 13), so marking it as still running
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 16), so marking it as still running
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 2), so marking it as still running
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Starting task 14.2 in stage 0.0 (TID 39, 10.0.0.39, ANY, 1484 bytes)
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Resubmitted ShuffleMapTask(0, 14), so marking it as still running
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Executor lost: 233 (epoch 33)
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Trying to remove executor 233 from BlockManagerMaster.
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Removing block manager BlockManagerId(233, 10.0.0.42, 35858)
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Removed 233 successfully in removeExecutor
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] ShuffleMapStage 0 is now unavailable on executor 233 (0/20, false)
  [INFO ] [2015-07-02 22:06:02] [Logging$class:logInfo:59] Starting task 2.2 in stage 0.0 (TID 40, 10.0.0.41, ANY, 1484 bytes)
  [INFO ] [2015-07-02 22:06:03] [Logging$class:logInfo:59] Starting task 16.2 in stage 0.0 (TID 41, 10.0.0.39, ANY, 1484 bytes)
  [INFO ] [2015-07-02 22:06:03] [Logging$class:logInfo:59] Starting task 13.2 in stage 0.0 (TID 42, 10.0.0.41, ANY, 1484 bytes)
  [INFO ] [2015-07-02 22:06:03] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/233 is now EXITED (Command exited with code 137)
  [INFO ] [2015-07-02 22:06:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/233 removed: Command exited with code 137
  [ERROR] [2015-07-02 22:06:03] [Logging$class:logError:75] Asked to remove non-existent executor 233
  [INFO ] [2015-07-02 22:06:03] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/233 because it is EXITED
  [INFO ] [2015-07-02 22:06:03] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/236 on worker worker-20150702200417-10.0.0.42-36843
  [INFO ] [2015-07-02 22:06:07] [Logging$class:logInfo:59] Executor app-20150702201026-0001/233 finished with state EXITED message Command exited with code 137 exitStatus 137
  [WARN ] [2015-07-02 22:06:07] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkExecutor@10.0.0.42:36686] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 22:06:08] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/236 for PageRank
  [INFO ] [2015-07-02 2[INFO ] [2015-07-02 22:06:09] [Logging$class:logInfo:59] Got assigned task 40
  [INFO ] [2015-07-02 22:06:09] [Logging$class:logInfo:59] Got assigned task 42
  [INFO ] [2015-07-02 22:06:09] [Logging$class:logInfo:59] Running task 13.2 in stage 0.0 (TID 42)
  [INFO ] [2015-07-02 22:06:09] [Logging$class:logInfo:59] Running task 2.2 in stage 0.0 (TID 40)
  [INFO ] [2015-07-02 22:06:09] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:872415232+67108864
  [INFO ] [2015-07-02 22:06:09] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:134217728+67108864
  oarseGrainedScheduler" "--executor-id" "236" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  [INFO ] [2015-07-02 22:06:11] [Logging$class:logInfo:59] Got assigned task 39
  [INFO ] [2015-07-02 22:06:11] [Logging$class:logInfo:59] Got assigned task 41
  [INFO ] [2015-07-02 22:06:11] [Logging$class:logInfo:59] Running task 14.2 in stage 0.0 (TID 39)
  [INFO ] [2015-07-02 22:06:11] [Logging$class:logInfo:59] Running task 16.2 in stage 0.0 (TID 41)
  [INFO ] [2015-07-02 22:06:11] [Logging$class:logInfo:59] Fetching http://10.0.0.38:37040/jars/spark-examples-1.4.0-hadoop1.2.1.jar with timestamp 1435839025941
  ome/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "237" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  12] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/237 is now RUNNING
  [INFO ] [2015-07-02 22:06:18] [Logging$class:logInfo:59] Executor app-20150702201026-0001/237 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:18] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/238 for PageRank
  [INFO ] [2015-07-02 22:06:19] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "238" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  15] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/238 is now LOADING
  [INFO ] [2015-07-02 22:06:21] [Logging$class:logInfo:59] Executor app-20150702201026-0001/238 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:22] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/239 for PageRank
  -02 22:06:18] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/238 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 22:06:18] [Logging$class:logInfo:59] Executor app-20150702201026-0001/238 removed: Command exited with code 1
  [ERROR] [2015-07-02 22:06:18] [Logging$class:logError:75] Asked to remove non-existent executor 238
  [INFO ] [2015-07-02 22:06:18] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/239 on worker-20150702200417-10.0.0.42-36843 (10.0.0.42:36843) with 4 cores
  [INFO ] [2015-07-02 22:06:18] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/239 on hostPort 10.0.0.42:36843 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 22:06:18] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/239 is now LOADING
  [INFO ] [2015-07-02 22:06:18] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/239 is now RUNNING
  [INFO ] [2015-07-02 22:06:22] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "239" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  [INFO ] [2015-07-02 22:06:24] [Logging$class:logInfo:59] Copying /tmp/spark-19286363-5ffb-4037-aed7-3b6a627f7d32/executor-d555957d-2ef7-4437-a53b-0a0c624669be/-4441382601435839025941_cache to /home/spark/spark-1.3.1/work/app-20150702201026-0001/235/./spark-examples-1.4.0-hadoop1.2.1.jar
  ank
  [INFO ] [2015-07-02 22:06:25] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "240" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  21] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/240 is now RUNNING
  [INFO ] [2015-07-02 22:06:28] [Logging$class:logInfo:59] Executor app-20150702201026-0001/240 finished with state EXITED message Command exited with code 1 exitStatus 1
  logInfo:59] Launching executor app-20150702201026-0001/241 on worker worker-20150702200417-10.0.0.42-36843
  [INFO ] [2015-07-02 22:06:29] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/241 for PageRank
   exited with code 1)
  [INFO ] [2015-07-02 22:06:25] [Logging$class:logInfo:59] Executor app-20150702201026-0001/240 removed: Command exited with code 1
  [ERROR] [2015-07-02 22:06:25] [Logging$class:logError:75] Asked to remove non-existent executor 240
  [INFO ] [2015-07-02 22:06:30] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "241" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  [INFO ] [2015-07-02 22:06:32] [Logging$class:logInfo:59] Executor app-20150702201026-0001/241 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:32] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/242 for PageRank
  [INFO ] [2015-07-02 22:06:33] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "242" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  29] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/242 is now RUNNING
  [INFO ] [2015-07-02 22:06:35] [Logging$class:logInfo:59] Executor app-20150702201026-0001/242 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:36] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/243 for PageRank
  [INFO ] [2015-07-02 22:06:36] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "243" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  32] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/243 is now LOADING
  [INFO ] [2015-07-02 22:06:38] [Logging$class:logInfo:59] Executor app-20150702201026-0001/243 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:38] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/244 for PageRank
  [INFO ] [2015-07-02 22:06:38] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "244" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  [INFO ] [2015-07-02 22:06:34] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/244 is now RUNNING
  [INFO ] [2015-07-02 22:06:39] [Logging$class:logInfo:59] Executor app-20150702201026-0001/244 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:40] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/245 for PageRank
  [INFO ] [2015-07-02 22:06:40] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "245" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  36] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/245 is now RUNNING
  [INFO ] [2015-07-02 22:06:44] [Logging$class:logInfo:59] Executor app-20150702201026-0001/245 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:45] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/246 for PageRank
  [INFO ] [2015-07-02 22:06:46] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "246" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  41] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/246 is now LOADING
  [INFO ] [2015-07-02 22:06:48] [Logging$class:logInfo:59] Executor app-20150702201026-0001/246 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:48] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/247 for PageRank
  [INFO ] [2015-07-02 22:06:48] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "247" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  44] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/247 is now RUNNING
  [INFO ] [2015-07-02 22:06:49] [Logging$class:logInfo:59] Executor app-20150702201026-0001/247 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:50] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/248 for PageRank
  [INFO ] [2015-07-02 22:06:50] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "248" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  [INFO ] [2015-07-02 22:06:46] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/248 is now RUNNING
  [INFO ] [2015-07-02 22:06:51] [Logging$class:logInfo:59] Executor app-20150702201026-0001/248 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:51] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/249 for PageRank
  [INFO ] [2015-07-02 22:06:51] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "249" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  47] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/249 is now RUNNING
  [INFO ] [2015-07-02 22:06:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/249 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:06:59] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/250 for PageRank
  [INFO ] [2015-07-02 22:07:00] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "250" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  56] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/250 is now LOADING
  [INFO ] [2015-07-02 22:06:59] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/250 is now EXITED (Command exited with code 1)
  [INFO ] [2015-07-02 22:06:59] [Logging$class:logInfo:59] Executor app-20150702201026-0001/250 removed: Command exited with code 1
  [INFO ] [2015-07-02 22:06:59] [Logging$class:logInfo:59] Removing executor app-20150702201026-0001/250 because it is EXITED
  [ERROR] [2015-07-02 22:06:59] [Logging$class:logError:75] Asked to remove non-existent executor 250
  [INFO ] [2015-07-02 22:06:59] [Logging$class:logInfo:59] Launching executor app-20150702201026-0001/251 on worker worker-20150702200417-10.0.0.42-36843
  [INFO ] [2015-07-02 22:06:59] [Logging$class:logInfo:59] Executor added: app-20150702201026-0001/251 on worker-20150702200417-10.0.0.42-36843 (10.0.0.42:36843) with 4 cores
  [INFO ] [2015-07-02 22:06:59] [Logging$class:logInfo:59] Granted executor ID app-20150702201026-0001/251 on hostPort 10.0.0.42:36843 with 4 cores, 8.7 GB RAM
  [INFO ] [2015-07-02 22:06:59] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/251 is now RUNNING
  [INFO ] [2015-07-02 22:07:03] [Logging$class:logInfo:59] Executor app-20150702201026-0001/250 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:07:04] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/251 for PageRank
  [INFO ] [2015-07-02 22:07:05] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "251" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  [INFO ] [2015-07-02 22:07:07] [Logging$class:logInfo:59] Executor app-20150702201026-0001/251 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:07:07] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/252 for PageRank
  [INFO ] [2015-07-02 22:07:07] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "252" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  03] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/252 is now RUNNING
  [INFO ] [2015-07-02 22:07:08] [Logging$class:logInfo:59] Executor app-20150702201026-0001/252 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:07:08] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/253 for PageRank
  [INFO ] [2015-07-02 22:07:08] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "253" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  04] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/253 is now RUNNING
  [INFO ] [2015-07-02 22:07:09] [Logging$class:logInfo:59] Executor app-20150702201026-0001/253 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:07:09] [Logging$class:logInfo:59] Asked to launch executor app-20150702201026-0001/254 for PageRank
  [INFO ] [2015-07-02 22:07:10] [Logging$class:logInfo:59] Launch command: "/usr/bin/java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf/:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.4.0-hadoop1.2.1.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/hadoop-1.2.1/conf/" "-Xms8900M" "-Xmx8900M" "-Dspark.driver.port=44543" "-XX:MaxPermSize=128m" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler" "--executor-id" "254" "--hostname" "10.0.0.42" "--cores" "4" "--app-id" "app-20150702201026-0001" "--worker-url" "akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker"
  05] [Logging$class:logInfo:59] Executor updated: app-20150702201026-0001/254 is now LOADING
  [INFO ] [2015-07-02 22:07:19] [Logging$class:logInfo:59] Thread 81 spilling in-memory map of 374.4 MB to disk (1 time so far)
  [INFO ] [2015-07-02 22:07:22] [Logging$class:logInfo:59] Thread 80 spilling in-memory map of 320.6 MB to disk (1 time so far)
  nging modify acls to: spark
  [INFO ] [2015-07-02 22:07:36] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 22:07:38] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 22:07:38] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 22:07:39] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10.0.0.42:52079]
  [INFO ] [2015-07-02 22:07:39] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 52079.
  [INFO ] [2015-07-02 22:07:41] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 22:07:41] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-07-02 22:07:41] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-07-02 22:07:41] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-07-02 22:07:41] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-07-02 22:07:41] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-07-02 22:07:41] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-07-02 22:07:41] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10.0.0.42:54789]
  [INFO ] [2015-07-02 22:07:41] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 54789.
  [INFO ] [2015-07-02 22:07:41] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-07-02 22:07:43] [Logging$class:logInfo:59] Created local directory at /tmp/spark-355db919-c195-4621-a3f6-e4ec22ae1b37/executor-d0afeb26-6e8a-4200-ac74-40a10210deaf/blockmgr-07a0f599-e200-4b1f-b2cc-ed9ccfb18b04
  [INFO ] [2015-07-02 22:07:43] [Logging$class:logInfo:59] MemoryStore started with capacity 4.5 GB
  [INFO ] [2015-07-02 22:07:44] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@10.0.0.38:44543/user/CoarseGrainedScheduler
  [INFO ] [2015-07-02 22:07:44] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker
  [INFO ] [2015-07-02 22:07:44] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@10.0.0.42:36843/user/Worker
  [INFO ] [2015-07-02 22:07:44] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-07-02 22:07:44] [Logging$class:logInfo:59] Starting executor ID 254 on host 10.0.0.42
  [INFO ] [2015-07-02 22:07:45] [Logging$class:logInfo:59] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52576.
  [INFO ] [2015-07-02 22:07:45] [Logging$class:logInfo:59] Server created on 52576
  [INFO ] [2015-07-02 22:07:45] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-07-02 22:07:45] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-07-02 22:08:05] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150702201026-0001/235/./spark-examples-1.4.0-hadoop1.2.1.jar to class loader
  [INFO ] [2015-07-02 22:08:09] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ] [2015-07-02 22:08:10] [Logging$class:logInfo:59] ensureFreeSpace(2291) called with curMem=0, maxMem=4829950771
  [INFO ] [2015-07-02 22:08:10] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 4.5 GB)
  [INFO ] [2015-07-02 22:08:11] [Logging$class:logInfo:59] Reading broadcast variable 1 took 1430 ms
  [INFO ] [2015-07-02 22:08:18] [Logging$class:logInfo:59] ensureFreeSpace(4024) called with curMem=2291, maxMem=4829950771
  [INFO ] [2015-07-02 22:08:18] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 4.5 GB)
  [INFO ] [2015-07-02 22:08:19] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:1073741824+67108864
  [INFO ] [2015-07-02 22:08:19] [Logging$class:logInfo:59] Input split: hdfs://spark1:9000/user/spark/twitter-edges.csv:939524096+67108864
  [INFO ] [2015-07-02 22:08:19] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-07-02 22:08:23] [Logging$class:logInfo:59] ensureFreeSpace(4294) called with curMem=6315, maxMem=4829950771
  [INFO ] [2015-07-02 22:08:23] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 4.5 GB)
  [INFO ] [2015-07-02 22:08:23] [Logging$class:logInfo:59] Reading broadcast variable 0 took 4435 ms
  [INFO ] [2015-07-02 22:08:23] [Logging$class:logInfo:59] ensureFreeSpace(71248) called with curMem=10609, maxMem=4829950771
  [INFO ] [2015-07-02 22:08:23] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 69.6 KB, free 4.5 GB)
  [WARN ] [2015-07-02 22:08:25] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-07-02 22:08:25] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  eartbeat timed out after 154206 ms
  [INFO ] [2015-07-02 22:09:01] [Logging$class:logInfo:59] Thread 80 spilling in-memory map of 250.9 MB to disk (2 times so far)
  [INFO ] [2015-07-02 22:09:11] [Logging$class:logInfo:59] Finished task 1.2 in stage 0.0 (TID 38). 1946 bytes result sent to driver
  [INFO ] [2015-07-02 22:09:57] [Logging$class:logInfo:59] Finished task 13.2 in stage 0.0 (TID 42). 1946 bytes result sent to driver
  [INFO ] [2015-07-02 22:10:04] [Logging$class:logInfo:59] Finished task 2.2 in stage 0.0 (TID 40). 1946 bytes result sent to driver
  [INFO ] [2015-07-02 22:10:47] [Logging$class:logInfo:59] Thread 51 spilling in-memory map of 687.4 MB to disk (1 time so far)
  [INFO ] [2015-07-02 22:11:40] [Logging$class:logInfo:59] Finished task 16.2 in stage 0.0 (TID 41). 1946 bytes result sent to driver
  [INFO ] [2015-07-02 22:12:45] [Logging$class:logInfo:59] Finished task 14.2 in stage 0.0 (TID 39). 1946 bytes result sent to driver
  [INFO ] [2015-07-02 22:40:18] [Logging$class:logInfo:59] Invoking stop() from shutdown hook
  [WARN ] [2015-07-02 22:40:19] [QueuedThreadPool:doStop:145] 2 threads could not be stopped
  [INFO ] [2015-07-02 22:40:19] [Logging$class:logInfo:59] Stopped Spark web UI at http://10.0.0.38:4040
  [INFO ] [2015-07-02 22:40:19] [Logging$class:logInfo:59] Stopping DAGScheduler
  [INFO ] [2015-07-02 22:40:19] [Logging$class:logInfo:59] ShuffleMapStage 0 (distinct at SparkPageRank.scala:60) failed in 8985.277 s
  [INFO ] [2015-07-02 22:40:20] [Logging$class:logInfo:59] Shutting down all executors
  [INFO ] [2015-07-02 22:40:20] [Logging$class:logInfo:59] Asking each executor to shut down
  [INFO ] [2015-07-02 22:40:20] [Logging$class:logInfo:59] Job 0 failed: collect at SparkPageRank.scala:71, took 8986.201635 s
  [INFO ] [2015-07-02 22:40:24] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-07-02 22:40:24] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-07-02 22:40:24] [Logging$class:logInfo:59] B[INFO ] [2015-07-02 22:40:24] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-07-02 22:40:26] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-07-02 22:40:26] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-07-02 22:40:26] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 22:40:26] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing r[INFO ] [2015-07-02 22:40:26] [Logging$class:logInfo:59] Executor app-20150702201026-0001/254 finished with state EXITED message Command exited with code 0 exitStatus 0
  rk-f3de98ac-ae9b-4391-b5bc-8ae93dbd0c70/blockmgr-d5eec9e4-c7c2-44fd-aef9-c8760dfde31e, already present as root for deletion.
  [INFO ] [2015-07-02 22:40:22] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-07-02 22:40:22] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-07-02 22:40:22] [Logging$class:logInfo:59] BlockManagerMaster stopped
  [INFO ] [2015-07-02 22:40:22] [Logging$class:logInfo:59] Successfully stopped SparkContext
  [INFO ] [2015-07-02 22:40:22] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 22:40:22] [Logging$class:logInfo:59] OutputCommitCoordinator stopped!
  [INFO ] [2015-07-02 22:40:22] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-07-02 22:40:22] [Logging$class:logInfo:59] Deleting directory /tmp/spark-f3de98ac-ae9b-4391-b5bc-8ae93dbd0c70
  [INFO ] [2015-07-02 22:40:22] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [WARN ] [2015-07-02 22:40:23] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkDriver@10.0.0.38:44543] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [ERROR] [2015-07-02 22:40:28] [Logging$class:logError:75] Driver 10.0.0.38:44543 disassociated! Shutting down.
  [WARN ] [2015-07-02 22:40:28] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkDriver@10.0.0.38:44543] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 22:40:29] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 22:40:29] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 22:40:27] [Logging$class:logInfo:59] Executor app-20150702201026-0001/231 finished with state EXITED message Command exited with code 0 exitStatus 0
  35 finished with state EXITED message Command exited with code 0 exitStatus 0
  [WARN ] [2015-07-02 22:40:39] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkExecutor@10.0.0.39:37097] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 22:40:40] [Logging$class:logInfo:59] Executor app-20150702201026-0001/1 finished with state EXITED message Command exited with code 1 exitStatus 1
  [INFO ] [2015-07-02 22:40:47] [Logging$class:logInfo:59] Asked to kill unknown executor app-20150702201026-0001/235
  [INFO ] [2015-07-02 22:40:47] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150702201026-0001
  oving it.
  [INFO ] [2015-07-02 22:40:43] [Logging$class:logInfo:59] akka.tcp://sparkDriver@10.0.0.38:44543 got disassociated, removing it.
  [WARN ] [2015-07-02 22:40:43] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/231
  [WARN ] [2015-07-02 22:40:43] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/235
  [WARN ] [2015-07-02 22:40:43] [Logging$class:logWarning:71] Got status update for unknown executor app-20150702201026-0001/1
  [INFO ] [2015-07-02 22:40:48] [Logging$class:logInfo:59] Asked to kill unknown executor app-20150702201026-0001/231
  [INFO ] [2015-07-02 22:40:49] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150702201026-0001
  [ERROR] [2015-07-02 22:42:53] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Deleting directory /tmp/spark-19286363-5ffb-4037-aed7-3b6a627f7d32
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/1 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/229 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/234 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/232 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/230 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/235 finished with state EXITED message Worker shutting down exitStatus 0
  [INFO ] [2015-07-02 22:42:54] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/227 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:42:51] [Logging$class:logInfo:59] akka.tcp://sparkWorker@10.0.0.39:55458 got disassociated, removing it.
  [WARN ] [2015-07-02 22:42:51] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkWorker@10.0.0.39:55458] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 22:42:51] [Logging$class:logInfo:59] Removing worker worker-20150702200419-10.0.0.39-55458 on 10.0.0.39:55458
  [INFO ] [2015-07-02 22:42:51] [Logging$class:logInfo:59] akka.tcp://sparkWorker@10.0.0.39:55458 got disassociated, removing it.
  [ERROR] [2015-07-02 22:44:31] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702200538-0000/0 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/254 finished with state EXITED message Worker shutting down exitStatus 0
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/253 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/252 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/251 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/250 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/249 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/248 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/247 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/3 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/245 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/244 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/243 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/242 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/241 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/240 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/239 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/238 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/237 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/236 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Shutdown hook called
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/233 finished with state EXITED message Worker shutting down exitStatus 137
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/246 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Unknown Executor app-20150702201026-0001/0 finished with state EXITED message Worker shutting down exitStatus 1
  [INFO ] [2015-07-02 22:44:32] [Logging$class:logInfo:59] Deleting directory /tmp/spark-355db919-c195-4621-a3f6-e4ec22ae1b37
  [INFO ] [2015-07-02 22:44:29] [Logging$class:logInfo:59] akka.tcp://sparkWorker@10.0.0.42:36843 got disassociated, removing it.
  [INFO ] [2015-07-02 22:44:29] [Logging$class:logInfo:59] Removing worker worker-20150702200417-10.0.0.42-36843 on 10.0.0.42:36843
  [WARN ] [2015-07-02 22:44:29] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkWorker@10.0.0.42:36843] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-07-02 22:44:29] [Logging$class:logInfo:59] akka.tcp://sparkWorker@10.0.0.42:36843 got disassociated, removing it.
  [ERROR] [2015-07-02 22:44:52] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-07-02 22:44:53] [Logging$class:logInfo:59] Shutdown hook called
  