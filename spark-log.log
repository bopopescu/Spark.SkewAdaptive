[ERROR] [2015-06-10 10:41:02] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-06-10 10:41:02] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-06-10 10:41:02] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-06-10 10:41:02] [Logging$class:logInfo:59] Unknown Executor app-20150609174727-0000/3 finished with state EXITED message Worker shutting down exitStatus 0
  [INFO ] [2015-06-10 10:41:02] [Logging$class:logInfo:59] Unknown Executor app-20150609182232-0001/3 finished with state EXITED message Worker shutting down exitStatus 0
  [INFO ] [2015-06-10 10:41:08] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 10:41:08] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 10:41:09] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 10:41:09] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 10:41:09] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 10:41:10] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 10:41:10] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 10:41:10] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkWorker@spark3:49290]
  [INFO ] [2015-06-10 10:41:10] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting now listens on addresses: [akka.tcp://sparkWorker@spark3:49290]
  [INFO ] [[INFO ] [2015-06-10 10:41:10] [Logging$class:logInfo:59] Starting Spark worker spark5:59751 with 4 cores, 97[INFO ] [2015-06-10 10:41:10] [Logging$class:logInfo:59] Starting Spark worker spark3:49290 with 4 cores, 2.9 GB RAM
  [INFO ] [2015-06-10 10:41:10] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 10:41:10] [Logging$class:logInfo:59] Spark home: /home/spark/spark-1.3.1
  [INFO ] [2015-06-10 10:41:11] [Logging$class:logInfo:59] Successfully started service 'WorkerUI' on port 8081.
  [INFO ] [2015-06-10 10:41:11] [Logging$class:logInfo:59] Started WorkerWebUI at http://spark3:8081
  [INFO ] [2015-06-10 10:41:11] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [WARN ] [2015-06-10 10:41:11] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] AssociationError [akka.tcp://sparkWorker@spark3:49290] -> [akka.tcp://sparkMaster@spark1:7077]: Error [Invalid address: akka.tcp://sparkMaster@spark1:7077] [
akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@spark1:7077
Caused by: akka.remote.transport.Transport$InvalidAssociationException: 拒绝连接: spark1/10.0.0.38:7077
]
  [WARN ] [2015-06-10 10:41:11] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Tried to associate with unreachable remote address [akka.tcp://sparkMaster@spark1:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: 拒绝连接: spark1/10.0.0.38:7077
  [INFO ] [2015-06-10 10:41:11] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [org.apache.spark.deploy.DeployMessages$RegisterWorker] from Actor[akka://sparkWorker/user/Worker#51918508] to Actor[akka://sparkWorker/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 10:41:11] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.EndpointWriter$AckIdleCheckTimer$] from Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkMaster%40spark1%3A7077-0/endpointWriter#1045798050] to Actor[akka://sparsparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkMaster%40spark1%3A7077-0/endpointWriter#-674913110] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  0:41:13] [Logging$class:logInfo:59] I have been elected leader! New state: ALIVE
  [INFO ] [2015-06-10 10:41:16] [Logging$class:logInfo:59] Retrying connection to master (attempt # 1)
  [INFO ] [2015-06-10 10:41:16] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 10:41:16] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [org.apache.spark.deploy.DeployMessages$RegisterWorker] from Actor[akka://sparkWorker/user/Worker#51918508] to Actor[akka://sparkWorker/deadLetters] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 10:41:21] [Logging$class:logInfo:59] Retrying connection to master (attempt # 2)
  [INFO ] [2015-06-10 10:41:21] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 10:41:21] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  [INFO ] [2015-06-10 10:41:53] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 10:41:53] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 10:41:53] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 10:41:53] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 10:41:54] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 10:41:54] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 10:41:55] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkDriver@spark1:57579]
  [INFO ] [2015-06-10 10:41:55] [Logging$class:logInfo:59] Successfully started service 'sparkDriver' on port 57579.
  [INFO ] [2015-06-10 10:41:55] [Logging$class:logInfo:59] Registering MapOutputTracker
  [INFO ] [2015-06-10 10:41:55] [Logging$class:logInfo:59] Registering BlockManagerMaster
  [INFO ] [2015-06-10 10:41:55] [Logging$class:logInfo:59] Created local directory at /tmp/spark-c7c9b665-80d6-4ffd-93d2-1817feb12e1b/blockmgr-54a7780a-ed40-4d52-8a10-421d878eb88e
  [INFO ] [2015-06-10 10:41:55] [Logging$class:logInfo:59] MemoryStore started with capacity 530.3 MB
  [INFO ] [2015-06-10 10:41:55] [Logging$class:logInfo:59] HTTP File server directory is /tmp/spark-9d62f3b7-ee1a-451e-81c1-2b58df5fafd0/httpd-1855bff8-18eb-4e83-926c-8de9bd4f82b1
  [INFO ] [2015-06-10 10:41:55] [Logging$class:logInfo:59] Starting HTTP Server
  [INFO ] [2015-06-10 10:41:55] [Logging$class:logInfo:59] Successfully started service 'HTTP file server' on port 57731.
  [INFO ] [2015-06-10 10:41:55] [Logging$class:logInfo:59] Registering OutputCommitCoordinator
  [INFO ] [2015-06-10 10:41:56] [Logging$class:logInfo:59] Successfully started service 'SparkUI' on port 4040.
  [INFO ] [2015-06-10 10:41:56] [Logging$class:logInfo:59] Started SparkUI at http://spark1:4040
  [INFO ] [2015-06-10 10:41:56] [Logging$class:logInfo:59] Added JAR file:/home/spark/spark-1.3.1/examples/target/scala-2.10/spark-examples_2.10-1.3.1.jar at http://10.0.0.38:57731/jars/spark-examples_2.10-1.3.1.jar with timestamp 1433904116128
  [INFO ] [2015-06-10 10:41:56] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Registering app PageRank
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Registered app PageRank with ID app-20150610104157-0000
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Connected to Spark cluster with app ID app-20150610104157-0000
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Launching executor app-20150610104157-0000/0 on worker worker-20150610104110-spark4-47131
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Launching executor app-20150610104157-0000/1 on worker worker-20150610104110-spark5-59751
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Launching executor app-20150610104157-0000/2 on worker worker-20150610104110-spark3-49290
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Launching executor app-20150610104157-0000/3 on worker worker-20150610104110-spark2-46342
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor added: app-20150610104157-0000/0 on worker-20150610104110-spark4-47131 (spark4:47131) with 4 cores
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Granted executor ID app-20150610104157-0000/0 on hostPort spark4:47131 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor added: app-20150610104157-0000/1 on worker-20150610104110-spark5-59751 (spark5:59751) with 4 cores
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Granted executor ID app-20150610104157-0000/1 on hostPort spark5:59751 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor added: app-20150610104157-0000/2 on worker-20150610104110-spark3-49290 (spark3:49290) with 4 cores
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Granted executor ID app-20150610104157-0000/2 on hostPort spark3:49290 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor added: app-20150610104157-0000/3 on worker-20150610104110-spark2-46342 (spark2:46342) with 4 cores
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Granted executor ID app-20150610104157-0000/3 on hostPort spark2:46342 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Asked to launch executor app-20150610104157-0000/3 for PageRank
   ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor updated: app-20150610104157-0000/1 is now RUNNING
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor updated: app-20150610104157-0000/2 is now RUNNING
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor updated: app-20150610104157-0000/3 is now RUNNING
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor updated: app-20150610104157-0000/1 is now LOADING
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor updated: app-20150610104157-0000/2 is now LOADING
  [INFO ] [2015-06-10 10:41:57] [Logging$class:logInfo:59] Executor updated: app-20150610104157-0000/0 is now LOADING
  [INFO ] [2015-06-10 10:41:58] [Logging$class:logInfo:59] Launch command: "java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar" "-XX:MaxPermSize=128m" "-Dspark.driver.port=57579" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@spark1:57579/user/CoarseGrainedScheduler" "--executor-id" "1" "--hostname" "spark5" "--cores" "4" "--app-id" "app-20150610104157-0000" "--worker-url" "akka.tcp://sparkWorker@spark5:59751/user/Worker"
  "--cores" "4" "--app-id" "app-20150610104157-0000" "--worker-url" "akka.tcp://sparkWorker@spark2:46342/user/Worker"
  [WARN ] [2015-06-10 10:41:59] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO ] [2015-06-10 10:41:59] [Logging$class:logInfo:59] Logging events to file:/tmp/spark-events/app-20150610104157-0000
  [INFO ] [2015-06-10 10:41:59] [Logging$class:logInfo:59] SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
  [INFO ] [2015-06-10 10:41:59] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 10:42:00] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 10:42:00] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 10:42:00] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 10:42:01] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 10:42:01] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 10:42:01] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@spark3:34144]
  [INFO ] [2015-06-10 10:42:01] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 34144.
  [INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@spark3:56250]
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 56250.
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Connecting to MapOutputTracker: akka.tcp://sparkDriver@spark1:57579/user/MapOutputTracker
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Connecting to BlockManagerMaster: akka.tcp://sparkDriver@spark1:57579/user/BlockManagerMaster
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Created local directory at /tmp/spark-43798f24-76cd-418b-b2ac-0feb3fc0030f/spark-40fefb56-24f3-4f3f-894e-500318280c96/spark-b93b026f-c279-451a-8300-5e3d41335a64/blockmgr-fd39c0ed-c61b-4eef-8df7-449225c3ddfa
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] MemoryStore started with capacity 265.4 MB
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Connecting to OutputCommitCoordinator: akka.tcp://sparkDriver@spark1:57579/user/OutputCommitCoordinator
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@spark1:57579/user/CoarseGrainedScheduler
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@spark4:47131/user/Worker
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@spark5:59751/user/Worker
  [INFO ] [
	at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:82)
	at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)
	at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$Batch.run(BatchingExecutor.scala:58)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
[INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 10:42:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@spark2:37489]
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 37489.
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Connecting to MapOutputTracker: akka.tcp://sparkDriver@spark1:57579/user/MapOutputTracker
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Connecting to BlockManagerMaster: akka.tcp://sparkDriver@spark1:57579/user/BlockManagerMaster
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Created local directory at /tmp/spark-194ffb00-ac72-47ed-ba0a-e033ae5c3418/spark-68993248-fb81-4707-9460-8400240e5fea/spark-327695d8-a800-40e6-bb3b-8c9de4818e19/blockmgr-c3ae2f99-0a2e-4f0b-bc24-9123334b6d4b
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] MemoryStore started with capacity 265.4 MB
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Connecting to OutputCommitCoordinator: akka.tcp://sparkDriver@spark1:57579/user/OutputCommitCoordinator
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@spark1:57579/user/CoarseGrainedScheduler
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@spark2:46342/user/Worker
  [INFO ] [2015-06-10 10:42:02] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@spark2:46342/user/Worker
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Starting executor ID 3 on host spark2
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Server created on 48768
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark1:57579/user/HeartbeatReceiver
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] @@@@ExecutorID: 1 to launch task,time: 10日10时42分03秒243毫秒
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433904123244
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Got assigned task 0
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:[INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Server created on 56612
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark1:57579/user/HeartbeatReceiver
  :03] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark1:57579/user/HeartbeatReceiver
  31433904116128_cache to /home/spark/spark-1.3.1/work/app-20150610104157-0000/1/./spark-examples_2.10-1.3.1.jar
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150610104157-0000/1/./spark-examples_2.10-1.3.1.jar to class loader
  [INFO ] [2015-06-10 10:42:03] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] ensureFreeSpace(2172) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Updated info of block broadcast_1_piece0
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Reading broadcast variable 1 took 354 ms
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] ensureFreeSpace(3656) called with curMem=2172, maxMem=278302556
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.6 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Input split: hdfs://spark1/user/spark/data/mllib/pagerank_data.txt:0+24
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] ensureFreeSpace(3985) called with curMem=5828, maxMem=278302556
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Updated info of block broadcast_0_piece0
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Reading broadcast variable 0 took 29 ms
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] ensureFreeSpace(39120) called with curMem=9813, maxMem=278302556
  [INFO ] [2015-06-10 10:42:04] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 38.2 KB, free 265.4 MB)
  [WARN ] [2015-06-10 10:42:04] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-06-10 10:42:04] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Finished task 0.0 in stage 0.0 (TID 0). 1920 bytes result sent to driver
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] @@@@ExecutorID: 1 to launch task,time: 10日10时42分05秒547毫秒
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433904125547
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Got assigned task 1
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Running task 0.0 in stage 1.0 (TID 1)
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Updating epoch to 1 and clearing cache
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Started reading broadcast variable 2
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] ensureFreeSpace(1669) called with curMem=48933, maxMem=278302556
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1669.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Updated info of block broadcast_2_piece0
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Reading broadcast variable 2 took 25 ms
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] ensureFreeSpace(2976) called with curMem=50602, maxMem=278302556
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Don't have map outputs for shuffle 2, fetching them
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:57579/user/MapOutputTracker#887606809]
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Started 0 remote fetches in 8 ms
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Finished task 0.0 in stage 1.0 (TID 1). 1014 bytes result sent to driver
  o:59] Added broadcast_2_piece0 in memory on spark5:34032 (size: 1669.0 B, free: 265.4 MB)
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] @@@@ExecutorID: 3 to launch task,time: 10日10时42分06秒045毫秒
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433904126047
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Got assigned task 2
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Running task 0.0 in stage 2.0 (TID 2)
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Fetching http://10.0.0.38:57731/jars/spark-examples_2.10-1.3.1.jar with timestamp 1433904116128
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Fetching http://10.0.0.38:57731/jars/spark-examples_2.10-1.3.1.jar to /tmp/spark-194ffb00-ac72-47ed-ba0a-e033ae5c3418/spark-68993248-fb81-4707-9460-8400240e5fea/spark-f205b039-984d-4ae2-bd95-eb958f5ce4af/fetchFileTemp4258921544291156776.tmp
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Copying /tmp/spark-194ffb00-ac72-47ed-ba0a-e033ae5c3418/spark-68993248-fb81-4707-9460-8400240e5fea/spark-f205b039-984d-4ae2-bd95-eb958f5ce4af/-20179281331433904116128_cache to /home/spark/spark-1.3.1/work/app-20150610104157-0000/3/./spark-examples_2.10-1.3.1.jar
   ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] @@@@ShuffleWriteRecords: 6
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Removed TaskSet 1.0, whose tasks have all completed, from pool 
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Missing parents for Stage 3: List(Stage 2)
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Submitting Stage 2 (MapPartitionsRDD[12] at flatMap at SparkPageRank.scala:64), which is now runnable
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] ensureFreeSpace(4424) called with curMem=47226, maxMem=556038881
  [INFO ] [2015-06-10 10:42:05] [Logging$class:logInfo:59] Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 530.2 MB)
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] ensureFreeSpace(2239) called with curMem=51650, maxMem=556038881
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 530.2 MB)
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Added broadcast_3_piece0 in memory on spark1:32863 (size: 2.2 KB, free: 530.3 MB)
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Updated info of block broadcast_3_piece0
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Created broadcast 3 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 2 (MapPartitionsRDD[12] at flatMap at SparkPageRank.scala:64)
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Adding task set 2.0 with 1 tasks
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Starting task 0.0 in stage 2.0 (TID 2, spark2, PROCESS_LOCAL, 4480 bytes)
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150610104157-0000/3/./spark-examples_2.10-1.3.1.jar to class loader
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Updating epoch to 2 and clearing cache
  [INFO ] [2015-06-10 10:42:06] [Logging$class:logInfo:59] Started reading broadcast variable 3
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] ensureFreeSpace(2239) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] Updated info of block broadcast_3_piece0
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] Reading broadcast variable 3 took 256 ms
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] ensureFreeSpace(4424) called with curMem=2239, maxMem=278302556
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] Partition rdd_6_0 not found, computing it
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] Don't have map outputs for shuffle 1, fetching them
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:57579/user/MapOutputTracker#887606809]
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 10:42:07] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [ERROR] [2015-06-10 10:42:08] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks 
  java.io.IOException: Failed to connect to spark5:34032
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:191)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:87)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:149)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:262)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:115)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:76)
	at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.UnresolvedAddressException
	at sun.nio.ch.Net.checkAddress(Net.java:127)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:644)
	at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:193)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:200)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1029)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:463)
	at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:849)
	at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:199)
	at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:165)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-06-10 10:42:08] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
  [INFO ] [2015-06-10 10:42:08] [Logging$class:logInfo:59] Started 1 remote fetches in 857 ms
  [ERROR] [2015-06-10 10:42:13] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 1 retries)
  java.io.IOException: Failed to connect to spark5:34032
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:191)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.UnresolvedAddressException
	at sun.nio.ch.Net.checkAddress(Net.java:127)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:644)
	at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:193)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:200)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1029)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:463)
	at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:849)
	at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:199)
	at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:165)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-06-10 10:42:13] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (2/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-06-10 10:42:18] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 2 retries)
  java.io.IOException: Failed to connect to spark5:34032
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:191)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.UnresolvedAddressException
	at sun.nio.ch.Net.checkAddress(Net.java:127)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:644)
	at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:193)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:200)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1029)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:463)
	at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:849)
	at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:199)
	at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:165)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-06-10 10:42:18] [RetryingBlockFetcher:initiateRetry:163] Retrying fetch (3/3) for 1 outstanding blocks after 5000 ms
  [ERROR] [2015-06-10 10:42:23] [RetryingBlockFetcher:fetchAllOutstanding:142] Exception while beginning fetch of 1 outstanding blocks (after 3 retries)
  java.io.IOException: Failed to connect to spark5:34032
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:191)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.UnresolvedAddressException
	at sun.nio.ch.Net.checkAddress(Net.java:127)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:644)
	at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:193)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:200)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1029)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:463)
	at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:849)
	at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:199)
	at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:165)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[ERROR] [2015-06-10 10:42:23] [Logging$class:logError:96] Failed to get block(s) from spark5:34032
  java.io.IOException: Failed to connect to spark5:34032
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:191)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.UnresolvedAddressException
	at sun.nio.ch.Net.checkAddress(Net.java:127)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:644)
	at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:193)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:200)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1029)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)
	at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:496)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:481)
	at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:463)
	at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:849)
	at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:199)
	at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:165)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 1 more
[INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] @@@@ExecutorID: 3 to launch task,time: 10日10时42分24秒091毫秒
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433904144092
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Got assigned task 3
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Running task 0.0 in stage 0.1 (TID 3)
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Updating epoch to 6 and clearing cache
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Started reading broadcast variable 4
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] ensureFreeSpace(2172) called with curMem=6663, maxMem=278302556
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Updated info of block broadcast_4_piece0
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Reading broadcast variable 4 took 27 ms
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] ensureFreeSpace(3656) called with curMem=8835, maxMem=278302556
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Input split: hdfs://spark1/user/spark/data/mllib/pagerank_data.txt:0+24
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] ensureFreeSpace(3985) called with curMem=12491, maxMem=278302556
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Updated info of block broadcast_0_piece0
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Reading broadcast variable 0 took 26 ms
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] ensureFreeSpace(39120) called with curMem=16476, maxMem=278302556
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 38.2 KB, free 265.4 MB)
  [WARN ] [2015-06-10 10:42:24] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-06-10 10:42:24] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  dcast_4_piece0
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Created broadcast 4 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[3] at distinct at SparkPageRank.scala:60)
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Adding task set 0.1 with 1 tasks
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Starting task 0.0 in stage 0.1 (TID 3, spark2, NODE_LOCAL, 1374 bytes)
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Added broadcast_4_piece0 in memory on spark2:48768 (size: 2.1 KB, free: 265.4 MB)
  [INFO ] [2015-06-10 10:42:24] [Logging$class:logInfo:59] Added broadcast_0_piece0 in memory on spark2:48768 (size: 3.9 KB, free: 265.4 MB)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Finished task 0.0 in stage 0.1 (TID 3). 1920 bytes result sent to driver
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] @@@@ExecutorID: 3 to launch task,time: 10日10时42分25秒335毫秒
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433904145336
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Got assigned task 4
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Running task 0.0 in stage 1.1 (TID 4)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Updating epoch to 7 and clearing cache
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Started reading broadcast variable 5
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] ensureFreeSpace(1669) called with curMem=55596, maxMem=278302556
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Block broadcast_5_piece0 stored as bytes in memory (estimated size 1669.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Updated info of block broadcast_5_piece0
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Reading broadcast variable 5 took 27 ms
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] ensureFreeSpace(2976) called with curMem=57265, maxMem=278302556
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Block broadcast_5 stored as values in memory (estimated size 2.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Don't have map outputs for shuffle 2, fetching them
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:57579/user/MapOutputTracker#887606809]
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Started 0 remote fetches in 2 ms
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Finished task 0.0 in stage 1.1 (TID 4). 1014 bytes result sent to driver
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [ERROR] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$1:apply$mcV$sp:65] AssociationError [akka.tcp://sparkWorker@spark2:46342] <- [akka.tcp://sparkExecutor@spark2:37489]: Error [Shut down address: akka.tcp://sparkExecutor@spark2:37489] [
akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@spark2:37489
Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.
]
  s:logInfo:59] Added broadcast_5_piece0 in memory on spark1:32863 (size: 1669.0 B, free: 530.3 MB)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Updated info of block broadcast_5_piece0
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Created broadcast 5 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 1 (MapPartitionsRDD[5] at distinct at SparkPageRank.scala:60)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Adding task set 1.1 with 1 tasks
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Ignoring failure of Stage 0 because all jobs depending on it are done
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Starting task 0.0 in stage 1.1 (TID 4, spark2, PROCESS_LOCAL, 1112 bytes)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Added broadcast_5_piece0 in memory on spark2:48768 (size: 1669.0 B, free: 265.4 MB)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Asked to send map output locations for shuffle 2 to sparkExecutor@spark2:37489
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Size of output statuses for shuffle 2 is 132 bytes
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Stage 1 (distinct at SparkPageRank.scala:60) finished in 0.204 s
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] looking for newly runnable stages
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] running: Set()
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] waiting: Set(Stage 2, Stage 3)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] failed: Set()
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Missing parents for Stage 2: List()
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Finished task 0.0 in stage 1.1 (TID 4) in 192 ms on spark2 (1/1)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] @@@@ShuffleRemoteRead: 0
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] @@@@ShuffleLocalRead: 64
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] @@@@ShuffleWrite: 66
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] @@@@ShuffleWriteRecords: 6
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Missing parents for Stage 3: List(Stage 2)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Removed TaskSet 1.1, whose tasks have all completed, from pool 
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Submitting Stage 2 (MapPartitionsRDD[12] at flatMap at SparkPageRank.scala:64), which is now runnable
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] ensureFreeSpace(4424) called with curMem=64362, maxMem=556038881
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Block broadcast_6 stored as values in memory (estimated size 4.3 KB, free 530.2 MB)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] ensureFreeSpace(2239) called with curMem=68786, maxMem=556038881
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 530.2 MB)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Added broadcast_6_piece0 in memory on spark1:32863 (size: 2.2 KB, free: 530.3 MB)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Updated info of block broadcast_6_piece0
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Created broadcast 6 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 2 (MapPartitionsRDD[12] at flatMap at SparkPageRank.scala:64)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Adding task set 2.1 with 1 tasks
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Starting task 0.0 in stage 2.1 (TID 5, spark3, PROCESS_LOCAL, 4480 bytes)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] @@@@ExecutorID: 2 to launch task,time: 10日10时42分25秒288毫秒
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433904145291
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Got assigned task 5
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Running task 0.0 in stage 2.1 (TID 5)
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Fetching http://10.0.0.38:57731/jars/spark-examples_2.10-1.3.1.jar with timestamp 1433904116128
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Fetching http://10.0.0.38:57731/jars/spark-examples_2.10-1.3.1.jar to /tmp/spark-43798f24-76cd-418b-b2ac-0feb3fc0030f/spark-40fefb56-24f3-4f3f-894e-500318280c96/spark-c8ce1c15-fc38-4087-b177-3db71ec1e15b/fetchFileTemp2241740301211292328.tmp
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Copying /tmp/spark-43798f24-76cd-418b-b2ac-0feb3fc0030f/spark-40fefb56-24f3-4f3f-894e-500318280c96/spark-c8ce1c15-fc38-4087-b177-3db71ec1e15b/-20179281331433904116128_cache to /home/spark/spark-1.3.1/work/app-20150610104157-0000/2/./spark-examples_2.10-1.3.1.jar
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150610104157-0000/2/./spark-examples_2.10-1.3.1.jar to class loader
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Updating epoch to 8 and clearing cache
  [INFO ] [2015-06-10 10:42:25] [Logging$class:logInfo:59] Started reading broadcast variable 6
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] ensureFreeSpace(2239) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Updated info of block broadcast_6_piece0
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Reading broadcast variable 6 took 322 ms
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] ensureFreeSpace(4424) called with curMem=2239, maxMem=278302556
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Block broadcast_6 stored as values in memory (estimated size 4.3 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Partition rdd_6_0 not found, computing it
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Don't have map outputs for shuffle 1, fetching them
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:57579/user/MapOutputTracker#887606809]
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Started 1 remote fetches in 10 ms
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] ensureFreeSpace(800) called with curMem=6663, maxMem=278302556
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Block rdd_6_0 stored as values in memory (estimated size 800.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Updated info of block rdd_6_0
  [INFO ] [2015-06-10 10:42:26] [Logging$class:logInfo:59] Found block rdd_6_0 locally
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Finished task 0.0 in stage 2.1 (TID 5). 2705 bytes result sent to driver
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] @@@@ExecutorID: 2 to launch task,time: 10日10时42分27秒135毫秒
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433904147135
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Got assigned task 6
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Running task 0.0 in stage 3.0 (TID 6)
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Updating epoch to 9 and clearing cache
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Started reading broadcast variable 7
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] ensureFreeSpace(1541) called with curMem=7463, maxMem=278302556
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Block broadcast_7_piece0 stored as bytes in memory (estimated size 1541.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Updated info of block broadcast_7_piece0
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Reading broadcast variable 7 took 16 ms
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] ensureFreeSpace(2624) called with curMem=9004, maxMem=278302556
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Block broadcast_7 stored as values in memory (estimated size 2.6 KB, free 265.4 MB)
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Don't have map outputs for shuffle 0, fetching them
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:57579/user/MapOutputTracker#887606809]
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Started 0 remote fetches in 1 ms
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Finished task 0.0 in stage 3.0 (TID 6). 918 bytes result sent to driver
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [ERROR] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$1:apply$mcV$sp:65] AssociationError [akka.tcp://sparkWorker@spark3:49290] <- [akka.tcp://sparkExecutor@spark3:56250]: Error [Shut down address: akka.tcp://sparkExecutor@spark3:56250] [
akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@spark3:56250
Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.
]
  run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Stopped Spark web UI at http://spark1:4040
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Stopping DAGScheduler
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Shutting down all executors
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Asking each executor to shut down
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [ERROR] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$1:apply[INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.t[INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-t[INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  e turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  FO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Successfully stopped SparkContext
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.41%3A36159-3#709996596] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.41%3A36159-3#709996596] was not delivered. [4] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote[INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.42%3A52594-3#382231011] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.42%3A52594-3#382231011] was not delivered. [4] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 10:42:27] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [ak[INFO ] [2015-06-10 10:42:27] [Logging$class:logInfo:59] Executor app-20150610104157-0000/2 finished with state EXITED message Command exited with code 0 exitStatus 0
  [INFO ] [2015-06-10 10:42:28] [Logging$class:logInfo:59] Asked to kill unknown executor app-20150610104157-0000/2
  [INFO ] [2015-06-10 10:42:28] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610104157-0000
   be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 10:42:28] [Logging$class:logInfo:59] Executor app-20150610104157-0000/1 finished with state EXITED message Command exited with code 0 exitStatus 0
  [INFO ] [2015-06-10 10:42:29] [Logging$class:logInfo:59] Asked to kill unknown executor app-20150610104157-0000/1
  [INFO ] [2015-06-10 10:42:29] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610104157-0000
  0000/3
  [WARN ] [2015-06-10 10:42:29] [Logging$class:logWarning:71] Got status update for unknown executor app-20150610104157-0000/1
  [WARN ] [2015-06-10 10:42:29] [Logging$class:logWarning:71] Got status update for unknown executor app-20150610104157-0000/0
  [ERROR] [2015-06-10 10:55:17] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-06-10 10:55:17] [Logg[INFO ] [2015-06-10 10:55:17] [Logging$class:logInfo:59] Killing process!
  L 15: SIGTERM
  [INFO ] [2015-06-10 10:55:17] [Logging$class:logInfo:59] Killing process!
  message Worker shutting down exitStatus 0
  [INFO ] [2015-06-10 12:07:17] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 12:07:22] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 12:07:25] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 12:07:25] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 12:07:25] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 12:07:27] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 12:07:28] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 12:07:29] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkWorker@spark4:48537]
  [INFO ] [2015-06-10 12:07:29] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting now listens on addresses: [akka.tcp://sparkWorker@spark4:48537]
  [INFO ] [2015-06-10 12:07:29] [Logging$class:logInfo:59] Successfully started service 'sparkWorker' on port 48537.
  started; listening on addresses :[akka.tcp://sparkWorker@spark2:49309]
  [INFO ] [2015-06-10 12:07:29] [Logging$class:logInfo:59] Successfully started service 'sparkWorker' on port 49309.
  [INFO ] [2015-06-10 12:07:29] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting now listens on addresses: [akka.tcp://sparkWorker@spark2:49309]
  [INFO ] [2015-06-10 12:07:30] [Logging$class:logInfo:59] Starting Spark worker spark4:48537 with 4 cores, 970.0 MB RAM
  [INFO ] [2015-06-10 12:07:30] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 12:07:30] [Logging$class:logInfo:59] Spark home: /home/spark/spark-1.3.1
  [INFO ] [2015-06-10 12:07:31] [Logging$class:logInfo:59] Successfully started service 'WorkerUI' on port 8081.
  [INFO ] [2015-06-10 12:07:31] [Logging$class:logInfo:59] Started WorkerWebUI at http://spark4:8081
  [INFO ] [2015-06-10 12:07:31] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  12:07:32] [Logging$class:logInfo:59] Registering worker spark2:49309 with 4 cores, 970.0 MB RAM
  [INFO ] [2015-06-10 12:07:32] [Logging$class:logInfo:59] Registering worker spark4:48537 with 4 cores, 970.0 MB RAM
  [INFO ] [2015-06-10 12:07:32] [Logging$class:logInfo:59] I have been elected leader! New state: ALIVE
  [INFO ] [2015-06-10 12:07:38] [Logging$class:logInfo:59] Retrying connection to master (attempt # 1)
  [INFO ] [2015-06-10 12:07:39] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 12:07:39] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  :59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 12:07:43] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 12:07:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 12:07:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkWorker@spark3:34797]
  [INFO ] [2015-06-10 12:07:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting now listens on addresses: [akka.tcp://sparkWorker@spark3:34797]
  [INFO ] [2015-06-10 12:07:43] [Logging$class:logInfo:59] Successfully started service 'sparkWorker' on port[INFO ] [2015-06-10 12:07:41] [Logging$class:logInfo:59] Starting Spark worker spark5:42271 with 4 cores, 970.0 MB RAM[INFO ] [2015-06-10 12:07:44] [Logging$class:logInfo:59] Starting Spark worker spark3:34797 with 4 cores, 2.9 GB RAM
  [INFO ] [2015-06-10 12:07:44] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 12:07:44] [Logging$class:logInfo:59] Spark home: /home/spark/spark-1.3.1
  [INFO ] [2015-06-10 12:07:44] [Logging$class:logInfo:59] Successfully started service 'WorkerUI' on port 8081.
  [INFO ] [2015-06-10 12:07:44] [Logging$class:logInfo:59] Started WorkerWebUI at http://spark3:8081
  [INFO ] [2015-06-10 12:07:44] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 12:07:45] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
   [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 12:08:07] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 12:08:07] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 12:08:07] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 12:08:08] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 12:08:08] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkDriver@spark1:43350]
  [INFO ] [2015-06-10 12:08:08] [Logging$class:logInfo:59] Successfully started service 'sparkDriver' on port 43350.
  [INFO ] [2015-06-10 12:08:08] [Logging$class:logInfo:59] Registering MapOutputTracker
  [INFO ] [2015-06-10 12:08:08] [Logging$class:logInfo:59] Registering BlockManagerMaster
  [INFO ] [2015-06-10 12:08:08] [Logging$class:logInfo:59] Created local directory at /tmp/spark-c3f81412-1845-4cc1-8e8d-e5d411d9e21d/blockmgr-091de92f-c48c-41a6-8165-e1b70f5aeda4
  [INFO ] [2015-06-10 12:08:08] [Logging$class:logInfo:59] MemoryStore started with capacity 530.3 MB
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Asked to launch executor app-20150610120810-0000/3 for PageRank
  bb6e37583f0/httpd-493a5512-32bf-4bcb-8310-fd3f29c2b576
  [INFO ] [2015-06-10 12:08:08] [Logging$class:logInfo:59] Starting HTTP Server
  [INFO ] [2015-06-10 12:08:08] [Logging$class:logInfo:59] Successfully started service 'HTTP file server' on port 34810.
  [INFO ] [2015-06-10 12:08:09] [Logging$class:logInfo:59] Registering OutputCommitCoordinator
  [INFO ] [2015-06-10 12:08:09] [Logging$class:logInfo:59] Successfully started service 'SparkUI' on port 4040.
  [INFO ] [2015-06-10 12:08:09] [Logging$class:logInfo:59] Started SparkUI at http://spark1:4040
  [INFO ] [2015-06-10 12:08:09] [Logging$class:logInfo:59] Added JAR file:/home/spark/spark-1.3.1/examples/target/scala-2.10/spark-examples_2.10-1.3.1.jar at http://10.0.0.38:34810/jars/spark-examples_2.10-1.3.1.jar with timestamp 1433909289354
  [INFO ] [2015-06-10 12:08:09] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 12:08:09] [Logging$class:logInfo:59] Registering app PageRank
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Registered app PageRank with ID app-20150610120810-0000
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Connected to Spark cluster with app ID app-20150610120810-0000
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Launching executor app-20150610120810-0000/0 on worker worker-20150610120729-spark2-49309
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Launching executor app-20150610120810-0000/1 on worker worker-20150610120741-spark5-42271
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Launching executor app-20150610120810-0000/2 on worker worker-20150610120729-spark4-48537
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Launching executor app-20150610120810-0000/3 on worker worker-20150610120743-spark3-34797
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Executor added: app-20150610120810-0000/0 on worker-20150610120729-spark2-49309 (spark2:49309) with 4 cores
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Granted executor ID app-20150610120810-0000/0 on hostPort spark2:49309 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Executor added: app-20150610120810-0000/1 on worker-20150610120741-spark5-42271 (spark5:42271) with 4 cores
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Granted executor ID app-20150610120810-0000/1 on hostPort spark5:42271 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Executor added: app-20150610120810-0000/2 on worker-20150610120729-spark4-48537 (spark4:48537) with 4 cores
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Granted executor ID app-20150610120810-0000/2 on hostPort spark4:48537 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Executor added: app-20150610120810-0000/3 on worker-20150610120743-spark3-34797 (spark3:34797) with 4 cores
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Granted executor ID app-20150610120810-0000/3 on hostPort spark3:34797 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Asked to launch executor app-20150610120810-0000/2 for PageRank
   ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Executor updated: app-20150610120810-0000/1 is now RUNNING
  [INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Asked to launch executor app-20150610120810-0000/0 for PageRa[INFO ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Asked to launch executor app-20150610120810-0000/1 for PageRank
   ] [2015-06-10 12:08:10] [Logging$class:logInfo:59] Executor updated: app-20150610120810-0000/2 is now LOADING
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] Launch command: "java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/h[INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] Launch command: "java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar" "-XX:MaxPermSize=128m" "-Dspark.driver.port=43350" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@spark1:43350/user/CoarseGrainedScheduler" "--executor-id" "3" "--hostname" "spark3" "--cores" "4" "--app-id" "app-20150610120810-0000" "--worker-url" "akka.tcp://sparkWorker@spark3:34797/user/Worker"
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] akka.tcp://sparkDriver@spark1:43350 got disassociated, removing it.
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] Removing app app-20150610120810-0000
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] Asked to kill executor app-20150610120810-0000/1
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] Runner thread for executor app-20150610120810-0000/1 interrupted
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] Executor app-20150610120810-0000/1 finished with state KILLED exitStatus 143
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610120810-0000
  -10 12:08:11] [Logging$class:logInfo:59] Executor app-20150610120810-0000/3 finished with state KILLED exitStatus 143
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610120810-0000
  0 12:08:11] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.EndpointWriter$AckIdleCheckTimer$] from Actor[akka://sparkMaster/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkDriver%40spark1%3A43350-4/endpointWriter#-251695650] to Actor[akka://sparkMaster/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkDriver%40spark1%3A43350-4/endpointWriter#-251695650] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [WARN ] [2015-06-10 12:08:11] [Logging$class:logWarning:71] No event logs found for application PageRank in Some(file:/tmp/spark-events).
  [INFO ] [2015-06-10 12:08:11] [Logging$class:logInfo:59] akka.tcp://sparkDriver@spark1:43350 got disassociated, removing it.
  [WARN ] [2015-06-10 12:08:11] [Logging$class:logWarning:71] Got status update for unknown executor app-20150610120810-0000/3
  [WARN ] [2015-06-10 12:08:11] [Logging$class:logWarning:71] Got status update for unknown executor app-20150610120810-0000/0
  [WARN ] [2015-06-10 12:08:11] [Logging$class:logWarning:71] Got status update for unknown executor app-20150610120810-0000/1
  [WARN ] [2015-06-10 12:08:11] [Logging$class:logWarning:71] Got status update for unknown executor app-20150610120810-0000/2
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Asked to launch executor app-20150610120847-0001/1 for PageRank
  ng$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 12:08:45] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 12:08:45] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 12:08:46] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 12:08:46] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 12:08:46] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkDriver@spark1:55101]
  [INFO ] [2015-06-10 12:08:46] [Logging$class:logInfo:59] Successfully started service 'sparkDriver' on port 55101.
  [INFO ] [2015-06-10 12:08:46] [Logging$class:logInfo:59] Registering MapOutputTracker
  [INFO ] [2015-06-10 12:08:46] [Logging$class:logInfo:59] Registering BlockManagerMaster
  [INFO ] [2015-06-10 12:08:46] [Logging$class:logInfo:59] Created local directory at /tmp/spark-08d5d37c-fa9d-43c5-af08-8ee79841e696/blockmgr-0072595a-88eb-4c2c-aa0b-beba5f82cc6e
  [INFO ] [2015-06-10 12:08:46] [Logging$class:logInfo:59] MemoryStore started with capacity 530.3 MB
  [INFO ] [2015-06-10 12:08:46] [Logging$class:logInfo:59] HTTP File server directory is /tmp/spark-8c5f9984-f5c0-4835-91b1-8db3ae440377/httpd-ae4cd7ac-aee5-430b-aa65-ff4295ab2cba
  [INFO ] [2015-06-10 12:08:46] [Logging$class:logInfo:59] Starting HTTP Server
  [INFO ] [2015-06-10 12:08:46] [Logging$class:logInfo:59] Successfully started service 'HTTP file server' on port 34317.
  [INFO ] [2015-06-10 12:08:46] [Logging$class:logInfo:59] Registering OutputCommitCoordinator
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Successfully started service 'SparkUI' on port 4040.
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Started SparkUI at http://spark1:4040
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Added JAR file:/home/spark/spark-1.3.1/examples/target/scala-2.10/spark-examples_2.10-1.3.1.jar at http://10.0.0.38:34317/jars/spark-examples_2.10-1.3.1.jar with timestamp 1433909327074
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Registering app PageRank
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Registered app PageRank with ID app-20150610120847-0001
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Launching executor app-20150610120847-0001/0 on worker worker-20150610120729-spark2-49309
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Launching executor app-20150610120847-0001/1 on worker worker-20150610120741-spark5-42271
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Launching executor app-20150610120847-0001/2 on worker worker-20150610120729-spark4-48537
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Launching executor app-20150610120847-0001/3 on worker worker-20150610120743-spark3-34797
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Asked to launch executor app-20150610120847-0001/2 for PageRank
  INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor added: app-20150610120847-0001/0 on worker-20150610120729-spark2-49309 (spark2:49309) with 4 cores
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Granted executor ID app-20150610120847-0001/0 on hostPort spark2:49309 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor added: app-20150610120847-0001/1 on worker-20150610120741-spark5-42271 (spark5:42271) with 4 cores
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Granted executor ID app-20150610120847-0001/1 on hostPort spark5:42271 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor added: app-20150610120847-0001/2 on worker-20150610120729-spark4-48537 (spark4:48537) with 4 cores
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Granted executor ID app-20150610120847-0001/2 on hostPort spark4:48537 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor added: app-20150610120847-0001/3 on worker-20150610120743-spark3-34797 (spark3:34797) with 4 cores
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Granted executor ID app-20150610120847-0001/3 on hostPort spark3:34797 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor updated: app-20150610120847-0001/3 is now LOADING
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor updated: app-20150610120847-0001/0 is now LOADING
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor updated: app-20150610120847-0001/1 is now LOADING
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor updated: app-20150610120847-0001/0 is now RUNNING
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor updated: app-20150610120847-0001/1 is now RUNNING
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor updated: app-20150610120847-0001/2 is now RUNNING
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor updated: app-20150610120847-0001/3 is now RUNNING
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Executor updated: app-20150610120847-0001/2 is now LOADING
  [INFO ] [2015-06-10 12:08:47] [Logging$class:logInfo:59] Launch command: "java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar" "-XX:MaxPermSize=128m" "-Dspark.driver.port=55101" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@spark1:55101/user/CoarseGrainedScheduler" "--executor-id" "0" "--hostname" "spark2" "--cores" "4" "--app-id" "app-20150610120847-0001" "--worker-url" "akka.tcp://sparkWorker@spark2:49309/user/Worker"
  ort=55101" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@spark1:55101/user/CoarseGrainedScheduler" "--executor-id" "3" "--hostname" "spark3" "--cores" "4" "--app-id" "app-20150610120847-0001" "--worker-url" "akka.tcp://sparkWorker@spark3:34797/user/Worker"
  610120847-0001" "--worker-url" "akka.tcp://sparkWorker@spark4:48537/user/Worker"
  [INFO ] [2015-06-10 12:08:48] [Logging$class:logInfo:59] SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
  [INFO ] [2015-06-10 12:08:48] [Logging$class:logInfo:59] ensureFreeSpace(32768) called with curMem=0, maxMem=556038881
  [INFO ] [2015-06-10 12:08:48] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 530.2 MB)
  [INFO ] [2015-06-10 12:08:49] [Logging$class:logInfo:59] ensureFreeSpace(3985) called with curMem=32768, maxMem=556038881
  [INFO ] [2015-06-10 12:08:49] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.2 MB)
  [INFO ] [2015-06-10 12:08:49] [Logging$class:logInfo:59] Added broadcast_0_piece0 in memory on spark1:41764 (size: 3.9 KB, free: 530.3 MB)
  [INFO ] [2015-06-10 12:08:49] [Logging$class:logInfo:59] Updated info of block broadcast_0_piece0
  [INFO ] [2015-06-10 12:08:49] [Logging$class:logInfo:59] Created broadcast 0 from textFile at SparkPageRank.scala:56
  [WARN ] [2015-06-10 12:08:49] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-06-10 12:08:49] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 12:08:50] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 12:08:50] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 12:08:50] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 12:08:51] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 12:08:51] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 12:08:52] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@spark5:42686]
  [INFO ] [2015-06-10 12:08:52] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 42686.
  [INFO ] [2015-06-10 12:08:52] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 12:08:52] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 12:08:52] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 12:08:52] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 12:08:52] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
   ] [2015-06-10 12:08:50] [Logging$class:logInfo:59] Added broadcast_1_piece0 in memory on spark1:41764 (size: 2.1 KB, free: 530.3 MB)
  [INFO ] [2015-06-10 12:08:50] [Logging$class:logInfo:59] Updated info of block broadcast_1_piece0
  [INFO ] [2015-06-10 12:08:50] [Logging$class:logInfo:59] Created broadcast 1 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 12:08:50] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[3] at distinct at SparkPageRank.scala:60)
  [INFO ] [2015-06-10 12:08:50] [Logging$class:logInfo:59] Adding task set 0.0 with 1 tasks
  [INFO ] [2015-06-10 12:08:52] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@spark3:38529]
  [INFO ] [2015-06-10 12:08:52] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 38529.
  [INFO ] [2015-06-10 12:08:53] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 12:08:53] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 12:08:53] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-06-10 12:08:53] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 12:08:53] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 12:08:53] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@spark3:33946]
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 33946.
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Connecting to MapOutputTracker: akka.tcp://sparkDriver@spark1:55101/user/MapOutputTracker
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Connecting to BlockManagerMaster: akka.tcp://sparkDriver@spark1:55101/user/BlockManagerMaster
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Created local directory at /tmp/spark-be95472f-3b2f-4af0-976f-32db85087f07/spark-2d2acaf7-afde-4785-881e-7c18d64bae3e/spark-c149bd9a-e0f4-437b-b4ab-9e18fa9f2434/blockmgr-ca3431b6-3101-413b-938b-0ed15d4754fc
  [INFO ] [2015-06-10 12:[INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Server created on 49387
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark1:55101/user/HeartbeatReceiver
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] @@@@ExecutorID: 2 to launch task,time: 10日12时08分53秒919毫秒
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433909333943
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Got assigned task 0
  [INFO ] [2015-06-10 12:08:53] [Logging$class:logInfo:59] Running task 0.0 in stage 0.0 (TID 0)
  [INFO ] [2015-06-10 12:08:53] [Logg[INFO ] [2015-06-10 12:08:54] [Logging$class:logInfo:59] Server created on 39233
  [INFO ] [2015-06-10 12:08:54] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 12:08:54] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 12:08:54] [Logging$class:logInfo:59] Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark1:55101/user/HeartbeatReceiver
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] @@@@ExecutorID: 3 to launch task,time: 10日12时08分56秒855毫秒
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433909336869
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Got assigned task 1
  [INFO ] [2015-06-10 12:08:56] [Log[INFO ] [2015-06-10 12:08:54] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150610120847-0001/2/./spark-examples_2.10-1.3.1.jar to class loader
  [INFO ] [2015-06-10 12:08:54] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] ensureFreeSpace(2171) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 265.4 MB)
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Updated info of block broadcast_1_piece0
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Reading broadcast variable 1 took 364 ms
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] ensureFreeSpace(3656) called with curMem=2171, maxMem=278302556
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.6 KB, free 265.4 MB)
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Input split: hdfs://spark1/user/spark/data/mllib/pagerank_data.txt:0+24
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] ensureFreeSpace(3985) called with curMem=5827, maxMem=278302556
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Updated info of block broadcast_0_piece0
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Reading broadcast variable 0 took 31 ms
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] ensureFreeSpace(39120) called with curMem=9812, maxMem=278302556
  [INFO ] [2015-06-10 12:08:55] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 38.2 KB, free 265.4 MB)
  [WARN ] [2015-06-10 12:08:56] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-06-10 12:08:56] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Finished task 0.0 in stage 0.0 (TID 0). 1920 bytes result sent to driver
  9] waiting: Set(Stage 1, Stage 2, Stage 3)
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] failed: Set()
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Removed TaskSet 0.0, whose tasks have all completed, from pool 
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Missing parents for Stage 1: List()
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Missing parents for Stage 2: List(Stage 1)
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Missing parents for Stage 3: List(Stage 2)
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Submitting Stage 1 (MapPartitionsRDD[5] at distinct at SparkPageRank.scala:60), which is now runnable
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] ensureFreeSpace(2976) called with curMem=42580, maxMem=556038881
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 530.2 MB)
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] ensureFreeSpace(1669) called with curMem=45556, maxMem=556038881
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1669.0 B, free 530.2 MB)
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Added broadcast_2_piece0 in memory on spark1:41764 (size: 1669.0 B, free: 530.3 MB)
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Updated info of block broadcast_2_piece0
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Created broadcast 2 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 1 (MapPartitionsRDD[5] at distinct at SparkPageRank.scala:60)
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Adding task set 1.0 with 1 tasks
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Starting task 0.0 in stage 1.0 (TID 1, spark3, PROCESS_LOCAL, 1112 bytes)
  [INFO ] [2015-06-10 12:08:56] [Logging$class:logInfo:59] Ignoring failure of Stage 0 because all jobs depending on it are done
  [INFO ] [2015-06-10 12:08:57] [Logging$class:logInfo:59] Fetching http://10.0.0.38:34317/jars/spark-examples_2.10-1.3.1.jar to /tmp/spark-be95472f-3b2f-4af0-976f-32db85087f07/spark-2d2acaf7-afde-4785-881e-7c18d64bae3e/spark-5900b707-63bd-4b66-beee-cd1eca1f0455/fetchFileTemp1428455425244391147.tmp
  [INFO ] [2015-06-10 12:08:57] [Logging$class:logInfo:59] Copying /tmp/spark-be95472f-3b2f-4af0-976f-32db85087f07/spark-2d2acaf7-afde-4785-881e-7c18d64bae3e/spark-5900b707-63bd-4b66-beee-cd1eca1f0455/-19569506661433909327074_cache to /home/spark/spark-1.3.1/work/app-20150610120847-0001/3/./spark-examples_2.10-1.3.1.jar
  [INFO ] [2015-06-10 12:08:57] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150610120847-0001/3/./spark-examples_2.10-1.3.1.jar to class loader
  [INFO ] [2015-06-10 12:08:57] [Logging$class:logInfo:59] Updating epoch to 1 and clearing cache
  [INFO ] [2015-06-10 12:08:57] [Logging$class:logInfo:59] Started reading broadcast variable 2
  [INFO ] [2015-06-10 12:08:58] [Logging$class:logInfo:59] ensureFreeSpace(1669) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 12:08:58] [Logging$class:logInfo:59] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1669.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 12:08:58] [Logging$class:logInfo:59] Updated info of block broadcast_2_piece0
  [INFO ] [2015-06-10 12:08:58] [Logging$class:logInfo:59] Reading broadcast variable 2 took 355 ms
  [INFO ] [2015-06-10 12:08:58] [Logging$class:logInfo:59] ensureFreeSpace(2976) called with curMem=1669, maxMem=278302556
  [INFO ] [2015-06-10 12:08:58] [Logging$class:logInfo:59] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 12:08:58] [Logging$class:logInfo:59] Don't have map outputs for shuffle 2, fetching them
  [INFO ] [2015-06-10 12:08:58] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:55101/user/MapOutputTracker#20954860]
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Started 1 remote fetches in 20 ms
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Finished task 0.0 in stage 1.0 (TID 1). 1014 bytes result sent to driver
  [2015-06-10 12:08:59] [Logging$class:logInfo:59] ensureFreeSpace(4424) called with curMem=47225, maxMem=556038881
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 530.2 MB)
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] ensureFreeSpace(2239) called with curMem=51649, maxMem=556038881
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 530.2 MB)
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Added broadcast_3_piece0 in memory on spark1:41764 (size: 2.2 KB, free: 530.3 MB)
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Updated info of block broadcast_3_piece0
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Created broadcast 3 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 2 (MapPartitionsRDD[12] at flatMap at SparkPageRank.scala:64)
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Adding task set 2.0 with 1 tasks
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Starting task 0.0 in stage 2.0 (TID 2, spark2, PROCESS_LOCAL, 4480 bytes)
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] @@@@ExecutorID: 0 to launch task,time: 10日12时08分59秒500毫秒
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] @@@@StartMilliTime: 1433909339516
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Got assigned task 2
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Running task 0.0 in stage 2.0 (TID 2)
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Fetching http://10.0.0.38:34317/jars/spark-examples_2.10-1.3.1.jar with timestamp 1433909327074
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Fetching http://10.0.0.38:34317/jars/spark-examples_2.10-1.3.1.jar to /tmp/spark-36954f81-e28c-4d2f-ba3d-60df72f50c56/spark-5c2ba7ed-eb5e-4f81-be70-a03ec6808632/spark-28989d5c-8d4a-4379-9184-6a757a236af3/fetchFileTemp2798810471237552409.tmp
  [INFO ] [2015-06-10 12:08:59] [Logging$class:logInfo:59] Copying /tmp/spark-36954f81-e28c-4d2f-ba3d-60df72f50c56/spark-5c2ba7ed-eb5e-4f81-be70-a03ec6808632/spark-28989d5c-8d4a-4379-9184-6a757a236af3/-19569506661433909327074_cache to /home/spark/spark-1.3.1/work/app-20150610120847-0001/0/./spark-examples_2.10-1.3.1.jar
  [INFO ] [2015-06-10 12:09:00] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150610120847-0001/0/./spark-examples_2.10-1.3.1.jar to class loader
  [INFO ] [2015-06-10 12:09:00] [Logging$class:logInfo:59] Updating epoch to 2 and clearing cache
  [INFO ] [2015-06-10 12:09:00] [Logging$class:logInfo:59] Started reading broadcast variable 3
  [INFO ] [2015-06-10 12:09:00] [Logging$class:logInfo:59] ensureFreeSpace(2239) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 12:09:00] [Logging$class:logInfo:59] Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.4 MB)
  [INFO ] [2015-06-10 12:09:00] [Logging$class:logInfo:59] Updated info of block broadcast_3_piece0
  [INFO ] [2015-06-10 12:09:00] [Logging$class:logInfo:59] Reading broadcast variable 3 took 263 ms
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] ensureFreeSpace(4424) called with curMem=2239, maxMem=278302556
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 265.4 MB)
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Partition rdd_6_0 not found, computing it
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Don't have map outputs for shuffle 1, fetching them
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:55101/user/MapOutputTracker#20954860]
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Started 1 remote fetches in 19 ms
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] ensureFreeSpace(800) called with curMem=6663, maxMem=278302556
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Block rdd_6_0 stored as values in memory (estimated size 800.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Updated info of block rdd_6_0
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Found block rdd_6_0 locally
  [INFO ] [2015-06-10 12:09:01] [Logging$class:logInfo:59] Finished task 0.0 in stage 2.0 (TID 2). 2705 bytes result sent to driver
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to A[INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] @@@@ExecutorID: 2 to launch task,time: 10日12时09分02秒004毫秒
  [INFO ] [2015-06-10 12:09ed. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
   [Logging$class:logInfo:59] Running task 0.0 in stage 3.0 (TID 3)
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Updating epoch to 3 and clearing cache
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Started reading broadcast variable 4
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] ensureFreeSpace(1541) called with curMem=48932, maxMem=278302556
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1541.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Updated info of block broadcast_4_piece0
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Reading broadcast variable 4 took 26 ms
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] ensureFreeSpace(2624) called with curMem=50473, maxMem=278302556
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Block broadcast_4 stored as values in memory (estimated size 2.6 KB, free 265.4 MB)
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Don't have map outputs for shuffle 0, fetching them
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:55101/user/MapOutputTracker#20954860]
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Started 1 remote fetches in 16 ms
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Finished task 0.0 in stage 3.0 (TID 3). 918 bytes result sent to driver
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-06-10 12:09:02] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.41%3A48116-2#-758455416] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  opped
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.40%3A46521-2#-2059112362] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  RROR] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$1:apply$mcV$sp:65] AssociationError [akka.tcp://sparkWorker@spark5:42271] <- [akka.tcp://sparkExecutor@spark5:[ERROR] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$1:apply$mcV$sp:65] AssociationError [akka.tcp://sparkWorker@spark3:34797] <- [akka.tcp://sparkExecutor@spark3:33946]: Error [Shut down address: akka.tcp://sparkExecutor@spark3:33946] [
akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@spark3:33946
Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.
]
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.40%3A46521-2#-2059112362] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.EndpointWriter$AckIdleCheckTimer$] from Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark3%3A33946-1/endpointWriter#630430805] to Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark3%3A33946-1/endpointWriter#630430805] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  -shutdown'.
  [INFO ] [2015-06-10 12:09:02] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.EndpointWriter$AckIdleCheckTimer$] from Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark4%3A33927-1/endpointWriter#-1962284385] to Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark4%3A33927-1/endpointWriter#-1962284385] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  Writer#1300296565] was not delivered. [5] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 12:09:03] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 12:09:03] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 12:09:03] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 12:09:03] [Logging$class:logInfo:59] Executor app-20150610120847-0001/1 finished with state EXITED message Command exited with code 0 exitStatus 0
  [INFO ] [2015-06-10 12:09:03] [Logging$class:logInfo:59] Asked to kill unknown executor app-20150610120847-0001/1
  [INFO ] [2015-06-10 12:09:03] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610120847-0001
  his logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 12:09:03] [Logging$class:logInfo:59] Executor app-20150610120847-0001/3 finished with state EXITED message Command exited with code 0 exitStatus 0
  [INFO ] [2015-06-10 12:09:03] [Logging$class:logInfo:59] Asked to kill unknown executor app-20150610120847-0001/3
  [INFO ] [2015-06-10 12:09:03] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610120847-0001
  gging$class:logWarning:71] Got status update for unknown executor app-20150610120847-0001/2
  [ERROR] [2015-06-10 13:05:09] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-06-10 13:05:09] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-06-10 13:05:09] [Logging$class:logInfo:59] Unknown Executor app-20150610120847-0001/1 finished with state EXITED message Worker shutting down exitStatus 0
  [WARN ] [2015-06-10 13:05:09] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$2:apply$mcV$sp:71] Association with remote system [akka.tcp://sparkWorker@spark5:42271] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
  [INFO ] [2015-06-10 13:05:09] [Logging$class:logInfo:59] akka.tcp://sparkWorker@spark5:42271 got disassociated, removing it.
  [INFO ] [2015-06-10 13:05:09] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.0.42%3A41982-3#-1501591923] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 13:05:09] [Logging$class:logInfo:59] Removing worker worker-20150610120741-spark5-42271 on spark5:42271
  [INFO ] [2015-06-10 13:05:09] [Logging$class:logInfo:59] akka.tcp://sparkWorker@spark5:42271 got disassociated, removing it.
  [ERROR] [2015-06-10 13:05:47] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-06-10 13:05:47] [Logg[ERROR] [2015-06-10 13:05:47] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-06-10 13:05:47] [Logging$class:logInfo:59] Killing process!
  message Worker shutting down exitStatus 0
  TERM
  [INFO ] [2015-06-10 13:39:36] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 13:39:38] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 13:39:38] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 13:39:38] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 13:39:41] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 13:39:41] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 13:39:42] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkWorker@spark3:55862]
  [INFO ] [2015-06-10 13:39:42] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting now listens on addresses: [akka.tcp://sparkWorker@spark3:55862]
  [INFO ] [2015-06-10 13:39:42] [Logging$class:logInfo:59] Successfully started service 'sparkWorker' on port 55862.
  [INFO ] [2015-06-10 13:39:44] [Logging$class:logInfo:59] Starting Spark worker spark4:58834 with 4 cores, 970.0 MB RAM
  [INFO ] [2015-06-10 13:39:44] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 13:39:44] [Logging$class:logInfo:59] Spark home: /home/spark/spark-1.3.1/spark-1.3.1
  [INFO ] [2015-06-10 13:39:46] [Logging$class:logInfo:59] Successfully started service 'WorkerUI' on port 8081.
  [INFO ] [2015-06-10 13:39:46] [Logging$class:logInfo:59] Started WorkerWebUI at http://spark4:8081
  [INFO ] [2015-06-10 13:39:46] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  service 'sparkMaster' on port 7077.
  [INFO ] [2015-06-10 13:39:47] [Logging$class:logInfo:59] Successfully started service on port 6066.
  [INFO ] [2015-06-10 13:39:47] [Logging$class:logInfo:59] Started REST server for submitting applications on port 6066
  [INFO ] [2015-06-10 13:39:47] [Logging$class:logInfo:59] Starting Spark master at spark://spark1:7077
  [INFO ] [2015-06-10 13:39:47] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 13:39:47] [Logging$class:logInfo:59] Successfully started service 'MasterUI' on port 8080.
  [INFO ] [2015-06-10 13:39:47] [Logging$class:logInfo:59] Started MasterWebUI at http://spark1:8080
  [INFO ] [2015-06-10 13:39:48] [Logging$class:logInfo:59] Registering worker spark4:58834 with 4 cores, 970.0 MB RAM
  [INFO ] [2015-06-10 13:39:48] [Logging$class:logInfo:59] Registering worker spark2:43039 with 4 cores, 970.0 MB RAM
  [INFO ] [2015-06-10 13:39:48] [Logging$class:logInfo:59] Registering worker spark3:55862 with 4 cores, 2.9 GB RAM
  [INFO ] [2015-06-10 13:39:48] [Logging$class:logInfo:59] Registering worker spark5:59315 with 4 cores, 970.0 MB RAM
  [INFO ] [2015-06-10 13:39:48] [Logging$class:logInfo:59] I have been elected leader! New state: ALIVE
  [INFO ] [2015-06-10 13:39:52] [Logging$class:logInfo:59] Retrying connection to master (attempt # 1)
  [INFO ] [2015-0[INFO ] [2015-06-10 13:39:53] [Logging$class:logInfo:59] Retrying connection to master (attempt # 1)
  [INFO ] [2015-06-10 13:39:53] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 13:39:53] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  
  [INFO ] [2015-06-10 13:39:58] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  
  [INFO ] [2015-06-10 13:40:01] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  [INFO ] [2015-06-10 13:40:12] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 13:40:13] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 13:40:13] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 13:40:13] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 13:40:14] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 13:40:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 13:40:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkDriver@spark1:50496]
  [INFO ] [2015-06-10 13:40:14] [Logging$class:logInfo:59] Successfully started service 'sparkDriver' on port 50496.
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] Registering MapOutputTracker
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] Registering BlockManagerMaster
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] Created local directory at /tmp/spark-d957f450-7767-4338-b33e-27b5bc6cee14/blockmgr-8495908d-cd8a-4ed0-b885-111c0638b002
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] MemoryStore started with capacity 530.3 MB
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] HTTP File server directory is /tmp/spark-c14ec31f-a377-4d64-aaf8-7bb8a6fb620e/httpd-ca3a643d-26d2-4f59-a09a-84bd066569ae
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] Starting HTTP Server
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] Successfully started service 'HTTP file server' on port 46805.
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] Registering OutputCommitCoordinator
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] Successfully started service 'SparkUI' on port 4040.
  [INFO ] [2015-06-10 13:40:15] [Logging$class:logInfo:59] Started SparkUI at http://spark1:4040
  [INFO ] [2015-06-10 13:40:16] [Logging$class:logInfo:59] Added JAR file:/home/spark/spark-1.3.1/examples/target/scala-2.10/spark-examples_2.10-1.3.1.jar at http://10.0.0.38:46805/jars/spark-examples_2.10-1.3.1.jar with timestamp 1433914816091
  [INFO ] [2015-06-10 13:40:16] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Registering app PageRank
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Registered app PageRank with ID app-20150610134017-0000
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Connected to Spark cluster with app ID app-20150610134017-0000
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Launching executor app-20150610134017-0000/0 on worker worker-20150610133942-spark3-55862
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Launching executor app-20150610134017-0000/1 on worker worker-20150610133942-spark2-43039
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor added: app-20150610134017-0000/0 on worker-20150610133942-spark3-55862 (spark3:55862) with 4 cores
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Granted executor ID app-20150610134017-0000/0 on hostPort spark3:55862 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Launching executor app-20150610134017-0000/2 on worker worker-20150610133942-spark4-58834
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor added: app-20150610134017-0000/1 on worker-20150610133942-spark2-43039 (spark2:43039) with 4 cores
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Granted executor ID app-20150610134017-0000/1 on hostPort spark2:43039 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Launching executor app-20150610134017-0000/3 on worker worker-20150610133942-spark5-59315
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor added: app-20150610134017-0000/2 on worker-20150610133942-spark4-58834 (spark4:58834) with 4 cores
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Granted executor ID app-20150610134017-0000/2 on hostPort spark4:58834 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor added: app-20150610134017-0000/3 on worker-20150610133942-spark5-59315 (spark5:59315) with 4 cores
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Granted executor ID app-20150610134017-0000/3 on hostPort spark5:59315 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Asked to launch executor app-20150610134017-0000/1 for PageRank
   ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Asked to launch executor app-20150610134017-0000/2 for PageRank
   ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor updated: app-20150610134017-0000/2 is now RUNNING
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor updated: app-20150610134017-0000/3 is now RUNNING
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor updated: app-20150610134017-0000/2 is now LOADING
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor updated: app-20150610134017-0000/3 is now LOADING
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor updated: app-20150610134017-0000/0 is now LOADING
  [INFO ] [2015-06-10 13:40:17] [Logging$class:logInfo:59] Executor updated: app-20150610134017-0000/1 is now LOADING
  [INFO ] [2015-06-10 13:40:18] [Logging$class:logInfo:59] Launch command: "java" "-cp" "/home/spark/spark-1.3.1/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/spark-1.3.1/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar" "-XX:MaxPermSize=128m" "-Dspark.driver.port=50496" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@spark1:50496/user/CoarseGrainedScheduler" "--executor-id" "1" "--hostname" "spark2" "--cores" "4" "--app-id" "app-20150610134017-0000" "--worker-url" "akka.tcp://sparkWorker@spark2:43039/user/Worker"
  [WARN ] [2015-06-10 13:40:18] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [INFO ] [2015-06-10 13:40:18] [Logging$class:logInfo:59] Logging events to file:/tmp/spark-events/app-20150610134017-0000
  [INFO ] [2015-06-10 13:40:18] [Logging$class:logInfo:59] SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
  [INFO ] [2015-06-10 13:40:20] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 13:40:20] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 13:40:20] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 13:40:20] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 13:40:21] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 13:40:21] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 13:40:21] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@spark2:36100]
  [INFO ] [2015-06-10 13:40:21] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 36100.
  [INFO ] [2015-06-10 13:40:22] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 13:40:22] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 13:40:22] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 13:40:22] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 13:40:22] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 13:40:22] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 13:40:22] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 13:40:22] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-06-10 13:40:22] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@spark2:34805]
  [INFO ] [2015-06-10 13:40:22] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 34805.
  [INFO ] [2015-06-10 13:40:22] [Logging$class:logInfo:59] Connecting to MapOutputTracker: akka.tcp://sparkDriver@spark1:50496/user/MapOutputTracker
  [INFO ] [2015-06-10 13:40:22] [Logging$class:logInfo:59] Connecting to BlockManagerMaster: akka.tcp://sparkDriver@spark1:50496/user/BlockManagerMaster
  [INFO ] [2015-06-10 13:40:22] [Logging$class:logInfo:59] Created local directory at /tmp/spark-b7a47eab-31cd-4212-bd70-3fcca4641321/spark-6f338a8e-bc87-4548-9589-fd9b40a815f9/spark-c222eea3-2437-4718-a4a2-b71e0d9389e4/blockmgr-45389ee7-a12d-4836-97d1-2ad4f50224b7
  [INFO ] [2015-06-10 13:40:22] [Logging$class:logInfo:59] MemoryStore started with capacity 265.4 MB
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Connecting to OutputCommitCoordinator: akka.tcp://sparkDriver@spark1:50496/user/OutputCommitCoordinator
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@spark1:50496/user/CoarseGrainedScheduler
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@spark3:55862/user/Worker
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@spark3:55862/user/Worker
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Starting executor ID 0 on host spark3
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Server created on 39426
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark1:50496/user/HeartbeatReceiver
  cp://sparkDriver@spark1:50496/user/CoarseGrainedScheduler
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@spark2:43039/user/Worker
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@spark2:43039/user/Worker
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Starting executor ID 1 on host spark2
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Registered executor: Actor[akka.tcp://sparkExecutor@spark3:58624/user/Executor#1876605062] with ID 0
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Server created on 33329
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 13:40:23] [Logging$clas[INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Server created on 50964
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 13:40:23] [Logging$class:logInfo:59] Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark1:50496/user/HeartbeatReceiver
  015-06-10 13:40:36] [Logging$class:logInfo:59] Starting job: collect at SparkPageRank.scala:71
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Registering RDD 3 (distinct at SparkPageRank.scala:60)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Registering RDD 5 (distinct at SparkPageRank.scala:60)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Registering RDD 12 (flatMap at SparkPageRank.scala:64)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Got job 0 (collect at SparkPageRank.scala:71) with 1 output partitions (allowLocal=false)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Final stage: Stage 3(collect at SparkPageRank.scala:71)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Parents of final stage: List(Stage 2)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Missing parents: List(Stage 2)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Submitting Stage 0 (MapPartitionsRDD[3] at distinct at SparkPageRank.scala:60), which has no missing parents
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] ensureFreeSpace(3656) called with curMem=36753, maxMem=556038881
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.6 KB, free 530.2 MB)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] ensureFreeSpace(2171) called with curMem=40409, maxMem=556038881
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 530.2 MB)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Added broadcast_1_piece0 in memory on spark1:42725 (size: 2.1 KB, free: 530.3 MB)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Updated info of block broadcast_1_piece0
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Created broadcast 1 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[3] at distinct at SparkPageRank.scala:60)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Adding task set 0.0 with 1 tasks
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Starting task 0.0 in stage 0.0 (TID 0, spark2, NODE_LOCAL, 1374 bytes)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Got assigned task 0
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Running task 0.0 in stage 0.0 (TID 0)
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Fetching http://10.0.0.38:46805/jars/spark-examples_2.10-1.3.1.jar with timestamp 1433914816091
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Fetching http://10.0.0.38:46805/jars/spark-examples_2.10-1.3.1.jar to /tmp/spark-b7a47eab-31cd-4212-bd70-3fcca4641321/spark-6f338a8e-bc87-4548-9589-fd9b40a815f9/spark-282c7ac5-685f-4780-8587-47caf5869366/fetchFileTemp1800724912405638674.tmp
  [INFO ] [2015-06-10 13:40:36] [Logging$class:logInfo:59] Copying /tmp/spark-b7a47eab-31cd-4212-bd70-3fcca4641321/spark-6f338a8e-bc87-4548-9589-fd9b40a815f9/spark-282c7ac5-685f-4780-8587-47caf5869366/15929158911433914816091_cache to /home/spark/spark-1.3.1/spark-1.3.1/work/app-20150610134017-0000/1/./spark-examples_2.10-1.3.1.jar
  [INFO ] [2015-06-10 13:40:37] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/spark-1.3.1/work/app-20150610134017-0000/1/./spark-examples_2.10-1.3.1.jar to class loader
  [INFO ] [2015-06-10 13:40:37] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ] [2015-06-10 13:40:37] [Logging$class:logInfo:59] ensureFreeSpace(2171) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 13:40:37] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 265.4 MB)
  [INFO ] [2015-06-10 13:40:37] [Logging$class:logInfo:59] Updated info of block broadcast_1_piece0
  [INFO ] [2015-06-10 13:40:37] [Logging$class:logInfo:59] Reading broadcast variable 1 took 464 ms
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] ensureFreeSpace(3656) called with curMem=2171, maxMem=278302556
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.6 KB, free 265.4 MB)
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] Input split: hdfs://spark1/user/spark/data/mllib/pagerank_data.txt:0+24
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] ensureFreeSpace(3985) called with curMem=5827, maxMem=278302556
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] Updated info of block broadcast_0_piece0
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] Reading broadcast variable 0 took 28 ms
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] ensureFreeSpace(39120) called with curMem=9812, maxMem=278302556
  [INFO ] [2015-06-10 13:40:38] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 38.2 KB, free 265.4 MB)
  [WARN ] [2015-06-10 13:40:38] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-06-10 13:40:38] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-06-10 13:40:39] [Logging$class:logInfo:59] Finished task 0.0 in stage 0.0 (TID 0). 1920 bytes result sent to driver
  lass:logInfo:59] ensureFreeSpace(1669) called with curMem=45556, maxMem=556038881
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1669.0 B, free 530.2 MB)
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Added broadcast_2_piece0 in memory on spark1:42725 (size: 1669.0 B, free: 530.3 MB)
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Updated info of block broadcast_2_piece0
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Created broadcast 2 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 1 (MapPartitionsRDD[5] at distinct at SparkPageRank.scala:60)
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Adding task set 1.0 with 1 tasks
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Starting task 0.0 in stage 1.0 (TID 1, spark3, PROCESS_LOCAL, 1112 bytes)
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/spark-1.3.1/work/app-20150610134017-0000/0/./spark-examples_2.10-1.3.1.jar to class loader
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Updating epoch to 1 and clearing cache
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Started reading broadcast variable 2
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] ensureFreeSpace(1669) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1669.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Updated info of block broadcast_2_piece0
  [INFO ] [2015-06-10 13:40:40] [Logging$class:logInfo:59] Reading broadcast variable 2 took 297 ms
  [INFO ] [2015-06-10 13:40:41] [Logging$class:logInfo:59] ensureFreeSpace(2976) called with curMem=1669, maxMem=278302556
  [INFO ] [2015-06-10 13:40:41] [Logging$class:logInfo:59] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 13:40:41] [Logging$class:logInfo:59] Don't have map outputs for shuffle 2, fetching them
  [INFO ] [2015-06-10 13:40:41] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:50496/user/MapOutputTracker#-1740481976]
  [INFO ] [2015-06-10 13:40:41] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 13:40:41] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 13:40:41] [Logging$class:logInfo:59] Started 1 remote fetches in 17 ms
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Finished task 0.0 in stage 1.0 (TID 1). 1014 bytes result sent to driver
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Got assigned task 3
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Running task 0.0 in stage 3.0 (TID 3)
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Updating epoch to 3 and clearing cache
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Started reading broadcast variable 4
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] ensureFreeSpace(1541) called with curMem=4645, maxMem=278302556
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1541.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Updated info of block broadcast_4_piece0
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Reading broadcast variable 4 took 31 ms
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] ensureFreeSpace(2624) called with curMem=6186, maxMem=278302556
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Block broadcast_4 stored as values in memory (estimated size 2.6 KB, free 265.4 MB)
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Don't have map outputs for shuffle 0, fetching them
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:50496/user/MapOutputTracker#-1740481976]
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Started 1 remote fetches in 2 ms
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Finished task 0.0 in stage 3.0 (TID 3). 918 bytes result sent to driver
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [ERROR] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$1:apply$mcV$sp:65] AssociationError [akka.tcp://sparkWorker@spark3:55862] <- [akka.tcp://sparkExecutor@spark3:58624]: Error [Shut down address: akka.tcp://sparkExecutor@spark3:58624] [
akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@spark3:58624
Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.
]
  $class:logInfo:59] Block rdd_6_0 stored as values in memory (estimated size 800.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Updated info of block rdd_6_0
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Found block rdd_6_0 locally
  [INFO ] [2015-06-10 13:40:42] [Logging$class:logInfo:59] Finished task 0.0 in stage 2.0 (TID 2). 2705 bytes result sent to driver
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.39%3A47863-2#-1312494168] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  g$class:logInfo:59] Removed TaskSet 3.0, whose tasks have all completed, from pool 
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Job 0 finished: collect at SparkPageRank.scala:71, took 7.187113 s
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Stopped Spark web UI at http://spark1:4040
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Stopping DAGScheduler
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Shutting down all executors
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Asking each executor to shut down
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.t[ERROR] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$1:apply$mcV$sp:65] AssociationError [akka.tcp://sparkWorker@spark5:59315] <- [akka.tcp://sparkExecutor@spark5:50301]: Error [Shut down address: akka.tcp://sparkExecutor@spark5:50301] [
akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@spark5:50301
Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.
]
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.42%3A43998-2#-1791188328] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.42%3A43998-2#-1791188328] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.EndpointWriter$AckIdleCheckTimer$] from Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark5%3A50301-1/endpointWriter#1293099658] to Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark5%3A50301-1/endpointWriter#1293099658] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 13:40:43] [Logging$class:logInfo:59] Executor app-20150610134017-0000/3 finished with state EXITED message Command exited with code 0 exitStatus 0
  [INFO ] [2015-06-10 13:40:45] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610134017-0000
  emote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.0.38%3A33407-5#80782127] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 13:40:44] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [org.apache.spark.deploy.DeployMessages$ExecutorUpdated] from Actor[akka://sparkMaster/user/Master#-889625832] to Actor[akka://sparkMaster/deadLetters] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 13:40:44] [Logging$class:logInfo:59] akka.tcp://sparkDriver@spark1:50496 got disassociated, removing it.
  [INFO ] [2015-06-10 13:40:44] [Logging$class:logInfo:59] Removing app app-20150610134017-0000
  [ERROR] [2015-06-10 13:40:43] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$1:apply$mcV$sp:65] AssociationError [akka.tcp://sparkWorker@spark4:58834] <- [akka.tcp://sparkExecutor@spark4:50996]: Error [Shut down address: akka.tcp://sparkExecutor@spark4:50996] [
akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@spark4:50996
Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.
]
  port.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.40%3A46168-2#-1389767389] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  an be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 13:40:44] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.41%3A46409-2#2137217771] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 13:40:44] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.EndpointWriter$AckIdleCheckTimer$] from Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark4%3A50996-1/endpointWriter#-730279924] to Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark4%3A50996-1/endpointWriter#-730279924] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 13:40:45] [Logging$class:logInfo:59] Executor app-20150610134017-0000/2 finished with state EXITED message Command exited with code 0 exitStatus 0
  [INFO ] [2015-06-10 13:40:45] [Logging$class:logInfo:59] Asked to kill unknown executor app-20150610134017-0000/2
  [INFO ] [2015-06-10 13:40:45] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610134017-0000
  [ERROR] [2015-06-10 13:56:17] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-06-10 13:56:17] [Logg[ERROR] [2015-06-10 13:56:17] [SignalLoggerHandler:handle:57] RECEIVED SIGNAL 15: SIGTERM
  [INFO ] [2015-06-10 13:56:17] [Logging$class:logInfo:59] Killing process!
  [INFO ] [2015-06-10 13:56:17] [Logging$class:logInfo:59] Unknown Executor app-20150610134017-0000/3 finished with state EXITED message Worker shutting down exitStatus 0
  TERM
  [INFO ] [2015-06-10 14:01:10] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 14:01:10] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 14:01:10] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 14:01:10] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 14:01:11] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 14:01:11] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 14:01:12] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkMaster@spark1:7077]
  [INFO ] [2015-06-10 14:01:12] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting now listens on addresses: [akka.tcp://sparkMaster@spark1:7077]
  [INFO ] [2015-06-10 14:01:12] [Logging$class:logInfo:59] Successfully started service 'sparkMaster' on port 7077.
  [INFO ] [2015-06-10 14:01:12] [Logging$class:logInfo:59] Successfully started service on port 6066.
  [INFO ] [2015-06-10 14:01:12] [Logging$class:logInfo:59] Started REST server for submitting applications on port 6066
  [INFO ] [2015-06-10 14:01:12] [Logging$class:logInfo:59] Starting Spark master at spark://spark1:7077
  [INFO ] [2015-06-10 14:01:12] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 14:01:12] [Logging$class:logInfo:59] Successfully started service 'MasterUI' on port 8080.
  [INFO ] [2015-06-10 14:01:12] [Logging$class:logInfo:59] Started MasterWebUI at http://spark1:8080
  [INFO ] [2015-06-10 14:01:13] [Logging$class:logInfo:59] I have been elected leader! New state: ALIVE
  [INFO ] [2015-06-10 14:01:14] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 14:01:14] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 14:01:14] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 14:01:14] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 14:01:15] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 14:01:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 14:01:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkWorker@spark5:37082]
  [INFO ] [2015-06-10 14:01:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting now listens on addresses: [akka.tcp://sparkWorker@spark5:37082]
  [INFO ] [2015-06-10 14:01:15] [Logging$class:logInfo:59] Successfully started service 'sparkWorker' on port 37082.
  [INFO ] [2015-06-10 14:01:16] [Logging$class:logInfo:59] Starting Spark worker spark5:37082 with 4 cores, 970.0 MB RAM
  [INFO ] [2015-06-10 14:01:16] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 14:01:16] [Logging$class:logInfo:59] Spark home: /home/spark/spark-1.3.1
  [INFO ] [2015-06-10 14:01:16] [Logging$class:logInfo:59] Successfully started service 'WorkerUI' on port 8081.
  [INFO ] [2015-06-10 14:01:16] [Logging$class:logInfo:59] Started WorkerWebUI at http://spark5:8081
  [INFO ] [2015-06-10 14:01:16] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 14:01:16] [Logging$class:logInfo:59] Successfully registered with master spark://spark1:7077
  [INFO ] [2015-06-10 14:01:53] [Logging$class:logInfo:59] Running Spark version 1.3.1
  [INFO ] [2015-06-10 14:01:53] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 14:01:53] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 14:01:53] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 14:01:54] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 14:01:54] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 14:01:55] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkDriver@spark1:33779]
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] Successfully started service 'sparkDriver' on port 33779.
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] Registering MapOutputTracker
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] Registering BlockManagerMaster
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] Created local directory at /tmp/spark-020057fc-6c57-4fe2-96b8-7649fe737f9b/blockmgr-58b0c28a-67f9-46e7-8afe-dc29f8c29d08
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] MemoryStore started with capacity 530.3 MB
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] HTTP File server directory is /tmp/spark-206827a3-e35e-4171-a08d-90421339646a/httpd-5ac4cf54-c9c8-4aca-9e83-a453040b4b3d
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] Starting HTTP Server
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] Successfully started service 'HTTP file server' on port 54277.
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] Registering OutputCommitCoordinator
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] Successfully started service 'SparkUI' on port 4040.
  [INFO ] [2015-06-10 14:01:55] [Logging$class:logInfo:59] Started SparkUI at http://spark1:4040
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Added JAR file:/home/spark/spark-1.3.1/examples/target/scala-2.10/spark-examples-1.3.1-hadoop1.0.4.jar at http://10.0.0.38:54277/jars/spark-examples-1.3.1-hadoop1.0.4.jar with timestamp 1433916129199
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Registering app PageRank
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Registered app PageRank with ID app-20150610140209-0000
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Connected to Spark cluster with app ID app-20150610140209-0000
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Launching executor app-20150610140209-0000/0 on worker worker-20150610140115-spark4-38995
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Launching executor app-20150610140209-0000/1 on worker worker-20150610140115-spark3-41058
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Launching executor app-20150610140209-0000/2 on worker worker-20150610140115-spark5-37082
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Launching executor app-20150610140209-0000/3 on worker worker-20150610140115-spark2-48248
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Executor added: app-20150610140209-0000/0 on worker-20150610140115-spark4-38995 (spark4:38995) with 4 cores
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Granted executor ID app-20150610140209-0000/0 on hostPort spark4:38995 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Executor added: app-20150610140209-0000/1 on worker-20150610140115-spark3-41058 (spark3:41058) with 4 cores
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Granted executor ID app-20150610140209-0000/1 on hostPort spark3:41058 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Executor added: app-20150610140209-0000/2 on worker-20150610140115-spark5-37082 (spark5:37082) with 4 cores
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Granted executor ID app-20150610140209-0000/2 on hostPort spark5:37082 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Executor added: app-20150610140209-0000/3 on worker-20150610140115-spark2-48248 (spark2:48248) with 4 cores
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Granted executor ID app-20150610140209-0000/3 on hostPort spark2:48248 with 4 cores, 512.0 MB RAM
  [INFO ] [2015-06-10 14:02:09] [Logging$class:logInfo:59] Asked to launch executor app-20150610140209-0000/2 for PageRank
   ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Executor updated: app-20150610140209-0000/1 is now RUNNING
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Executor updated: app-20150610140209-0000/2 is now RUNNING
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Executor updated: app-20150610140209-0000/3 is now RUNNING
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Executor updated: app-20150610140209-0000/0 is now LOADING
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Executor updated: app-20150610140209-0000/3 is now LOADING
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Executor updated: app-20150610140209-0000/2 is now LOADING
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Executor updated: app-20150610140209-0000/1 is now LOADING
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Server created on 58714
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Registering block manager spark1:58714 with 530.3 MB RAM, BlockManagerId(<driver>, spark1, 58714)
  [INFO ] [2015-06-10 14:02:10] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 14:02:11] [Logging$class:logInfo:59] Launch command: "java" "-cp" "/home/spark/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/home/spark/spark-1.3.1/sbin/../conf:/home/spark/spark-1.3.1/assembly/target/scala-2.10/spark-assembly-1.3.1-hadoop1.0.4.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-core-3.2.10.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/home/spark/spark-1.3.1/lib_managed/jars/datanucleus-rdbms-3.2.9.jar" "-XX:MaxPermSize=128m" "-Dspark.driver.port=33779" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@spark1:33779/user/CoarseGrainedScheduler" "--executor-id" "2" "--hostname" "spark5" "--cores" "4" "--app-id" "app-20150610140209-0000" "--worker-url" "akka.tcp://sparkWorker@spark5:37082/user/Worker"
  [INFO ] [2015-06-10 14:02:11] [Logging$class:logInfo:59] ensureFreeSpace(32768) called with curMem=0, maxMem=556038881
  [INFO ] [2015-06-10 14:02:11] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 32.0 KB, free 530.2 MB)
  [INFO ] [2015-06-10 14:02:11] [Logging$class:logInfo:59] ensureFreeSpace(3985) called with curMem=32768, maxMem=556038881
  [INFO ] [2015-06-10 14:02:11] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.2 MB)
  [INFO ] [2015-06-10 14:02:11] [Logging$class:logInfo:59] Added broadcast_0_piece0 in memory on spark1:58714 (size: 3.9 KB, free: 530.3 MB)
  [INFO ] [2015-06-10 14:02:11] [Logging$class:logInfo:59] Updated info of block broadcast_0_piece0
  [INFO ] [2015-06-10 14:02:12] [Logging$class:logInfo:59] Created broadcast 0 from textFile at SparkPageRank.scala:56
  [WARN ] [2015-06-10 14:02:12] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-06-10 14:02:12] [SignalLogger$:register:47] Registered signal handlers for [TERM, HUP, INT]
  [INFO ] [2015-06-10 14:02:13] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 14:02:13] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 14:02:13] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 14:02:13] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 14:02:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 14:02:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@spark3:57275]
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 57275.
  [INFO ] [2015-06-10 14:02:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 14:02:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 14:02:14] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 14:02:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 14:02:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-06-10 14:02:14] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@spark3:53885]
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 53885.
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Connecting to MapOutputTracker: akka.tcp://sparkDriver@spark1:33779/user/MapOutputTracker
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Connecting to BlockManagerMaster: akka.tcp://sparkDriver@spark1:33779/user/BlockManagerMaster
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Created local directory at /tmp/spark-cc7fdcb2-eba0-4392-9f4a-c4c2c73d7aaa/spark-30a27d3f-9133-4ba9-8e01-c260666cdf41/spark-f8ab7d30-26b1-44a0-9831-e198815ae30e/blockmgr-89679b0c-3673-466b-ab2f-7d38c941d160
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] MemoryStore started with capacity 265.4 MB
  [INFO ] [2015-06-10 14:02:15] [Logging$class:logInfo:59] Connecting to OutputCommitCoordinator: akka.tcp://sparkDriver@spark1:33779/user/OutputCommitCoordinator
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@spark1:33779/user/CoarseGrainedScheduler
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@spark4:38995/user/Worker
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@spark4:38995/user/Worker
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-06-10 14:02:14] [Logging$class:logInfo:59] Starting executor ID 0 on host spark4
  [INFO ] [2015-06-10 14:02:16] [Logging$class:logInfo:59] Server created on 57810
  [INFO ] [2015-06-10 14:02:16] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 14:02:16] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 14:02:16] [Logging$class:logInfo:59] Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark1:33779/user/HeartbeatReceiver
   with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 14:02:17] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 14:02:17] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 14:02:17] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@spark5:35365]
  [INFO ] [2015-06-10 14:02:18] [Logging$class:logInfo:59] Successfully started service 'driverPropsFetcher' on port 35365.
  [INFO ] [2015-06-10 14:02:18] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 14:02:18] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 14:02:18] [Logging$class:logInfo:59] Changing view acls to: spark
  [INFO ] [2015-06-10 14:02:18] [Logging$class:logInfo:59] Changing modify acls to: spark
  [INFO ] [2015-06-10 14:02:18] [Logging$class:logInfo:59] SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)
  [INFO ] [2015-06-10 14:02:18] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-06-10 14:02:18] [Slf4jLogger$$anonfun$receive$1:applyOrElse:80] Slf4jLogger started
  [INFO ] [2015-06-10 14:02:18] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Starting remoting
  [INFO ] [2015-06-10 14:02:18] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting started; listening on addresses :[akka.tcp://sparkExecutor@spark5:39999]
  [INFO ] [2015-06-10 14:02:18] [Logging$class:logInfo:59] Successfully started service 'sparkExecutor' on port 39999.
  [INFO ] [2015-06-10 14:02:18] [Logging$class:logInfo:59] Connecting to MapOutputTracker: akka.tcp://sparkDriver@spark1:33779/user/MapOutputTracker
  [INFO ] [2015-06-10 14:02:18] [Logging$class:logInfo:59] Connecting to BlockManagerMaster: akka.tcp://sparkDriver@spark1:33779/user/BlockManagerMaster
  [INFO ] [2015-06-10 14:02:18] [Logging$class:logInfo:59] Created local directory at /tmp/spark-3d7d8572-428d-4fe8-a5b0-2d7055fe758f/spark-51f3f13b-350c-4734-8fad-6df583773807/spark-0e8e65ac-5553-40ca-a360-32a3e274cdf6/blockmgr-1eb55468-9df7-453d-9c74-627858be9bc0
  [INFO ] [2015-06-10 14:02:18] [Logging$class:logInfo:59] MemoryStore started with capacity 265.4 MB
  g$class:logInfo:59] Created broadcast 1 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 14:02:17] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[3] at distinct at SparkPageRank.scala:60)
  [INFO ] [2015-06-10 14:02:17] [Logging$class:logInfo:59] Adding task set 0.0 with 1 tasks
  [INFO ] [2015-06-10 14:02:17] [Logging$class:logInfo:59] Starting task 0.0 in stage 0.0 (TID 0, spark3, NODE_LOCAL, 1381 bytes)
  [INFO ] [2015-06-10 14:02:19] [Logging$class:logInfo:59] Connecting to OutputCommitCoordinator: akka.tcp://sparkDriver@spark1:33779/user/OutputCommitCoordinator
  [INFO ] [2015-06-10 14:02:20] [Logging$class:logInfo:59] Connecting to driver: akka.tcp://sparkDriver@spark1:33779/user/CoarseGrainedScheduler
  [INFO ] [2015-06-10 14:02:20] [Logging$class:logInfo:59] Connecting to worker akka.tcp://sparkWorker@spark5:37082/user/Worker
  [INFO ] [2015-06-10 14:02:20] [Logging$class:logInfo:59] Successfully connected to akka.tcp://sparkWorker@spark5:37082/user/Worker
  [INFO ] [2015-06-10 14:02:21] [Logging$class:logInfo:59] Successfully registered with driver
  [INFO ] [2015-06-10 14:02:21] [Logging$class:logInfo:59] Starting executor ID 2 on host spark5
  [INFO ] [2015-06-10 14:02:22] [Logging$class:logInfo:59] Server created on 59427
  [INFO ] [2015-06-10 14:02:22] [Logging$class:logInfo:59] Trying to register BlockManager
  [INFO ] [2015-06-10 14:02:22] [Logging$class:logInfo:59] Registered BlockManager
  [INFO ] [2015-06-10 14:02:22] [Logging$class:logInfo:59] Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@spark1:33779/user/HeartbeatReceiver
  [INFO ] [2015-06-10 14:02:40] [Logging$class:logInfo:59] Copying /tmp/spark-cc7fdcb2-eba0-4392-9f4a-c4c2c73d7aaa/spark-30a27d3f-9133-4ba9-8e01-c260666cdf41/spark-32c57942-7e0a-41bc-946b-8a88429fb78e/-21431118801433916129199_cache to /home/spark/spark-1.3.1/work/app-20150610140209-0000/1/./spark-examples-1.3.1-hadoop1.0.4.jar
  [INFO ] [2015-06-10 14:02:49] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150610140209-0000/1/./spark-examples-1.3.1-hadoop1.0.4.jar to class loader
  [INFO ] [2015-06-10 14:02:49] [Logging$class:logInfo:59] Started reading broadcast variable 1
  [INFO ] [2015-06-10 14:02:50] [Logging$class:logInfo:59] ensureFreeSpace(2171) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 14:02:50] [Logging$class:logInfo:59] Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 265.4 MB)
  [INFO ] [2015-06-10 14:02:50] [Logging$class:logInfo:59] Updated info of block broadcast_1_piece0
  [INFO ] [2015-06-10 14:02:50] [Logging$class:logInfo:59] Reading broadcast variable 1 took 528 ms
  [INFO ] [2015-06-10 14:02:50] [Logging$class:logInfo:59] ensureFreeSpace(3656) called with curMem=2171, maxMem=278302556
  [INFO ] [2015-06-10 14:02:50] [Logging$class:logInfo:59] Block broadcast_1 stored as values in memory (estimated size 3.6 KB, free 265.4 MB)
  [INFO ] [2015-06-10 14:02:51] [Logging$class:logInfo:59] Input split: hdfs://spark1/user/spark/data/mllib/pagerank_data.txt:0+24
  [INFO ] [2015-06-10 14:02:51] [Logging$class:logInfo:59] Started reading broadcast variable 0
  [INFO ] [2015-06-10 14:02:51] [Logging$class:logInfo:59] ensureFreeSpace(3985) called with curMem=5827, maxMem=278302556
  [INFO ] [2015-06-10 14:02:51] [Logging$class:logInfo:59] Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 14:02:51] [Logging$class:logInfo:59] Updated info of block broadcast_0_piece0
  [INFO ] [2015-06-10 14:02:51] [Logging$class:logInfo:59] Reading broadcast variable 0 took 27 ms
  [INFO ] [2015-06-10 14:02:51] [Logging$class:logInfo:59] ensureFreeSpace(39120) called with curMem=9812, maxMem=278302556
  [INFO ] [2015-06-10 14:02:51] [Logging$class:logInfo:59] Block broadcast_0 stored as values in memory (estimated size 38.2 KB, free 265.4 MB)
  [WARN ] [2015-06-10 14:02:51] [NativeCodeLoader:<clinit>:52] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  [WARN ] [2015-06-10 14:02:51] [LoadSnappy:<clinit>:46] Snappy native library not loaded
  [INFO ] [2015-06-10 14:02:51] [Logging$class:logInfo:59] Finished task 0.0 in stage 0.0 (TID 0). 1920 bytes result sent to driver
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Got assigned task 1
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Running task 0.0 in stage 1.0 (TID 1)
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Updating epoch to 1 and clearing cache
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Started reading broadcast variable 2
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] ensureFreeSpace(1669) called with curMem=48932, maxMem=278302556
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Block broadcast_2_piece0 stored as bytes in memory (estimated size 1669.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Updated info of block broadcast_2_piece0
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Reading broadcast variable 2 took 35 ms
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] ensureFreeSpace(2976) called with curMem=50601, maxMem=278302556
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 265.4 MB)
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Don't have map outputs for shuffle 2, fetching them
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:33779/user/MapOutputTracker#-991972900]
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Started 0 remote fetches in 11 ms
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Finished task 0.0 in stage 1.0 (TID 1). 1014 bytes result sent to driver
  4), which is now runnable
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] ensureFreeSpace(4424) called with curMem=47225, maxMem=556038881
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 530.2 MB)
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] ensureFreeSpace(2239) called with curMem=51649, maxMem=556038881
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 530.2 MB)
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Added broadcast_3_piece0 in memory on spark1:58714 (size: 2.2 KB, free: 530.3 MB)
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Updated info of block broadcast_3_piece0
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Created broadcast 3 from broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 2 (MapPartitionsRDD[12] at flatMap at SparkPageRank.scala:64)
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Adding task set 2.0 with 1 tasks
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Starting task 0.0 in stage 2.0 (TID 2, spark2, PROCESS_LOCAL, 4487 bytes)
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Got assigned task 2
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Running task 0.0 in stage 2.0 (TID 2)
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Fetching http://10.0.0.38:54277/jars/spark-examples-1.3.1-hadoop1.0.4.jar with timestamp 1433916129199
  [INFO ] [2015-06-10 14:02:52] [Logging$class:logInfo:59] Fetching http://10.0.0.38:54277/jars/spark-examples-1.3.1-hadoop1.0.4.jar to /tmp/spark-e2ee05e0-9e05-4468-800c-06163305e19c/spark-c7fd0a9a-411e-4610-9580-96c758b70ae0/spark-a72a86ee-d7b3-4e32-9642-e4b1e2a7a260/fetchFileTemp1212420184543888652.tmp
  [INFO ] [2015-06-10 14:02:55] [Logging$class:logInfo:59] Copying /tmp/spark-e2ee05e0-9e05-4468-800c-06163305e19c/spark-c7fd0a9a-411e-4610-9580-96c758b70ae0/spark-a72a86ee-d7b3-4e32-9642-e4b1e2a7a260/-21431118801433916129199_cache to /home/spark/spark-1.3.1/work/app-20150610140209-0000/3/./spark-examples-1.3.1-hadoop1.0.4.jar
  [INFO ] [2015-06-10 14:03:03] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150610140209-0000/3/./spark-examples-1.3.1-hadoop1.0.4.jar to class loader
  [INFO ] [2015-06-10 14:03:03] [Logging$class:logInfo:59] Updating epoch to 2 and clearing cache
  [INFO ] [2015-06-10 14:03:03] [Logging$class:logInfo:59] Started reading broadcast variable 3
  [INFO ] [2015-06-10 14:03:03] [Logging$class:logInfo:59] ensureFreeSpace(2239) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 14:03:03] [Logging$class:logInfo:59] Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.4 MB)
  [INFO ] [2015-06-10 14:03:03] [Logging$class:logInfo:59] Updated info of block broadcast_3_piece0
  [INFO ] [2015-06-10 14:03:03] [Logging$class:logInfo:59] Reading broadcast variable 3 took 285 ms
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] ensureFreeSpace(4424) called with curMem=2239, maxMem=278302556
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 265.4 MB)
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Partition rdd_6_0 not found, computing it
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Don't have map outputs for shuffle 1, fetching them
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@spark1:33779/user/MapOutputTracker#-991972900]
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Got the output locations
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Getting 1 non-empty blocks out of 1 blocks
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Started 1 remote fetches in 19 ms
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] ensureFreeSpace(800) called with curMem=6663, maxMem=278302556
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Block rdd_6_0 stored as values in memory (estimated size 800.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Updated info of block rdd_6_0
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Found block rdd_6_0 locally
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Finished task 0.0 in stage 2.0 (TID 2). 2705 bytes result sent to driver
  broadcast at DAGScheduler.scala:839
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Submitting 1 missing tasks from Stage 3 (MapPartitionsRDD[14] at mapValues at SparkPageRank.scala:68)
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Adding task set 3.0 with 1 tasks
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Starting task 0.0 in stage 3.0 (TID 3, spark4, PROCESS_LOCAL, 1130 bytes)
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Got assigned task 3
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Running task 0.0 in stage 3.0 (TID 3)
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Fetching http://10.0.0.38:54277/jars/spark-examples-1.3.1-hadoop1.0.4.jar with timestamp 1433916129199
  [INFO ] [2015-06-10 14:03:04] [Logging$class:logInfo:59] Fetching http://10.0.0.38:54277/jars/spark-examples-1.3.1-hadoop1.0.4.jar to /tmp/spark-87c71109-cce5-44c7-b0ba-5ca6580b07bd/spark-057be4d1-7a4e-4caa-9ad0-209c12a1765f/spark-0d2634d5-a2be-4ad5-9d58-76eb5d463334/fetchFileTemp1258181717808078768.tmp
  [INFO ] [2015-06-10 14:03:06] [Logging$class:logInfo:59] Copying /tmp/spark-87c71109-cce5-44c7-b0ba-5ca6580b07bd/spark-057be4d1-7a4e-4caa-9ad0-209c12a1765f/spark-0d2634d5-a2be-4ad5-9d58-76eb5d463334/-21431118801433916129199_cache to /home/spark/spark-1.3.1/work/app-20150610140209-0000/0/./spark-examples-1.3.1-hadoop1.0.4.jar
  [INFO ] [2015-06-10 14:03:13] [Logging$class:logInfo:59] Adding file:/home/spark/spark-1.3.1/work/app-20150610140209-0000/0/./spark-examples-1.3.1-hadoop1.0.4.jar to class loader
  [INFO ] [2015-06-10 14:03:13] [Logging$class:logInfo:59] Updating epoch to 3 and clearing cache
  [INFO ] [2015-06-10 14:03:13] [Logging$class:logInfo:59] Started reading broadcast variable 4
  [INFO ] [2015-06-10 14:03:14] [Logging$class:logInfo:59] ensureFreeSpace(1541) called with curMem=0, maxMem=278302556
  [INFO ] [2015-06-10 14:03:14] [Logging$class:logInfo:59] Block broadcast_4_piece0 stored as bytes in memory (estimated size 1541.0 B, free 265.4 MB)
  [INFO ] [2015-06-10 14:03:14] [Logging$class:logInfo:59] Updated info of block broadcast_4_piece0
  [INFO ] [2015-06-10 14:03:14] [Logging$class:logInfo:59] Reading broadcast variable 4 took 344 ms
  [INFO ] [2015-06-10 14:03:14] [Logging$class:logInfo:59] ensureFreeSpace(2624) called with curMem=1541, maxMem=278302556
  [INFO ] [2015-06-10 14:03:14] [Logging$class:logInfo:59] Block broadcast_4 stored as values in memory (estimated size 2.6 KB, free 265.4 MB)
  [INFO ] [2015-06-10 14:03:15] [Logging$class:logInfo:59] Don't have map outputs for shuffle 0, fetching them
  [INFO ] [2015-06-10 14:03:15] [[INFO ] [2015-06-10 14:03:15] [Logging$class:logInfo:59] Driver commanded a shutdown
  [INFO ] [2015-06-10 14:03:15] [Logging$class:logInfo:59] MemoryStore cleared
  [INFO ] [2015-06-10 14:03:15] [Logging$class:logInfo:59] BlockManager stopped
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Shutting down remote daemon.
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.39%3A58846-2#-723299388] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
   .
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.41%3A58298-2#1136814217] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  applyOrElse$3:apply$mcV$sp:74] Remote daemon shut down; proceeding with flushing remote transports.
  [ERROR] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$1:apply$mcV$sp:65] AssociationError [akka.tcp://sparkWorker@spark5:37082] <- [akka.tcp://sparkExecutor@spark5:39999]: Error [Shut down address: akka.tcp://sparkExecutor@spark5:39999] [
akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@spark5:39999
Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.
]
   [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.AssociationHandle$Disassociated] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.0.38%3A33422-5#1089724365] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Remoting shut down.
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkMaster/deadLetters] to Actor[akka://sparkMaster/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkMaster%4010.0.0.38%3A33422-5#1089724365] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.EndpointWriter$AckIdleCheckTimer$] from Actor[akka://sparkMaster/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkDriver%40spark1%3A33779-4/endpointWriter#250155061] to Actor[akka://sparkMaster/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkDriver%40spark1%3A33779-4/endpointWriter#250155061] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 14:03:15] [Logging$class:logInfo:59] akka.tcp://sparkDriver@spark1:33779 got disassociated, removing it.
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.42%3A47715-2#-1656869545] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  fun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4010.0.0.39%3A58846-2#-723299388] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.EndpointWriter$AckIdleCheckTimer$] from Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark2%3A57964-1/endpointWriter#787257104] to Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark2%3A57964-1/endpointWriter#787257104] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 14:03:15] [Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3:apply$mcV$sp:74] Message [akka.remote.EndpointWriter$AckIdleCheckTimer$] from Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsparkExecutor%40spark2%3A57964-1/endpointWriter#787257104] to Actor[akka://sparkWorker/system/endpointManager/reliableEndpointWrite[INFO ] [2015-06-10 14:03:16] [Logging$class:logInfo:59] Executor app-20150610140209-0000/1 finished with state EXITED message Command exited with code 0 exitStatus 0
  [INFO ] [2015-06-10 14:03:17] [Logging$class:logInfo:59] Asked to kill unknown executor app-20150610140209-0000/1
  [INFO ] [2015-06-10 14:03:17] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610140209-0000
  %40spark5%3A39999-1/endpointWriter#-13326135] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
  [INFO ] [2015-06-10 14:03:16] [Logging$class:logInfo:59] Executor app-20150610140209-0000/2 finished with state EXITED message Command exited with code 0 exitStatus 0
  [INFO ] [2015-06-10 14:03:17] [Logging$class:logInfo:59] Asked to kill unknown executor app-20150610140209-0000/2
  [INFO ] [2015-06-10 14:03:17] [Logging$class:logInfo:59] Cleaning up local directories for application app-20150610140209-0000
  